{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lsXO3SvTx3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d4af1d-3e29-49d1-e00f-e29ba5c00c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.1.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.37 (from langchain_community)\n",
            "  Downloading langchain_core-0.2.37-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
            "  Downloading langsmith-0.1.108-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain_community)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai)\n",
            "  Downloading openai-1.43.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.1)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.8.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.112.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.6.3-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.5)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.4)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.27.0 (from chromadb)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.38.4-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.37->langchain_community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.64.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.8.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.37->langchain_community)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Downloading langchain_community-0.2.15-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.1.23-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading langchain-0.2.15-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-0.1.3-py3-none-any.whl (10 kB)\n",
            "Downloading chromadb-0.5.3-py3-none-any.whl (559 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.112.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.37-py3-none-any.whl (396 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.108-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.43.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.6.3-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.38.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=b96766ec107e38c8674f34a84a6c3430c77033d949b2d63ec1d609a86484d2b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, types-requests, tenacity, python-dotenv, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, jiter, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, langchainhub, jsonpatch, httpcore, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, fastapi, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation-asgi, openai, langsmith, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain-core, langchain-text-splitters, langchain-openai, chromadb, langchain_chroma, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-hnswlib-0.7.3 chromadb-0.5.3 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 fastapi-0.112.2 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.2 humanfriendly-10.0 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-30.1.0 langchain-0.2.15 langchain-core-0.2.37 langchain-openai-0.1.23 langchain-text-splitters-0.2.2 langchain_chroma-0.1.3 langchain_community-0.2.15 langchainhub-0.1.21 langsmith-0.1.108 marshmallow-3.22.0 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.19.0 openai-1.43.0 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 orjson-3.10.7 overrides-7.7.0 posthog-3.6.3 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.38.4 tenacity-8.5.0 tiktoken-0.7.0 types-requests-2.32.0.20240712 typing-inspect-0.9.0 uvicorn-0.30.6 uvloop-0.20.0 watchfiles-0.24.0 websockets-13.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain beautifulsoup4 langchain_chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"your_langchain_key\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"]=\"rag_implementation\""
      ],
      "metadata": {
        "id": "EcxjWS5dU6uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpkXSVpoWqTL",
        "outputId": "58a6bb6a-bf5a-40df-cd36-7c91e281aabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG on Webpage"
      ],
      "metadata": {
        "id": "etiadnrYXNpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bDWmAQXW9n6",
        "outputId": "10c7a638-a40a-494e-f587-464022c3e2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed = OpenAIEmbeddings()\n",
        "webpage = \"https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452\"\n",
        "loader = WebBaseLoader(webpage)\n",
        "data = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=10)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "for i in range(len(all_splits)):\n",
        "    print(\"Chunk \",i,\" : \",all_splits[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44RjxS4KXF46",
        "outputId": "546aae7f-3662-47c3-b3ef-99cc06399965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk  0  :  page_content='Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inINTUITIVE TRANSFORMERS SERIES NLPTransformers Explained Visually (Part 1): Overview of FunctionalityA Gentle Guide to Transformers, how they are used' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  1  :  page_content='are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.Ketan Doshi·FollowPublished inTowards Data Science·10 min read·Dec 13, 2020--23ListenSharePhoto by Arseny Togulev on UnsplashWe’ve been hearing a lot about Transformers and with good' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  2  :  page_content='with good reason. They have taken the world of NLP by storm in the last few years. The Transformer is an architecture that uses Attention to significantly improve the performance of deep learning NLP translation models. It was first introduced in the paper Attention is all you need and was quickly' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  3  :  page_content='quickly established as the leading architecture for most text data applications.Since then, numerous projects including Google’s BERT and OpenAI’s GPT series have built on this foundation and published performance results that handily beat existing state-of-the-art benchmarks.Over a series of' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  4  :  page_content='series of articles, I’ll go over the basics of Transformers, its architecture, and how it works internally. We will cover the Transformer functionality in a top-down manner. In later articles, we will look under the covers to understand the operation of the system in detail. We will also do a deep' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  5  :  page_content='do a deep dive into the workings of the multi-head attention, which is the heart of the Transformer.Here’s a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way.Overview of functionality' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  6  :  page_content='— this article (How Transformers are used, and why they are better than RNNs. Components of the architecture, and behavior during Training and Inference)How it works (Internal operation end-to-end. How data flows and what computations are performed, including matrix representations)Multi-head' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  7  :  page_content='Attention (Inner workings of the Attention module throughout the Transformer)Why Attention Boosts Performance (Not just what Attention does but why it works so well. How does Attention capture the relationships between words in a sentence)And if you’re interested in NLP applications in general, I' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  8  :  page_content='I have some other articles you might like.Beam Search (Algorithm commonly used by Speech-to-Text and NLP applications to enhance predictions)Bleu Score (Bleu Score and Word Error Rate are two essential metrics for NLP models)What is a TransformerThe Transformer architecture excels at handling text' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  9  :  page_content='text data which is inherently sequential. They take a text sequence as input and produce another text sequence as output. eg. to translate an input English sentence to Spanish.(Image by Author)At its core, it contains a stack of Encoder layers and Decoder layers. To avoid confusion we will refer to' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  10  :  page_content='refer to the individual layer as an Encoder or a Decoder and will use Encoder stack or Decoder stack for a group of Encoder layers.The Encoder stack and the Decoder stack each have their corresponding Embedding layers for their respective inputs. Finally, there is an Output layer to generate the' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  11  :  page_content='the final output.(Image by Author)All the Encoders are identical to one another. Similarly, all the Decoders are identical.(Image by Author)The Encoder contains the all-important Self-attention layer that computes the relationship between different words in the sequence, as well as a Feed-forward' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  12  :  page_content='layer.The Decoder contains the Self-attention layer and the Feed-forward layer, as well as a second Encoder-Decoder attention layer.Each Encoder and Decoder has its own set of weights.The Encoder is a reusable module that is the defining component of all Transformer architectures. In addition to' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  13  :  page_content='to the above two layers, it also has Residual skip connections around both layers along with two LayerNorm layers.(Image by Author)There are many variations of the Transformer architecture. Some Transformer architectures have no Decoder at all and rely only on the Encoder.What does Attention Do?The' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  14  :  page_content='Do?The key to the Transformer’s ground-breaking performance is its use of Attention.While processing a word, Attention enables the model to focus on other words in the input that are closely related to that word.eg. ‘Ball’ is closely related to ‘blue’ and ‘holding’. On the other hand, ‘blue’ is not' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  15  :  page_content='is not related to ‘boy’.The Transformer architecture uses self-attention by relating every word in the input sequence to every other word.eg. Consider two sentences:The cat drank the milk because it was hungry.The cat drank the milk because it was sweet.In the first sentence, the word ‘it’ refers' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  16  :  page_content='refers to ‘cat’, while in the second it refers to ‘milk. When the model processes the word ‘it’, self-attention gives the model more information about its meaning so that it can associate ‘it’ with the correct word.Dark colors represent higher attention (Image by Author)To enable it to handle more' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  17  :  page_content='more nuances about the intent and semantics of the sentence, Transformers include multiple attention scores for each word.eg. While processing the word ‘it’, the first score highlights ‘cat’, while the second score highlights ‘hungry’. So when it decodes the word ‘it’, by translating it into a' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  18  :  page_content='it into a different language, for instance, it will incorporate some aspect of both ‘cat’ and ‘hungry’ into the translated word.(Image by Author)Training the TransformerThe Transformer works slightly differently during Training and while doing Inference.Let’s first look at the flow of data during' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  19  :  page_content='during Training. Training data consists of two parts:The source or input sequence (eg. “You are welcome” in English, for a translation problem)The destination or target sequence (eg. “De nada” in Spanish)The Transformer’s goal is to learn how to output the target sequence, by using both the input' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  20  :  page_content='the input and target sequence.(Image by Author)The Transformer processes the data like this:The input sequence is converted into Embeddings (with Position Encoding) and fed to the Encoder.The stack of Encoders processes this and produces an encoded representation of the input sequence.The target' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  21  :  page_content='target sequence is prepended with a start-of-sentence token, converted into Embeddings (with Position Encoding), and fed to the Decoder.The stack of Decoders processes this along with the Encoder stack’s encoded representation to produce an encoded representation of the target sequence.The Output' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  22  :  page_content='Output layer converts it into word probabilities and the final output sequence.The Transformer’s Loss function compares this output sequence with the target sequence from the training data. This loss is used to generate gradients to train the Transformer during back-propagation.InferenceDuring' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  23  :  page_content='Inference, we have only the input sequence and don’t have the target sequence to pass as input to the Decoder. The goal of the Transformer is to produce the target sequence from the input sequence alone.So, like in a Seq2Seq model, we generate the output in a loop and feed the output sequence from' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  24  :  page_content='from the previous timestep to the Decoder in the next timestep until we come across an end-of-sentence token.The difference from the Seq2Seq model is that, at each timestep, we re-feed the entire output sequence generated thus far, rather than just the last word.Inference flow, after first timestep' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  25  :  page_content='timestep (Image by Author)The flow of data during Inference is:The input sequence is converted into Embeddings (with Position Encoding) and fed to the Encoder.The stack of Encoders processes this and produces an encoded representation of the input sequence.Instead of the target sequence, we use an' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  26  :  page_content='we use an empty sequence with only a start-of-sentence token. This is converted into Embeddings (with Position Encoding) and fed to the Decoder.The stack of Decoders processes this along with the Encoder stack’s encoded representation to produce an encoded representation of the target sequence.The' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  27  :  page_content='Output layer converts it into word probabilities and produces an output sequence.We take the last word of the output sequence as the predicted word. That word is now filled into the second position of our Decoder input sequence, which now contains a start-of-sentence token and the first word.Go' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  28  :  page_content='word.Go back to step #3. As before, feed the new Decoder sequence into the model. Then take the second word of the output and append it to the Decoder sequence. Repeat this until it predicts an end-of-sentence token. Note that since the Encoder sequence does not change for each iteration, we do not' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  29  :  page_content='we do not have to repeat steps #1 and #2 each time (Thanks to Michal Kučírka for pointing this out).Teacher ForcingThe approach of feeding the target sequence to the Decoder during training is known as Teacher Forcing. Why do we do this and what does that term mean?During training, we could have' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  30  :  page_content='have used the same approach that is used during inference. In other words, run the Transformer in a loop, take the last word from the output sequence, append it to the Decoder input and feed it to the Decoder for the next iteration. Finally, when the end-of-sentence token is predicted, the Loss' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  31  :  page_content='the Loss function would compare the generated output sequence to the target sequence in order to train the network.Not only would this looping cause training to take much longer, but it also makes it harder to train the model. The model would have to predict the second word based on a potentially' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  32  :  page_content='erroneous first predicted word, and so on.Instead, by feeding the target sequence to the Decoder, we are giving it a hint, so to speak, just like a Teacher would. Even though it predicted an erroneous first word, it can instead use the correct first word to predict the second word so that those' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  33  :  page_content='those errors don’t keep compounding.In addition, the Transformer is able to output all the words in parallel without looping, which greatly speeds up training.What are Transformers used for?Transformers are very versatile and are used for most NLP tasks such as language models and text' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  34  :  page_content='and text classification. They are frequently used in sequence-to-sequence models for applications such as Machine Translation, Text Summarization, Question-Answering, Named Entity Recognition, and Speech Recognition.There are different flavors of the Transformer architecture for different problems.' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  35  :  page_content='problems. The basic Encoder Layer is used as a common building block for these architectures, with different application-specific ‘heads’ depending on the problem being solved.Transformer Classification architectureA Sentiment Analysis application, for instance, would take a text document as input.' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  36  :  page_content='as input. A Classification head takes the Transformer’s output and generates predictions of the class labels such as a positive or negative sentiment.(Image by Author)Transformer Language Model architectureA Language Model architecture would take the initial part of an input sequence such as a text' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  37  :  page_content='as a text sentence as input, and generate new text by predicting sentences that would follow. A Language Model head takes the Transformer’s output and generates a probability for every word in the vocabulary. The highest probability word becomes the predicted output for the next word in the' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  38  :  page_content='in the sentence.(Image by Author)How are they better than RNNs?RNNs and their cousins, LSTMs and GRUs, were the de facto architecture for all NLP applications until Transformers came along and dethroned them.RNN-based sequence-to-sequence models performed well, and when the Attention mechanism was' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  39  :  page_content='was first introduced, it was used to enhance their performance.However, they had two limitations:It was challenging to deal with long-range dependencies between words that were spread far apart in a long sentence.They process the input sequence sequentially one word at a time, which means that it' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  40  :  page_content='that it cannot do the computation for time-step t until it has completed the computation for time-step t — 1. This slows down training and inference.As an aside, with CNNs, all of the outputs can be computed in parallel, which makes convolutions much faster. However, they also have limitations in' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  41  :  page_content='in dealing with long-range dependencies:In a convolutional layer, only parts of the image (or words if applied to text data) that are close enough to fit within the kernel size can interact with each other. For items that are further apart, you need a much deeper network with many layers.The' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  42  :  page_content='Transformer architecture addresses both of these limitations. It got rid of RNNs altogether and relied exclusively on the benefits of Attention.They process all the words in the sequence in parallel, thus greatly speeding up computation.(Image by Author)The distance between words in the input' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  43  :  page_content='the input sequence does not matter. It is equally good at computing dependencies between adjacent words and words that are far apart.Now that we have a high-level idea of what a Transformer is, we can go deeper into its internal functionality in the next article to understand the details of how it' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  44  :  page_content='of how it works.And finally, if you liked this article, you might also enjoy my other series on Audio Deep Learning, Geolocation Machine Learning, and Image Caption architectures.Audio Deep Learning Made Simple (Part 1): State-of-the-Art TechniquesA Gentle Guide to the world of disruptive deep' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  45  :  page_content='deep learning audio applications and architectures. And why we all need to…towardsdatascience.comLeveraging Geolocation Data for Machine Learning: Essential TechniquesA Gentle Guide to Feature Engineering and Visualization with Geospatial data, in Plain Englishtowardsdatascience.comImage Captions' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  46  :  page_content='Captions with Deep Learning: State-of-the-Art ArchitecturesA Gentle Guide to Image Feature Encoders, Sequence Decoders, Attention, and Multi-modal Architectures, in Plain Englishtowardsdatascience.comLet’s keep learning!Deep LearningMachine LearningNLPData ScienceArtificial' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n",
            "Chunk  47  :  page_content='Intelligence----23FollowWritten by Ketan Doshi5.9K Followers·Writer for Towards Data ScienceMachine Learning and Big DataFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams' metadata={'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science', 'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = Chroma(\n",
        "    collection_name=\"transformers_rag\",\n",
        "    embedding_function=embed,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "vector_store.add_documents(documents=all_splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEz_eez5YSic",
        "outputId": "cf273d0c-fbd9-4813-8a69-0e5557977737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f421138d-5ef5-49b4-a287-7185826fbc2b',\n",
              " 'e00947b2-2d14-4422-90f5-6f48a6abb6a0',\n",
              " '4a7f75ec-a867-47f8-99c6-fe03a6c8a442',\n",
              " 'a1e5a611-1ea9-4877-bba6-a63e932e45a2',\n",
              " 'd761e594-478f-4d02-a266-2aedc7874d3c',\n",
              " '722f97bc-dd39-4c91-b1c8-c363902aa1d4',\n",
              " 'f3e80584-3a63-438a-b44f-1c5a40d1e043',\n",
              " '17441f7f-1e14-4daa-a15d-a0f0479797f8',\n",
              " 'b36f4505-7874-44de-a030-98f0a46fd9ed',\n",
              " 'a63f7bde-6e33-41a1-a29c-b0c65e6c421a',\n",
              " 'dbe3a7c2-d18a-47a0-84d3-92a2f36a09fb',\n",
              " 'e8249395-e1ae-45c2-ac62-d993a8abd793',\n",
              " 'dd8bda96-6636-42f0-9151-50e33ea5cc5b',\n",
              " '936800d2-e136-4b37-ae73-c8e7bff60e27',\n",
              " '09a698eb-82c8-4c1d-9e3a-7716f9e4e2c3',\n",
              " '2f38a3e7-d237-42c1-af5a-fb800b5a6de5',\n",
              " '5af1d7ee-e073-46cc-ac27-b274e572f496',\n",
              " '885a7738-8f23-4e3e-ba2e-8d55818e9a06',\n",
              " '3651e25e-6501-4542-bcbb-bbbcc5cd7d69',\n",
              " '42700d9f-8e0b-437c-aef3-af5af13bdd48',\n",
              " '3841f854-f8ea-4b8e-bfb7-a2492b2c4ace',\n",
              " '5bfeb28a-d7ca-4e05-93dd-f689564395e0',\n",
              " 'ba318cde-2412-47bb-bfcf-3e46210b145d',\n",
              " '0e2cda30-b713-4436-9215-2fcee9207e4d',\n",
              " 'b3d287c2-8143-4e4e-a355-36978ce9256c',\n",
              " '95c8259b-62dd-488d-b283-f20665a061d2',\n",
              " '2a7ab971-20d1-4b40-a419-14c4f98bcb6e',\n",
              " '00becaa3-d844-466d-917a-6b50912cfb3a',\n",
              " '4964c081-04b2-446c-b52d-e1aaa4a49c88',\n",
              " '4721e24a-7475-4f9e-91ef-7b2a22f45078',\n",
              " '3d626b69-5c96-4a90-858d-3a4da7ba1f5d',\n",
              " '49d74aed-daef-4b80-9189-d9e8633129b8',\n",
              " '200b4cba-a45e-4e68-94b6-59df9f7c4583',\n",
              " 'd9fd6a3b-714b-4018-a501-305a43a7c9de',\n",
              " 'b5957538-bf53-426e-9e83-08a5d3aa106c',\n",
              " '828fb42f-1f67-4787-9e05-95f97e1ce52f',\n",
              " 'db94eb0d-9a50-474e-b797-2caf4261fb77',\n",
              " '57e6505f-eb30-407a-b170-47ffec552ce4',\n",
              " '9a98e1b5-edc4-4b27-b141-9c977a98129b',\n",
              " '72c019ce-b500-4bbc-aa6d-b8578f2b0a50',\n",
              " '1f76c4f2-895b-4bce-b34f-b0c4ab3f3d99',\n",
              " '39119464-12ee-41b1-947d-f49f40c04340',\n",
              " '8cfdc5f9-4ed1-41fe-b75e-5c04fc2e21de',\n",
              " 'add8849a-05dd-4ce8-81ac-fa6eab0ab6b8',\n",
              " '1076c0a9-50d4-482c-bd50-5f6497456610',\n",
              " '3753b9c5-e1ff-49e8-b04a-9af6ce3abe76',\n",
              " '70c20fa9-a2f7-46d5-8a5b-691e85cc04a1',\n",
              " '3e11e10a-1200-4211-86c5-0fbd21067b49']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrevier = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "ret_docs = retrevier.get_relevant_documents(\"Explain Transformer Language Model architecture.\")"
      ],
      "metadata": {
        "id": "wFfSsaA4afED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ret_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLM1uAIrb8ZR",
        "outputId": "de1682a7-cca7-4b0a-c3fa-9809ebee1871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en', 'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science'}, page_content='of the class labels such as a positive or negative sentiment.(Image by Author)Transformer Language Model architectureA Language Model architecture would take the initial part of an input sequence such as a text sentence as input, and generate new text by predicting sentences that would follow. A'),\n",
              " Document(metadata={'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en', 'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science'}, page_content='as input. A Classification head takes the Transformer’s output and generates predictions of the class labels such as a positive or negative sentiment.(Image by Author)Transformer Language Model architectureA Language Model architecture would take the initial part of an input sequence such as a text'),\n",
              " Document(metadata={'description': 'A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.', 'language': 'en', 'source': 'https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452', 'title': 'Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science'}, page_content='as language models and text classification. They are frequently used in sequence-to-sequence models for applications such as Machine Translation, Text Summarization, Question-Answering, Named Entity Recognition, and Speech Recognition.There are different flavors of the Transformer architecture for')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(ret_docs)):\n",
        "    print(\"Ret \",i,\" : \",ret_docs[i].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbDvF695bN48",
        "outputId": "1da84750-dd14-4c10-9150-48040ec9c60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ret  0  :  of the class labels such as a positive or negative sentiment.(Image by Author)Transformer Language Model architectureA Language Model architecture would take the initial part of an input sequence such as a text sentence as input, and generate new text by predicting sentences that would follow. A\n",
            "Ret  1  :  as input. A Classification head takes the Transformer’s output and generates predictions of the class labels such as a positive or negative sentiment.(Image by Author)Transformer Language Model architectureA Language Model architecture would take the initial part of an input sequence such as a text\n",
            "Ret  2  :  as language models and text classification. They are frequently used in sequence-to-sequence models for applications such as Machine Translation, Text Summarization, Question-Answering, Named Entity Recognition, and Speech Recognition.There are different flavors of the Transformer architecture for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "rag_chain = (\n",
        "    {\"context\": retrevier, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | ChatOpenAI(model='gpt-4o',temperature=0)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "rag_chain.invoke(\"Explain Transformer Language Model architecture.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Snjjt9Veb1qM",
        "outputId": "151841ed-9fe5-4e36-9be7-f7e9debf4468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Transformer Language Model architecture takes an initial part of an input sequence, such as a text sentence, and generates new text by predicting subsequent sentences. It is frequently used in applications like machine translation, text summarization, and question-answering. The architecture leverages attention mechanisms to improve performance over traditional RNNs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG on a PDF"
      ],
      "metadata": {
        "id": "kawRwW-vejj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQhehFwketRR",
        "outputId": "181d375e-4825-4875-b905-d26373b4c5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/hybridrag.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "0L9PzUWNeTtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pages)):\n",
        "    print(\"Page \",i,\" : \",pages[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WEoK8FAe6Bk",
        "outputId": "ea8e6e20-efe9-4f61-86dd-93d1f1d8dd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page  0  :  page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval\n",
            "Augmented Generation for Efficient Information Extraction\n",
            "Bhaskarjit Sarmah\n",
            "bhaskarjit.sarmah@blackrock.com\n",
            "BlackRock, Inc.\n",
            "Gurugram, IndiaBenika Hall\n",
            "bhall@nvidia.com\n",
            "NVIDIA\n",
            "Santa Clara, CA, USARohan Rao\n",
            "rohrao@nvidia.com\n",
            "NVIDIA\n",
            "Santa Clara, CA, USA\n",
            "Sunil Patel\n",
            "supatel@nvidia.com\n",
            "NVIDIA\n",
            "Santa Clara, CA, USAStefano Pasquali\n",
            "stefano.pasquali@blackrock.com\n",
            "BlackRock, Inc.\n",
            "New York, NY, USADhagash Mehta\n",
            "dhagash.mehta@blackrock.com\n",
            "BlackRock, Inc.\n",
            "New York, NY, USA\n",
            "ABSTRACT\n",
            "Extraction and interpretation of intricate information from unstruc-\n",
            "tured text data arising in financial applications, such as earnings\n",
            "call transcripts, present substantial challenges to large language\n",
            "models (LLMs) even using the current best practices to use Re-\n",
            "trieval Augmented Generation (RAG) (referred to as VectorRAG\n",
            "techniques which utilize vector databases for information retrieval)\n",
            "due to challenges such as domain specific terminology and complex\n",
            "formats of the documents. We introduce a novel approach based\n",
            "on a combination, called HybridRAG , of the Knowledge Graphs\n",
            "(KGs) based RAG techniques (called GraphRAG) and VectorRAG\n",
            "techniques to enhance question-answer (Q&A) systems for infor-\n",
            "mation extraction from financial documents that is shown to be\n",
            "capable of generating accurate and contextually relevant answers.\n",
            "Using experiments on a set of financial earning call transcripts\n",
            "documents which come in the form of Q&A format, and hence\n",
            "provide a natural set of pairs of ground-truth Q&As, we show that\n",
            "HybridRAG which retrieves context from both vector database and\n",
            "KG outperforms both traditional VectorRAG and GraphRAG indi-\n",
            "vidually when evaluated at both the retrieval and generation stages\n",
            "in terms of retrieval accuracy and answer generation. The proposed\n",
            "technique has applications beyond the financial domain.\n",
            "1 INTRODUCTION\n",
            "For the financial analyst, it is crucial to extract and analyze infor-\n",
            "mation from unstructured data sources like news articles, earnings\n",
            "reports, and other financial documents to have at least some chance\n",
            "to be on the better side of potential information asymmetry. These\n",
            "sources hold valuable insights that can impact investment decisions,\n",
            "market predictions, and overall sentiment. However, traditional\n",
            "data analysis methods struggle to effectively extract and utilize\n",
            "this information due to its unstructured nature. Large language\n",
            "models (LLMs) [ 1–4] have emerged as powerful tools for financial\n",
            "services and investment management. Their ability to process and\n",
            "understand vast amounts of textual data makes them invaluable\n",
            "for tasks such as sentiment analysis, market trend predictions, and\n",
            "automated report generation. Specifically, extracting information\n",
            "from annual reports and other financial documents can greatly en-\n",
            "hance the efficiency and accuracy of financial analysts [ 5]. A robust\n",
            "information extraction system can help analysts quickly gather\n",
            "relevant data, identify market trends, and make informed decisions,\n",
            "leading to better investment strategies and risk management [6].Although LLMs have substantial potential in financial applica-\n",
            "tions, there are notable challenges in using pre-trained models to\n",
            "extract information from financial documents outside their training\n",
            "data while also reducing hallucination [ 7,8]. Financial documents\n",
            "typically contain domain-specific language, multiple data formats,\n",
            "and unique contextual relationships that general purpose-trained\n",
            "LLMs do not handle well. In addition, extracting consistent and\n",
            "coherent information from multiple financial documents can be\n",
            "challenging due to variations in terminology, format, and context\n",
            "across different textual sources. The specialized terminology and\n",
            "complex data formats in financial documents make it difficult for\n",
            "models to extract meaningful insights, in turn, causing inaccurate\n",
            "predictions, overlooked insights, and unreliable analysis, which' metadata={'source': '/content/hybridrag.pdf', 'page': 0}\n",
            "Page  1  :  page_content='complex data formats in financial documents make it difficult for\n",
            "models to extract meaningful insights, in turn, causing inaccurate\n",
            "predictions, overlooked insights, and unreliable analysis, which\n",
            "ultimately hinder the ability to make well-informed decisions.\n",
            "Current approaches to mitigate these issues include various\n",
            "Retrieval-Augmented Generation (RAG) techniques [ 9], which aim\n",
            "to improve the performance of LLMs by incorporating relevant\n",
            "retrieval techniques. VectorRAG (the traditional RAG techniques\n",
            "that are based on vector databases) focuses on improving Natural\n",
            "Language Processing (NLP) tasks by retrieving relevant textual\n",
            "information to support the generation tasks. VectorRAG excels in\n",
            "situations where context from related textual documents is crucial\n",
            "for generating meaningful and coherent responses [ 9–11]. RAG-\n",
            "based methods ensure the LLMs generate relevant and coherent\n",
            "responses that are aligned with the original input query. How-\n",
            "ever, for financial documents, these approaches have significant\n",
            "challenges as a standalone solution. For instance, traditional RAG\n",
            "systems often use paragraph-level chunking techniques, which\n",
            "assume the text in those documents are uniform in length. This\n",
            "approach neglects the hierarchical nature of financial statements\n",
            "and can result in the loss of critical contextual information for an\n",
            "accurate analysis[ 12,13]. Due to the complexities in analyzing fi-\n",
            "nancial documents, the quality of the LLM retrieved-context from\n",
            "a vast and heterogeneous corpus can be inconsistent, leading to\n",
            "inaccuracies and incomplete analyses. These challenges demon-\n",
            "strate the need for more sophisticated methods that can effectively\n",
            "integrate and process the detailed and domain-specific information\n",
            "found in financial documents, ensuring more reliable and accurate\n",
            "results for informed decision-making.\n",
            "Knowledge graphs (KGs) [ 14] may provide a different point of\n",
            "view to looking at the financial documents where the documentsarXiv:2408.04948v1  [cs.CL]  9 Aug 2024' metadata={'source': '/content/hybridrag.pdf', 'page': 0}\n",
            "Page  2  :  page_content='Sarmah et al.\n",
            "are viewed as a collection of triplets of entities and their relation-\n",
            "ships as depicted in the text of the documents. KGs have become\n",
            "a pivotal technology in data management and analysis, provid-\n",
            "ing a structured way to represent knowledge through entities and\n",
            "relationships and have been widely adopted in various domains,\n",
            "including search engines, recommendation systems, and biomedical\n",
            "research [ 15–17]. The primary advantage of KGs lies in their ability\n",
            "to offer a structured representation, which facilitates efficient query-\n",
            "ing and reasoning. However, building and maintaining KGs and\n",
            "integrating data from different sources, such as documents, news\n",
            "articles, and other external sources, into a coherent knowledge\n",
            "graph poses significant challenges.\n",
            "The financial services industry has recognized the potential of\n",
            "KGs in enhancing data integration of heterogeneous data sources,\n",
            "risk management, and predictive analytics [ 18?–21]. Financial KGs\n",
            "integrate various financial data sources such as market data, fi-\n",
            "nancial reports, and news articles, creating a comprehensive view\n",
            "of financial entities and their relationships. This unified view im-\n",
            "proves the accuracy and comprehensiveness of financial analysis,\n",
            "facilitates risk management by identifying hidden relationships,\n",
            "and supports advanced predictive analytics for more accurate mar-\n",
            "ket predictions and investment decisions. However, handling large\n",
            "volumes of financial data and continuously updating the knowledge\n",
            "graph to reflect the dynamic nature of financial markets can be\n",
            "challenging and resource-intensive.\n",
            "GraphRAG (Graph-based Retrieval-Augmented Generation) [ 22–\n",
            "27] is a novel approach that leverages knowledge graphs (KGs) to\n",
            "enhance the performance of NLP tasks such as Q&A systems. By\n",
            "integrating KGs with RAG techniques, GraphRAG enables more\n",
            "accurate and context-aware generation of responses based on the\n",
            "structured information extracted from financial documents. But\n",
            "GraphRAG generally underperforms in abstractive Q&A tasks or\n",
            "when there is not explicit entity mentioned in the question.\n",
            "In the present work, we propose a combination of VectorRAG and\n",
            "GraphRAG, called HybridRAG, to retrieve the relevant information\n",
            "from external documents for a given query to the LLM that brings\n",
            "advantages of both the RAGs together to provide demonstrably\n",
            "more accurate answers to the queries.\n",
            "1.1 Prior Work and Our Contribution\n",
            "VectorRAG has been extensively investigated in the recent years\n",
            "and focuses on enhancing NLP tasks by retrieving relevant textual\n",
            "information to support generation processes [ 9–11,26]. However,\n",
            "the effectiveness of the retrieval mechanism across multiple docu-\n",
            "ments and longer contexts poses a significant challenge in extract-\n",
            "ing relevant responses. GraphRAG combines the capabilities of KGs\n",
            "with RAG to improve traditional NLP tasks [ 23–25]. Within our\n",
            "implementations of both VectorRAG GraphRAG techniques, we\n",
            "explicitly add information on the metadata of the documents that\n",
            "is also shown to improve the performance of VectorRAG [8].\n",
            "To the best of our knowledge the present work is the first work\n",
            "that proposes a RAG approach that is hybrid of both VectorRAG\n",
            "and GraphRAG and demonstrates its potential of more effective\n",
            "analysis and utilization of financial documents by leveraging the\n",
            "combined strengths of VectorRAG and GraphRAG. We also utilize a\n",
            "novel ground-truth Q&A dataset extracted from publicly availablefinancial call transcripts of the companies included in the Nifty-50\n",
            "index which is an Indian stock market index that represents the\n",
            "weighted average of 50 of the largest Indian companies listed on\n",
            "the National Stock Exchange1.\n",
            "2 METHODOLOGY\n",
            "In this Section, we provide details of the proposed methodology\n",
            "by first discussing details of VectorRAG, then methodologies of\n",
            "constructing KGs from given documents and our proposed method-\n",
            "ology of GraphRAG and finally the HybridGraph technique.\n",
            "2.1 VectorRAG' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Page  3  :  page_content='by first discussing details of VectorRAG, then methodologies of\n",
            "constructing KGs from given documents and our proposed method-\n",
            "ology of GraphRAG and finally the HybridGraph technique.\n",
            "2.1 VectorRAG\n",
            "The traditional RAG [ 9] process begins with a query that is related\n",
            "to the information possessed within external document(s) that are\n",
            "not a part of the training dataset for the LLM. This query is used to\n",
            "search an external repository, such as a vector database or indexed\n",
            "corpus, to fetch relevant documents or passages containing useful\n",
            "information. These retrieved documents are then fed back into the\n",
            "LLM as additional context. Hence, in turn, for the given query, the\n",
            "language model generates a response based not only on its inter-\n",
            "nal training data but also by incorporating the retrieved external\n",
            "information. This integration ensures that the generated content is\n",
            "grounded in more recent and verifiable data, improving the accu-\n",
            "racy and contextual relevance of the responses. By combining the\n",
            "retrieval of external information with the generative capabilities\n",
            "of large language models, RAG enhances the overall quality and\n",
            "reliability of the generated text.\n",
            "In traditional VectorRAG, the given external documents are di-\n",
            "vided into multiple chunks because of the limitation of context size\n",
            "of the language model. Those chunks are converted into embed-\n",
            "dings using an embeddings model and then stored into a vector\n",
            "database. After that, the retrieval component performs a similarity\n",
            "search within the vector database to identify and rank the chunks\n",
            "most relevant to the query. The top-ranked chunks are retrieved\n",
            "and aggregated to provide context for the generative model.\n",
            "Then, the generative model takes this retrieved context along\n",
            "with the original query and synthesizes a response. Thus, it merges\n",
            "the real-time information from the retrieved chunks with its pre-\n",
            "existing knowledge, ensuring that the response is both contextually\n",
            "relevant and detailed.\n",
            "The schematic diagram in Figure 1 provides details on the part\n",
            "of RAG that generates vector database from given external docu-\n",
            "ments in the traditional VectorRAG methodology where we also\n",
            "include explicit reference of metadata [8]. Section 4.2 will provide\n",
            "implementation details for our experiments.\n",
            "2.2 Knowledge Graph Construction\n",
            "A KG is a structured representation of real-world entities, their\n",
            "attributes, and their relations, usually stored in a graph database\n",
            "or a triplet store, i.e., a KG consists of nodes that represent entities\n",
            "and edges that represent relations, as well as labels and attributes\n",
            "for both. A graph triplet is a basic unit of information in a KG,\n",
            "consisting of a subject, a predicate, and an object.\n",
            "In most methodologies to build a KG from given documents,\n",
            "three main steps are involved: knowledge extraction, knowledge\n",
            "1https://www.nseindia.com/products/content/equities/indices/nifty_50.htm' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Page  4  :  page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\n",
            "Figure 1: A schematic diagram describing the vector database\n",
            "creation of a RAG application.\n",
            "improvement, and knowledge adaptation [ 28]. Here, we do not use\n",
            "knowledge adaptation and treat the KGs as static graphs.\n",
            "Knowledge Extraction:- This step aims to extract structured in-\n",
            "formation from unstructured or semi-structured data, such as text,\n",
            "databases, and existing ontologies. The main tasks in this step are\n",
            "entity recognition, relationship extraction, and co-reference reso-\n",
            "lution. Entity recognition and relationship extraction techniques\n",
            "use typical NLP tasks to identify entities and their relationships\n",
            "from textual sources [ 29]. Coreference resolution identifies and\n",
            "connects different references of the same entity, keeping coherence\n",
            "within the knowledge graph. For example, if the text refers to a\n",
            "company as both \"the company\" and \"it\", coreference resolution\n",
            "can link these mentions to the same entity node in the graph.\n",
            "Knowledge Improvement:- This step aims to enhance the quality\n",
            "and completeness of the KG by removing redundancies and address-\n",
            "ing gaps in the extracted information. The primary tasks in this\n",
            "step are KG completion and fusion. KG completion technique infers\n",
            "missing entities and relationships within the graph using meth-\n",
            "ods such as link prediction and entity resolution. Link prediction\n",
            "predicts the existence and type of a relation between two entities\n",
            "based on the graph structure and features, while entity resolution\n",
            "matches and merges different representations of the same entity\n",
            "from different sources.\n",
            "Knowledge fusion combines information from multiple sources\n",
            "to create a coherent and unified KG. This involves resolving conflicts\n",
            "and redundancies among the sources, such as contradictory or\n",
            "duplicate facts, and aggregating or reconciling the information\n",
            "based on rules, probabilities, or semantic similarity.\n",
            "We utilized a robust methodology for creating KG triplets from\n",
            "unstructured text data, specifically focusing on corporate docu-\n",
            "ments such as earnings call transcripts, adapted from Ref. [ 18?].\n",
            "This process involves several interconnected stages, each designed\n",
            "to extract, refine, and structure information effectively.\n",
            "We implement a two-tiered LLM chain for content refinement\n",
            "and information extraction. The first tier employs an LLM to gen-\n",
            "erate an abstract representation of each document chunk. This\n",
            "refinement process is crucial as it distills the essential information\n",
            "while preserving the original meaning and key relationships be-\n",
            "tween concepts that serves as a more focused input for subsequent\n",
            "processing, enhancing the overall efficiency and accuracy of ourtriplet extraction pipeline. The second tier of our LLM chain is\n",
            "dedicated to entity extraction and relationship identification.\n",
            "Both the steps are executed using carefully performed prompt\n",
            "engineering on a pre-trained LLM. A detailed discussion on imple-\n",
            "mentation of the methodology will be provided in Section 4.1\n",
            "2.3 GraphRAG\n",
            "KG based RAG [ 22], or GraphRAG, also begins with a query based\n",
            "on the user’s input same as VectorRAG. The main difference be-\n",
            "tween VectorRAG and GraphRAG lies in the retrieval part. The\n",
            "query here is now used to search the KG to retrieve relevant nodes\n",
            "(entities) and edges (relationships) related to the query. A subgraph,\n",
            "which consists of these relevant nodes and edges, is extracted from\n",
            "the full KG to provide context. This subgraph is then integrated\n",
            "with the language model’s internal knowledge, by encoding the\n",
            "graph structure into embeddings that the model can interpret. The\n",
            "language model uses this combined context to generate responses\n",
            "that are informed by both the structured information from the KG\n",
            "and its pre-trained knowledge. Crucially, when responding to user\n",
            "queries about a particular company, we leveraged the metadata\n",
            "information to selectively filter and retrieve only those document' metadata={'source': '/content/hybridrag.pdf', 'page': 2}\n",
            "Page  5  :  page_content='and its pre-trained knowledge. Crucially, when responding to user\n",
            "queries about a particular company, we leveraged the metadata\n",
            "information to selectively filter and retrieve only those document\n",
            "segments pertinent to the queried company [ 8]. This integration\n",
            "helps ensure that the generated outputs are accurate, contextually\n",
            "relevant, and grounded in verifiable information.\n",
            "A schematic diagram of the retrieval methodology of GraphRAG\n",
            "is given in Figure 2. Here we first write a prompt to clean the\n",
            "data and then write another prompt in the second stage to create\n",
            "knowledge triplets along with metadata. It will be covered in more\n",
            "detail in section 4.1\n",
            "Figure 2: A schematic diagram describing knowledge graph\n",
            "creation process of GraphRAG.\n",
            "2.3.1 HybridRAG. For the HybridRAG technique, we propose to\n",
            "integrate the aforementioned two distinct RAG techniques: Vec-\n",
            "torRAG and GraphRAG. This integration involves a systematic\n",
            "combination of contextual information retrieved from both the\n",
            "traditional vector-based retrieval mechanism and the KG-based\n",
            "retrieval system, the latter of which was constructed specifically\n",
            "for this study.\n",
            "The amalgamation of these two contexts allows us to leverage the\n",
            "strengths of both approaches. The VectorRAG component provides\n",
            "a broad, similarity-based retrieval of relevant information, while\n",
            "the GraphRAG element contributes structured, relationship-rich' metadata={'source': '/content/hybridrag.pdf', 'page': 2}\n",
            "Page  6  :  page_content='Sarmah et al.\n",
            "contextual data. This combined context is then utilized as input for a\n",
            "LLM to generate the final responses. Details on the implementation\n",
            "of the HybridRAG will be provided in Section 4.4.\n",
            "2.4 Evaluation Metrics\n",
            "To assess the efficacy of this integrated approach, we conducted a\n",
            "comparative analysis among the three approaches in a controlled ex-\n",
            "perimental set up: VectorRAG, GraphRAG and HybridRAG. The re-\n",
            "sponses generated using the combined VectorRAG and GraphRAG\n",
            "contexts were juxtaposed against those produced individually by\n",
            "VectorRAG and GraphRAG. This comparative evaluation aimed to\n",
            "discern potential improvements in response quality, accuracy, and\n",
            "comprehensiveness that might arise from the synergistic integra-\n",
            "tion of these two RAG methodologies.\n",
            "To objectively evaluate different RAG approaches (VectorRAG\n",
            "and GraphRAG in their case), Ref. [ 22] utilized metrics such as\n",
            "comprehensiveness (i.e., the amount of details the answer provides\n",
            "to cover all aspects and details of the question?); diversity (i.e.,\n",
            "the richness of the answer in providing different perspectives and\n",
            "insights on the question); empowerment (i.e., the helpfulness of the\n",
            "answer to the reader understand and make informed judgements\n",
            "about the topic); and, directness (i.e., clearness of the answer in\n",
            "addressing the question). here, the LLM was provided tuples of\n",
            "question, target metric, and a pair of answers, and was asked to\n",
            "assess which answer was better according to the metric and why.\n",
            "These metrics though compare the final generated answers, do\n",
            "not necessarily directly evaluate the retrieval and generation parts\n",
            "separately. Instead, here we implement a comprehensive set of\n",
            "evaluation metrics which are designed to capture different aspects\n",
            "of a given RAG system’s output quality, focusing on faithfulness,\n",
            "answer relevance, and context relevance [ 30]. Each metric provides\n",
            "unique insights into the system’s capabilities and limitations.\n",
            "2.4.1 Faithfulness. Faithfulness is a crucial metric that measures\n",
            "the extent to which the generated answer can be inferred from the\n",
            "provided context. Our implementation of the faithfulness metric\n",
            "involves a two-step process:\n",
            "Statement Extraction:- We use an LLM to decompose the gener-\n",
            "ated answer into a set of concise statements. This step is crucial\n",
            "for breaking down complex answers into more manageable and\n",
            "verifiable units. The prompt used for this step is:\n",
            "\"Given a question and answer, create one or more statements\n",
            "from each sentence in the given answer. question: [question] an-\n",
            "swer: [answer]\".\n",
            "Statement Verification:- For each extracted statement, we employ\n",
            "the LLM to determine if it can be inferred from the given context.\n",
            "This verification process uses the following prompt:\n",
            "\"Consider the given context and following statements, then de-\n",
            "termine whether they are supported by the information present in\n",
            "the context. Provide a brief explanation for each statement before\n",
            "arriving at the verdict (Yes/No). Provide a final verdict for each\n",
            "statement in order at the end in the given format. Do not deviate\n",
            "from the specified format. statement: [statement 1] ... statement:\n",
            "[statement n]\".\n",
            "The faithfulness score ( 𝐹) is𝐹=|𝑉|/|𝑆|, where |𝑉|is the number\n",
            "of supported statements and |𝑆|is the total number of statements.2.4.2 Answer Relevance:-. The answer relevance metric assesses\n",
            "how well the generated answer addresses the original question,\n",
            "irrespective of factual accuracy. This metric helps identify cases of\n",
            "incomplete answers or responses containing irrelevant information.\n",
            "Our implementation involves the following steps:\n",
            "Question Generation: We prompt the LLM to generate n potential\n",
            "questions based on the given answer:\n",
            "\"Generate a question for the given answer. answer: [answer]\".\n",
            "Then, we obtain embeddings for all generated questions and the\n",
            "original question using OpenAI’s text-embedding-ada-002 model2.\n",
            "We then calculate the cosine similarity between each generated' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Page  7  :  page_content='Then, we obtain embeddings for all generated questions and the\n",
            "original question using OpenAI’s text-embedding-ada-002 model2.\n",
            "We then calculate the cosine similarity between each generated\n",
            "question’s embedding and the original question’s embedding.\n",
            "Finally, the answer relevance score (AR) is computed as the aver-\n",
            "age similarity across all generated questions: 𝐴𝑅=1\n",
            "𝑛Í(𝑠𝑖𝑚(𝑞,𝑞𝑖)),\n",
            "where𝑠𝑖𝑚(𝑞,𝑞𝑖)is the cosine similarity between the embedding\n",
            "of the original question 𝑞and the embeddings of each of the 𝑛\n",
            "generated questions 𝑞𝑖.\n",
            "2.4.3 Context Precision. It is a metric used to evaluate the relevance\n",
            "of retrieved context chunks in relation to a specified ground truth\n",
            "for a given question3. It calculates the proportion of relevant items\n",
            "that appear in the top ranks of the context. The formula for context\n",
            "precision at K is the sum of the products of precision at each rank\n",
            "k and a binary relevance indicator v_k, divided by the total number\n",
            "of relevant items in the top K results. Precision at each rank k is\n",
            "determined by the ratio of true positives at k to the sum of true\n",
            "positives and false positives at k. This metric helps in assessing\n",
            "how well the context supports the ground truth, aiming for higher\n",
            "scores which indicate better precision.\n",
            "2.4.4 Context Recall. It is a metric used to evaluate how well the\n",
            "retrieved context aligns with the ground truth answer, which is\n",
            "considered the definitive correct response4. It is quantified by com-\n",
            "paring each sentence in the ground truth answer to see if it can\n",
            "be traced back to the retrieved context. The formula for context\n",
            "recall is the ratio of the number of ground truth sentences that can\n",
            "be attributed to the context to the total number of sentences in\n",
            "the ground truth. Higher values, ranging from 0 to 1, indicate bet-\n",
            "ter alignment and thus better context recall. This metric is crucial\n",
            "for assessing the effectiveness of information retrieval systems in\n",
            "providing relevant context.\n",
            "3 DATA DESCRIPTION\n",
            "Although there do exist some public financial datasets, none of them\n",
            "were suitable for the present experiments: e.g., FinQA [ 31], TAT-\n",
            "QA [ 32], FIQA [ 33], FinanceBench [ 34], etc. datasets are limited to\n",
            "specific usecases such as benchmarking LLMs’ abilities to perform\n",
            "complex numerical reasoning or sentiment analysis. On the other\n",
            "hand, FinTextQA [ 35] dataset was not publicly available at the\n",
            "time of writing the present work. In addition, in most of these\n",
            "datasets, access to the actual documents from which the ground-\n",
            "truth Q&As were created is not available, making it impossible to\n",
            "use them for our RAG techniques evaluation purposes. Hence, we\n",
            "resorted to a dataset of our own though through publicly available\n",
            "2https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
            "3https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html\n",
            "4https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Page  8  :  page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\n",
            "documents but such that we finally have access to both the actual\n",
            "financial documents and the ground-truth Q&As. Datasets like\n",
            "FinanceBench5provides question-context-answer triplets but they\n",
            "are not useful as here we are comparing VectorRAG, GraphRAG\n",
            "and HybridRAG and they do not provide the context generated\n",
            "from a KG. A recent paper [ 22] has not made the KG and triplets\n",
            "constructed by their algorithm public to the best of our knowledge\n",
            "either.\n",
            "In short, there is no publicly available benchmark dataset to\n",
            "compare VectorRAG and GraphRAG either for financial or general\n",
            "domains to the best of our knowledge. Hence, we had to rely on\n",
            "our own dataset constructed as explained below.\n",
            "We used transcripts from earnings calls of Nifty 50 constituents\n",
            "for our analysis. The NIFTY 50 is popular index in the Indian stock\n",
            "market that represents the weighted average of 50 of the largest\n",
            "Indian companies listed on the National Stock Exchange (NSE).\n",
            "The dataset of the earning call documents of Nifty 50 companies is\n",
            "widely recognized in the investment realm and is esteemed as an\n",
            "authoritative and extensive collection of earnings call transcripts.\n",
            "In our investigation, we focus on data spanning the quarter ending\n",
            "in June, 2023 i.e. the earnings reports for Q1 of the financial year\n",
            "2024 (A financial year in India starts on the 1st April and ends in\n",
            "31st March, so the quarter from 1st April to 30th June is the first\n",
            "quarter of 2024 for the Indian market).\n",
            "Our dataset encompasses 50 transcripts for this quarter, span-\n",
            "ning over 50 companies within Nifty 50 universe from diverse\n",
            "range of sectors including Infrastructure, Healthcare, Consumer\n",
            "Durables, Banking, Automobile, Financial Services, Energy - Oil\n",
            "& Gas, Telecommunication, Consumer Goods, Pharmaceuticals,\n",
            "Energy - Coal, Materials, Information Technology, Construction,\n",
            "Diversified, Metals, Energy - Power and Chemicals providing a\n",
            "substantial and diverse foundation for our study.\n",
            "We start the data collection process focused on acquiring earn-\n",
            "ings reports from company websites within the Nifty 50 universe by\n",
            "developing and deploying a custom web scraping tool to navigate\n",
            "through the websites of each company within the Nifty 50 index,\n",
            "systematically retrieving the pertinent earnings reports for Q1 of\n",
            "the financial year 2024. By utilizing this web scraping approach,\n",
            "we aimed to compile a comprehensive dataset encompassing the\n",
            "earnings reports of the constituent companies.\n",
            "Table 1 summarizes basic statistics of the documents we will be\n",
            "experimenting with in the remainder of this work.\n",
            "Number of companies/documents 50\n",
            "Average number of pages 27\n",
            "Average number of questions 16\n",
            "Average number of tokens 60,000\n",
            "Table 1: Summary Statistics for the call transcript documents\n",
            "used in the present work.\n",
            "These call transcripts documents consist of questions and an-\n",
            "swers between financial analysts and the company representatives\n",
            "for the respective companies, hence, there already exist certain\n",
            "Q&A pairs within these documents along with additional text. We\n",
            "examined the earnings reports within the Nifty50 universe, system-\n",
            "atically curated a comprehensive array of randomly selected 400\n",
            "5https://huggingface.co/datasets/PatronusAI/financebench-testquestions posed during the earnings calls from all the documents,\n",
            "and gathered the exact responses corresponding to these questions.\n",
            "These questions constitute the specific queries articulated by finan-\n",
            "cial analysts to the management during these calls.\n",
            "4 IMPLEMENTATION DETAILS\n",
            "In this Section, we provide details of implementation of the pro-\n",
            "posed methodology.\n",
            "4.1 Knowledge Graph Construction\n",
            "The initial phase of our approach centers on document preprocess-\n",
            "ing. We utilize the PyPDFLoader6to import PDF documents, which\n",
            "are subsequently segmented into manageable chunks using the\n",
            "RecursiveCharacterTextSplitter. This chunking strategy employs a' metadata={'source': '/content/hybridrag.pdf', 'page': 4}\n",
            "Page  9  :  page_content='ing. We utilize the PyPDFLoader6to import PDF documents, which\n",
            "are subsequently segmented into manageable chunks using the\n",
            "RecursiveCharacterTextSplitter. This chunking strategy employs a\n",
            "size of 2024 characters with an overlap of 204 characters, ensuring\n",
            "comprehensive coverage while maintaining context across segment\n",
            "boundaries.\n",
            "Following the preprocessing stage, we implement the two-tiered\n",
            "language model chain for content refinement and information ex-\n",
            "traction. It is not possible to include the exact prompt here due to\n",
            "the limited space, but a baseline prompt can be found in Ref. [18].\n",
            "Entity Type Examples\n",
            "Companies and Corpo-\n",
            "rationsOfficial names, abbreviations, infor-\n",
            "mal references\n",
            "Financial Metrics and\n",
            "IndicatorsRevenue, profit margins, EBITDA\n",
            "Corporate Executives\n",
            "and Key PersonnelCEOs, CFOs, board members\n",
            "Products and Services Tangible products and intangible ser-\n",
            "vices\n",
            "Geographical Locations Headquarters, operational regions,\n",
            "markets\n",
            "Corporate Events Mergers, acquisitions, product\n",
            "launches, earnings calls\n",
            "Legal and Regulatory\n",
            "InformationLegal cases, regulatory compliance\n",
            "Table 2: Entities extracted from earnings call transcripts\n",
            "Table 2 summarizes details on entities extracted from the earn-\n",
            "ing calls transcripts using our prompt based method. Concurrently,\n",
            "LLM identifies relationships between these entities using a curated\n",
            "set of verbs, capturing the nuanced interactions within the cor-\n",
            "porate narrative. A key improvement in our methodology is the\n",
            "enhanced prompt engineering to generate the structured output\n",
            "format for knowledge triplets. Each triplet is represented as a nested\n",
            "list [’h’, ’type’, ’r’, ’o’, ’type’, ’metadata’], where ’h’ and ’o’ denote\n",
            "the head and object entities respectively, ’type’ specifies the entity\n",
            "category, ’r’ represents the relationship, and ’metadata’ encapsu-\n",
            "lates additional contextual information. This format allows for a\n",
            "rich, multidimensional representation of information, facilitating\n",
            "more nuanced downstream analysis.\n",
            "6https://python.langchain.com/v0.1/docs/modules/data_connection/document_\n",
            "loaders/pdf/' metadata={'source': '/content/hybridrag.pdf', 'page': 4}\n",
            "Page  10  :  page_content='Sarmah et al.\n",
            "Our process incorporates several advanced features to enhance\n",
            "the quality and utility of the extracted triplets. Entity disambigua-\n",
            "tion techniques are employed to consolidate different references to\n",
            "the same entity, improving consistency across the KG. We also pri-\n",
            "oritize conciseness in entity representation, aiming for descriptions\n",
            "of less than four words where possible, which aids in maintaining\n",
            "a clean and navigable graph structure.\n",
            "The extraction pipeline is applied iteratively to each document\n",
            "chunk, with results aggregated to form a comprehensive knowledge\n",
            "graph representation of the entire document, allowing for scalable\n",
            "processing of large documents while maintaining local context\n",
            "within each chunk. We have added explicit instruction on obtaining\n",
            "metadata following Ref. [8] for both VectorRAG and GraphRAG.\n",
            "Finally, we implement a data persistence strategy, converting\n",
            "the extracted triplets from their initial string format to Python data\n",
            "structures and storing them in a pickle file. This facilitates easy\n",
            "retrieval and further manipulation of the knowledge graph data in\n",
            "subsequent analysis stages.\n",
            "Our methodology represents a significant advancement in auto-\n",
            "mated knowledge extraction from corporate documents. By com-\n",
            "bining advanced NLP techniques with a structured approach to\n",
            "information representation, we create a rich, queryable knowledge\n",
            "base that captures the complex relationships and key information\n",
            "present in corporate narratives. This approach opens up new pos-\n",
            "sibilities for financial analysis and automated reasoning in the\n",
            "business domain that will be explored further in the future.\n",
            "4.2 VectorRAG\n",
            "Our methodology builds upon the concept of RAG [ 9] which al-\n",
            "lows for the creation of a system that can provide context-aware,\n",
            "accurate responses to queries about company financial informa-\n",
            "tion, leveraging both the power of large language models and the\n",
            "efficiency of semantic search.\n",
            "At the core of our system is a Pinecone vector database7, which\n",
            "serves as the foundation for our information retrieval process. We\n",
            "employ OpenAI’s text-embedding-ada-002 model to transform tex-\n",
            "tual data from earnings call transcripts into high-dimensional vec-\n",
            "tor representations. This vectorization process enables semantic\n",
            "similarity searches, significantly enhancing the relevance and ac-\n",
            "curacy of retrieved information. Table 3 provides summary of the\n",
            "configuration of the set up in use for our experiments.\n",
            "LLM GPT-3.5-Turbo\n",
            "LLM Temperature 0\n",
            "Embedding Model text-embedding-ada-002\n",
            "Framework LangChain\n",
            "Vector Database Pinecone\n",
            "Chunk Size 1024\n",
            "Chunk Overlap 0\n",
            "Maximum Output Tokens 1024\n",
            "Chunks for Similarity Algorithm 20\n",
            "Number of Context Retrieved 4\n",
            "Table 3: VectorRAG Configuration\n",
            "7https://www.pinecone.io/The Q&A pipeline is constructed using the LangChain frame-\n",
            "work8. The begins with a context retrieval step, where we query the\n",
            "Pinecone vector store to obtain the most relevant document chunks\n",
            "for a given question. This retrieval process is fine-tuned with spe-\n",
            "cific filters for quarters, years, and company names, ensuring that\n",
            "the retrieved information is both relevant and current.\n",
            "Following retrieval, we implement a context formatting step that\n",
            "consolidates the retrieved document chunks into a coherent context\n",
            "string. This formatted context serves as the informational basis for\n",
            "the language model’s response generation. We have developed a\n",
            "sophisticated prompt template, that instructs the language model\n",
            "to function as an expert Q&A system, emphasizing the importance\n",
            "of utilizing only the provided context information and avoiding\n",
            "direct references to the context in the generated responses.\n",
            "For the core language processing task, we integrate OpenAI’s\n",
            "GPT-3.5-turbo model which processes the formatted context and\n",
            "query to generate natural language responses that are informative,\n",
            "coherent, and contextually appropriate.\n",
            "To evaluate the performance of our system, we developed a' metadata={'source': '/content/hybridrag.pdf', 'page': 5}\n",
            "Page  11  :  page_content='query to generate natural language responses that are informative,\n",
            "coherent, and contextually appropriate.\n",
            "To evaluate the performance of our system, we developed a\n",
            "comprehensive framework that includes the preparation of a cus-\n",
            "tom dataset of question-answer pairs specific to each company’s\n",
            "earnings call. Our system processes each question in this dataset,\n",
            "generating answers based on the retrieved context. The evaluation\n",
            "results, including the original question, generated answer, retrieved\n",
            "contexts, and ground truth, are compiled into structured formats\n",
            "(CSV and JSON) to facilitate further analysis. The outputs generated\n",
            "by our system are stored in both CSV and JSON formats, enabling\n",
            "easy integration with various analysis tools and dashboards. This\n",
            "approach facilitates both quantitative performance metrics and\n",
            "qualitative assessment of the system’s responses, providing a com-\n",
            "prehensive view of its effectiveness.\n",
            "By parameterizing company names, quarters, and years, we can\n",
            "easily adapt the system to different datasets and time periods. This\n",
            "design choice allows for seamless integration of new data and\n",
            "expansion to cover multiple companies and earnings calls.\n",
            "4.3 GraphRAG\n",
            "For GraphRAG, we developed an Q&A system specifically designed\n",
            "for corporate earnings call transcripts. Our implementation of\n",
            "GraphRAG leverages several key components and techniques:\n",
            "LLM GPT-3.5-Turbo\n",
            "LLM Temperature 0\n",
            "Framework LangChain\n",
            "KG Manipulation Networkx\n",
            "Chunk Size 1024\n",
            "Chunk Overlap 0\n",
            "Number of Triplets 13950\n",
            "Number of nodes 11405\n",
            "Number of edges 13883\n",
            "DFS Depth 1\n",
            "Table 4: GraphRAG Configuration\n",
            "8https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain' metadata={'source': '/content/hybridrag.pdf', 'page': 5}\n",
            "Page  12  :  page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\n",
            "Knowledge Graph Construction:- We begin by constructing\n",
            "a KG from a set of knowledge triplets extracted from corporate\n",
            "documents using the prompt engineering based methodology as\n",
            "described in Section 4.1. These triplets, stored in a pickle file, repre-\n",
            "sent structured information in the form of subject-predicate-object\n",
            "relationships. We use the NetworkxEntityGraph class from the\n",
            "LangChain library to create and manage this graph structure. Each\n",
            "triple is added to the graph, which encapsulates the head entity,\n",
            "relation, tail entity, and additional metadata.\n",
            "We implement the Q&A functionality using the GraphQAChain\n",
            "class from LangChain. This chain combines the KG with an LLM\n",
            "(in our case, OpenAI’s GPT-3.5-turbo) to generate responses. The\n",
            "GraphQAChain traverses the KG to find relevant information and\n",
            "uses the language model to formulate coherent answers based on\n",
            "the retrieved context. A summary of configuration of our LLM\n",
            "models and other libraries used for GraphRAG is shown in Table 4.\n",
            "In KG as the information is stored in the form of entities and\n",
            "relationships and there can be multiple relations emanating from\n",
            "one single entity, in this experiment, to extract relevant information\n",
            "from the KG, we employ a depth-first search strategy constrained\n",
            "by a depth of one from the specified entity.\n",
            "To prepare for assessing the performance of our GraphRAG\n",
            "system, we follow the below steps: Dataset Preparation:- We use\n",
            "a pre-generated CSV file containing question-answer pairs specific\n",
            "to the earnings call transcript.\n",
            "Iterative Processing:- For each question in the dataset, we run\n",
            "the GraphQAChain to generate an answer.\n",
            "Result Compilation:- We compile the results, including the origi-\n",
            "nal questions, generated answers, retrieved contexts, and ground\n",
            "truth answers, into a structured format.\n",
            "Finally, the evaluation results are saved in both CSV and JSON\n",
            "formats for further analysis and comparison. We then fed these\n",
            "outputs into our RAG evaluation pipeline. For each Q&A pair in\n",
            "our dataset, we compute all three metrics: faithfulness, answer\n",
            "relevance, context precision and context recall.\n",
            "4.4 HybridRAG\n",
            "For the proposed HybridRAG technique, upon obtaining all the\n",
            "contextual information from VectorRAG and GraphRAG, we con-\n",
            "catenate these contexts to form a unified context utilizing both\n",
            "techniques. This combined context is then fed into the answer gen-\n",
            "erator model to produce a response. The context used for response\n",
            "generation is relatively larger, which affects the precision of the\n",
            "generated response. The context from VectorRAG is appended first,\n",
            "followed by the context from GraphRAG. Consequently, the preci-\n",
            "sion of the generated answer depends on the source context. If the\n",
            "answer is generated from the GraphRAG context, it will have lower\n",
            "precision, as the GraphRAG context is added last in the sequence of\n",
            "contexts provided to the answer generator model, and vice versa.\n",
            "5 RESULTS\n",
            "We evaluate both the retrieval and generation parts of RAG for\n",
            "the three different RAG pipelines. Evaluating the RAG outputs is\n",
            "also an active area of research there is no standard tool which is\n",
            "universally accepted as of yet, though we use a currently popular\n",
            "framework RAGAS[ 30] to evaluate the three RAG pipelines in thepresent work where we have modified them a bit to make more\n",
            "precise comparisons.\n",
            "The results of our comparative analysis reveal notable differences\n",
            "in performance among VectorRAG, GraphRAG, and HybridRAG\n",
            "approaches as summarized in Table 5. In terms of Faithfulness,\n",
            "GraphRAG and HybridRAG demonstrated superior performance,\n",
            "both achieving a score of 0.96, while VectorRAG trailed slightly with\n",
            "a score of 0.94. Answer relevancy scores varied across the meth-\n",
            "ods, with HybridRAG outperforming the others at 0.96, followed\n",
            "by VectorRAG at 0.91, and GraphRAG at 0.89. Context precision' metadata={'source': '/content/hybridrag.pdf', 'page': 6}\n",
            "Page  13  :  page_content='a score of 0.94. Answer relevancy scores varied across the meth-\n",
            "ods, with HybridRAG outperforming the others at 0.96, followed\n",
            "by VectorRAG at 0.91, and GraphRAG at 0.89. Context precision\n",
            "was highest for GraphRAG at 0.96, significantly surpassing Vec-\n",
            "torRAG (0.84) and HybridRAG (0.79). However, in context recall,\n",
            "both VectorRAG and HybridRAG achieved perfect scores of 1, while\n",
            "GraphRAG lagged behind at 0.85.\n",
            "Overall, these results suggest that GraphRAG offers improve-\n",
            "ments over VectorRAG, particularly in faithfulness and context pre-\n",
            "cision. Furthermore, HybridRAG emerges as the most balanced and\n",
            "effective approach, outperforming both VectorRAG and GraphRAG\n",
            "in key metrics such as faithfulness and answer relevancy, while\n",
            "maintaining high context recall.\n",
            "The relatively lower context precision observed for HybridRAG\n",
            "(0.79) can be attributed to its unique approach of combining con-\n",
            "texts from both VectorRAG and GraphRAG methods. While this\n",
            "integration allows for more comprehensive information retrieval,\n",
            "it also introduces additional content that may not align precisely\n",
            "with the ground truth, thus affecting the context precision met-\n",
            "ric. Despite this trade-off, HybridRAG’s superior performance in\n",
            "faithfulness, answer relevancy, and context recall underscores its\n",
            "effectiveness. When considering the overall evaluation metrics,\n",
            "HybridRAG emerges as the most promising approach, balancing\n",
            "high-quality answers with comprehensive context retrieval.\n",
            "Overall GraphRAG performs better in extractive questions com-\n",
            "pared to VectorRAG. And VectorRAG does better in abstractive\n",
            "questions where information is not explicitly mentioned in the\n",
            "raw data. And also GraphRAG sometimes fails to answer ques-\n",
            "tions correctly whenever there is no entity explicitly mentioned in\n",
            "the question. So HybridRAG does a good job overall, as whenever\n",
            "VectorRAG fails to fetch correct context in extractive questions\n",
            "it falls back to GraphRAG to generate the answer. And whenever\n",
            "GraphRAG fails to fetch correct context in abstractive questions it\n",
            "falls back to VectorRAG to generate the answer.\n",
            "VectorRAG GraphRAG HybridRAG\n",
            "F 0.94 0.96 0.96\n",
            "AR 0.91 0.89 0.96\n",
            "CP 0.84 0.96 0.79\n",
            "CR 1 0.85 1\n",
            "Table 5: Performance Metrics for Different RAG Pipelines.\n",
            "Here, F, AR, CP and CR refer to Faithfulness, Answer\n",
            "Relevence, Context Precision and Context Recall.\n",
            "6 CONCLUSION AND FUTURE DIRECTIONS\n",
            "Among the current approaches to mitigate issues regarding infor-\n",
            "mation extraction from external documents that were not part of' metadata={'source': '/content/hybridrag.pdf', 'page': 6}\n",
            "Page  14  :  page_content='Sarmah et al.\n",
            "training data for the LLM, Retrieval Augmented Generation (RAG)\n",
            "techniques have emerged as the most popular ones that aim to\n",
            "improve the performance of LLMs by incorporating relevant re-\n",
            "trieval mechanisms. RAG methods enhance the LLMs’ capabilities\n",
            "by retrieving pertinent documents or text to provide additional\n",
            "context during the generation process. However, these approaches\n",
            "encounter significant limitations when applied to the specialized\n",
            "and intricate domain of financial documents. Furthermore, the qual-\n",
            "ity of the retrieved context from a vast and heterogeneous corpus\n",
            "can be inconsistent, leading to inaccuracies and incomplete analyses.\n",
            "These challenges highlight the need for more sophisticated methods\n",
            "that can effectively integrate and process the detailed and domain-\n",
            "specific information found in financial documents, ensuring more\n",
            "reliable and accurate outputs for informed decision-making.\n",
            "In the present work, we have introduced a novel approach that\n",
            "significantly advances the field of information extraction from finan-\n",
            "cial documents through the development of a hybrid RAG system.\n",
            "This system, called HybridRAG, which integrates the strengths\n",
            "of both Knowledge Graphs (KGs) and advanced language models,\n",
            "represents a leap forward in our ability to extract and interpret\n",
            "complex information from unstructured financial texts. The hybrid\n",
            "RAG system, by combining traditional vector-based RAG and KG-\n",
            "based RAG, has shown superior performance in terms of retrieval\n",
            "accuracy and answer generation, marking a pivotal step towards\n",
            "more effective financial analysis tools.\n",
            "Through a comparative analysis using objecive evaluation met-\n",
            "rics, we have highlighted the distinct performance advantages of\n",
            "the HybridRAG approach over its vector-based and KG-based coun-\n",
            "terparts. The HybridRAG system excels in faithfulness, answer\n",
            "relevancy, and context recall, demonstrating the benefits of inte-\n",
            "grating contexts from both VectorRAG and GraphRAG methods,\n",
            "despite potential trade-offs in context precision.\n",
            "The implications of this research extend beyond the immedi-\n",
            "ate realm of financial analysis. By developing a system capable\n",
            "of understanding and responding to nuanced queries about com-\n",
            "plex financial documents, we pave the way for more sophisticated\n",
            "AI-assisted financial decision-making tools that could potentially\n",
            "democratize access to financial insights, allowing a broader range of\n",
            "stakeholders to engage with and understand financial information.\n",
            "Future directions for this research include expanding the system\n",
            "to handle multi-modal inputs, incorporating numerical data analysis\n",
            "capabilities, and developing more sophisticated evaluation metrics\n",
            "that capture the nuances of financial language and the accuracy\n",
            "of numerical information in the responses. Additionally, exploring\n",
            "the integration of this system with real-time financial data streams\n",
            "could further enhance its utility in dynamic financial environments.\n",
            "7 ACKNOWLEDGEMENT\n",
            "The views expressed here are those of the authors alone and not of\n",
            "BlackRock, Inc or NVIDIA. We are grateful to Emma Lind for her\n",
            "invaluable support for this collaboration.\n",
            "REFERENCES\n",
            "[1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation\n",
            "of word representations in vector space. 2013.\n",
            "[2]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\n",
            "Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.\n",
            "Advances inneural information processing systems, 30, 2017.[3]Yi Yang, Mark Christopher Siy Uy, and Allen Huang. Finbert: A pretrained\n",
            "language model for financial communications. arXiv preprint arXiv:2006.08097 ,\n",
            "2020.\n",
            "[4]Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Se-\n",
            "bastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon\n",
            "Mann. Bloomberggpt: A large language model for finance. arXiv preprint\n",
            "arXiv:2303.17564, 2023.\n",
            "[5] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qing-' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Page  15  :  page_content='Mann. Bloomberggpt: A large language model for finance. arXiv preprint\n",
            "arXiv:2303.17564, 2023.\n",
            "[5] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qing-\n",
            "song Wen, and Stefan Zohren. A survey of large language models for financial ap-\n",
            "plications: Progress, prospects and challenges. arXiv preprint arXiv:2406.11903 ,\n",
            "2024.\n",
            "[6]Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu,\n",
            "Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, Ninghao Liu, and Tianming\n",
            "Liu. Revolutionizing finance with llms: An overview of applications and insights,\n",
            "2024.\n",
            "[7] Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang\n",
            "Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. Domain spe-\n",
            "cialization as the key to make large language models disruptive: A comprehensive\n",
            "survey. arXiv preprint arXiv:2305.18703, 2023.\n",
            "[8] Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, and Tianjie Zhu. Towards\n",
            "reducing hallucination in extracting information from financial reports using\n",
            "large language models. In Proceedings oftheThird International Conference\n",
            "onAI-ML Systems, pages 1–5, 2023.\n",
            "[9]Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\n",
            "Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\n",
            "Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp\n",
            "tasks. Advances inNeural Information Processing Systems , 33:9459–9474, 2020.\n",
            "[10] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative\n",
            "models for open domain question answering. arXiv preprint arXiv:2007.01282 ,\n",
            "2021.\n",
            "[11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.\n",
            "Realm: Retrieval-augmented language model pre-training. arXiv preprint\n",
            "arXiv:2002.08909, 2020.\n",
            "[12] Antonio Jose Jimeno Yepes et al. Financial report chunking for effective retrieval\n",
            "augmented generation. arXiv preprint arXiv:2402.05131, 2024.\n",
            "[13] SuperAcc. Retrieval-augmented generation on financial statements. SuperAcc\n",
            "Insights, 2024.\n",
            "[14] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A\n",
            "survey on knowledge graphs: Representation, acquisition, and applications. IEEE\n",
            "transactions onneural networks andlearning systems, 33(2):494–514, 2021.\n",
            "[15] Heiko Paulheim. Knowledge graphs: State of the art and future directions.\n",
            "Semantic Web Journal, 10(4):1–20, 2017.\n",
            "[16] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia D’Amato, Gerard de Melo,\n",
            "Claudio Gutiérrez, and Maria ... Maleshkova. Knowledge graphs. arXiv preprint\n",
            "arXiv:2003.02320, 2021.\n",
            "[17] Lisa Ehrlinger and Wolfram Wöß. Towards a definition of knowledge graphs. In\n",
            "SEMANTiCS (Posters, Demos, SuCCESS), pages 1–4, 2016.\n",
            "[18] Xiaohui Victor Li and Francesco Sanna Passino. Findkg: Dynamic knowledge\n",
            "graphs with large language models for detecting global trends in financial mar-\n",
            "kets. arXiv preprint arXiv:2407.10909, 2024.\n",
            "[19] Shourya De, Anima Aggarwal, and Anna Cinzia Squicciarini. Financial knowl-\n",
            "edge graphs: A novel approach to empower data-driven financial applications. In\n",
            "IEEE International Conference onBigData (Big Data), pages 1311–1320, 2018.\n",
            "[20] Marina Petrova and Birgit Reinwald. Knowledge graphs in finance: Applications\n",
            "and opportunities. Journal ofFinancial Data Science, 2(2):10–19, 2020.\n",
            "[21] Wei Liu, Jun Zhang, and Li Pan. Utilizing knowledge graphs for financial data\n",
            "integration and analysis. Data Science Journal, 18(1):1–15, 2019.\n",
            "[22] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva\n",
            "Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag\n",
            "approach to query-focused summarization. arXiv preprint arXiv:2404.16130 ,\n",
            "2024.\n",
            "[23] Xuchen Yao, Yanan Sun, Zhen Huang, and Dong Li. Retrieval-augmented gener-\n",
            "ation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2101.07554 , 2021.\n",
            "[24] Weizhi Zhao, Haoyu Chen, Kai Liu, and Jun Zhao. Graph-based retrieval-\n",
            "augmented generation for open-domain question answering. In Proceedings' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Page  16  :  page_content='[24] Weizhi Zhao, Haoyu Chen, Kai Liu, and Jun Zhao. Graph-based retrieval-\n",
            "augmented generation for open-domain question answering. In Proceedings\n",
            "oftheAAAI Conference onArtificial Intelligence , volume 36, pages 3098–3106,\n",
            "2022.\n",
            "[25] Bill Yuchen Lin, Yuanhe Liu, Ming Shen, and Xiang Ren. Kgpt: Knowledge-\n",
            "grounded pre-training for data-to-text generation. In Findings oftheAssociation\n",
            "forComputational Linguistics: EMNLP 2020, pages 710–724, 2020.\n",
            "[26] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Ji-\n",
            "awei Sun, and Haofen Wang. Retrieval-augmented generation for large language\n",
            "models: A survey. arXiv preprint arXiv:2312.10997, 2023.\n",
            "[27] Tyler Procko. Graph retrieval-augmented generation for large language models:\n",
            "A survey. Available atSSRN, 2024.\n",
            "[28] Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. A comprehensive\n",
            "survey on automatic knowledge graph construction. ACM Computing Surveys ,\n",
            "56(4):1–62, 2023.' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Page  17  :  page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\n",
            "[29] Ishani Mondal, Yufang Hou, and Charles Jochim. End-to-end nlp knowledge\n",
            "graph construction. arXiv preprint arXiv:2106.01167, 2021.\n",
            "[30] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ra-\n",
            "gas: Automated evaluation of retrieval augmented generation. arXiv preprint\n",
            "arXiv:2309.15217, 2023.\n",
            "[31] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan\n",
            "Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al.\n",
            "Finqa: A dataset of numerical reasoning over financial data. arXiv preprint\n",
            "arXiv:2109.00122, 2021.\n",
            "[32] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott,\n",
            "Manel Zarrouk, and Alexandra Balahur. Www’18 open challenge: financial\n",
            "opinion mining and question answering. In Companion proceedings ofthetheweb conference 2018, pages 1941–1942, 2018.\n",
            "[33] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang,\n",
            "Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: A question answering\n",
            "benchmark on a hybrid of tabular and textual content in finance. arXiv preprint\n",
            "arXiv:2105.07624, 2021.\n",
            "[34] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer,\n",
            "and Bertie Vidgen. Financebench: A new benchmark for financial question\n",
            "answering. arXiv preprint arXiv:2311.11944, 2023.\n",
            "[35] Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing\n",
            "Zhu, and Junwei Liang. Fintextqa: A dataset for long-form financial question\n",
            "answering. arXiv preprint arXiv:2405.09980, 2024.' metadata={'source': '/content/hybridrag.pdf', 'page': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "pdf_splits = pdf_text_splitter.split_documents(pages)\n",
        "pdf_splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv0Ak0TIe8o7",
        "outputId": "45e264c9-cfb8-44b3-8434-8011ebe0ef81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/hybridrag.pdf', 'page': 0}, page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval\\nAugmented Generation for Efficient Information Extraction\\nBhaskarjit Sarmah\\nbhaskarjit.sarmah@blackrock.com\\nBlackRock, Inc.\\nGurugram, IndiaBenika Hall\\nbhall@nvidia.com\\nNVIDIA\\nSanta Clara, CA, USARohan Rao\\nrohrao@nvidia.com\\nNVIDIA\\nSanta Clara, CA, USA\\nSunil Patel\\nsupatel@nvidia.com\\nNVIDIA\\nSanta Clara, CA, USAStefano Pasquali\\nstefano.pasquali@blackrock.com\\nBlackRock, Inc.\\nNew York, NY, USADhagash Mehta\\ndhagash.mehta@blackrock.com\\nBlackRock, Inc.\\nNew York, NY, USA\\nABSTRACT\\nExtraction and interpretation of intricate information from unstruc-\\ntured text data arising in financial applications, such as earnings\\ncall transcripts, present substantial challenges to large language\\nmodels (LLMs) even using the current best practices to use Re-\\ntrieval Augmented Generation (RAG) (referred to as VectorRAG\\ntechniques which utilize vector databases for information retrieval)\\ndue to challenges such as domain specific terminology and complex'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 0}, page_content='trieval Augmented Generation (RAG) (referred to as VectorRAG\\ntechniques which utilize vector databases for information retrieval)\\ndue to challenges such as domain specific terminology and complex\\nformats of the documents. We introduce a novel approach based\\non a combination, called HybridRAG , of the Knowledge Graphs\\n(KGs) based RAG techniques (called GraphRAG) and VectorRAG\\ntechniques to enhance question-answer (Q&A) systems for infor-\\nmation extraction from financial documents that is shown to be\\ncapable of generating accurate and contextually relevant answers.\\nUsing experiments on a set of financial earning call transcripts\\ndocuments which come in the form of Q&A format, and hence\\nprovide a natural set of pairs of ground-truth Q&As, we show that\\nHybridRAG which retrieves context from both vector database and\\nKG outperforms both traditional VectorRAG and GraphRAG indi-\\nvidually when evaluated at both the retrieval and generation stages'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 0}, page_content='HybridRAG which retrieves context from both vector database and\\nKG outperforms both traditional VectorRAG and GraphRAG indi-\\nvidually when evaluated at both the retrieval and generation stages\\nin terms of retrieval accuracy and answer generation. The proposed\\ntechnique has applications beyond the financial domain.\\n1 INTRODUCTION\\nFor the financial analyst, it is crucial to extract and analyze infor-\\nmation from unstructured data sources like news articles, earnings\\nreports, and other financial documents to have at least some chance\\nto be on the better side of potential information asymmetry. These\\nsources hold valuable insights that can impact investment decisions,\\nmarket predictions, and overall sentiment. However, traditional\\ndata analysis methods struggle to effectively extract and utilize\\nthis information due to its unstructured nature. Large language\\nmodels (LLMs) [ 1–4] have emerged as powerful tools for financial\\nservices and investment management. Their ability to process and'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 0}, page_content='this information due to its unstructured nature. Large language\\nmodels (LLMs) [ 1–4] have emerged as powerful tools for financial\\nservices and investment management. Their ability to process and\\nunderstand vast amounts of textual data makes them invaluable\\nfor tasks such as sentiment analysis, market trend predictions, and\\nautomated report generation. Specifically, extracting information\\nfrom annual reports and other financial documents can greatly en-\\nhance the efficiency and accuracy of financial analysts [ 5]. A robust\\ninformation extraction system can help analysts quickly gather\\nrelevant data, identify market trends, and make informed decisions,\\nleading to better investment strategies and risk management [6].Although LLMs have substantial potential in financial applica-\\ntions, there are notable challenges in using pre-trained models to\\nextract information from financial documents outside their training\\ndata while also reducing hallucination [ 7,8]. Financial documents'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 0}, page_content='extract information from financial documents outside their training\\ndata while also reducing hallucination [ 7,8]. Financial documents\\ntypically contain domain-specific language, multiple data formats,\\nand unique contextual relationships that general purpose-trained\\nLLMs do not handle well. In addition, extracting consistent and\\ncoherent information from multiple financial documents can be\\nchallenging due to variations in terminology, format, and context\\nacross different textual sources. The specialized terminology and\\ncomplex data formats in financial documents make it difficult for\\nmodels to extract meaningful insights, in turn, causing inaccurate\\npredictions, overlooked insights, and unreliable analysis, which'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 0}, page_content='complex data formats in financial documents make it difficult for\\nmodels to extract meaningful insights, in turn, causing inaccurate\\npredictions, overlooked insights, and unreliable analysis, which\\nultimately hinder the ability to make well-informed decisions.\\nCurrent approaches to mitigate these issues include various\\nRetrieval-Augmented Generation (RAG) techniques [ 9], which aim\\nto improve the performance of LLMs by incorporating relevant\\nretrieval techniques. VectorRAG (the traditional RAG techniques\\nthat are based on vector databases) focuses on improving Natural\\nLanguage Processing (NLP) tasks by retrieving relevant textual\\ninformation to support the generation tasks. VectorRAG excels in\\nsituations where context from related textual documents is crucial\\nfor generating meaningful and coherent responses [ 9–11]. RAG-\\nbased methods ensure the LLMs generate relevant and coherent\\nresponses that are aligned with the original input query. How-'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 0}, page_content='for generating meaningful and coherent responses [ 9–11]. RAG-\\nbased methods ensure the LLMs generate relevant and coherent\\nresponses that are aligned with the original input query. How-\\never, for financial documents, these approaches have significant\\nchallenges as a standalone solution. For instance, traditional RAG\\nsystems often use paragraph-level chunking techniques, which\\nassume the text in those documents are uniform in length. This\\napproach neglects the hierarchical nature of financial statements\\nand can result in the loss of critical contextual information for an\\naccurate analysis[ 12,13]. Due to the complexities in analyzing fi-\\nnancial documents, the quality of the LLM retrieved-context from\\na vast and heterogeneous corpus can be inconsistent, leading to\\ninaccuracies and incomplete analyses. These challenges demon-\\nstrate the need for more sophisticated methods that can effectively\\nintegrate and process the detailed and domain-specific information'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 0}, page_content='inaccuracies and incomplete analyses. These challenges demon-\\nstrate the need for more sophisticated methods that can effectively\\nintegrate and process the detailed and domain-specific information\\nfound in financial documents, ensuring more reliable and accurate\\nresults for informed decision-making.\\nKnowledge graphs (KGs) [ 14] may provide a different point of\\nview to looking at the financial documents where the documentsarXiv:2408.04948v1  [cs.CL]  9 Aug 2024'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 1}, page_content='Sarmah et al.\\nare viewed as a collection of triplets of entities and their relation-\\nships as depicted in the text of the documents. KGs have become\\na pivotal technology in data management and analysis, provid-\\ning a structured way to represent knowledge through entities and\\nrelationships and have been widely adopted in various domains,\\nincluding search engines, recommendation systems, and biomedical\\nresearch [ 15–17]. The primary advantage of KGs lies in their ability\\nto offer a structured representation, which facilitates efficient query-\\ning and reasoning. However, building and maintaining KGs and\\nintegrating data from different sources, such as documents, news\\narticles, and other external sources, into a coherent knowledge\\ngraph poses significant challenges.\\nThe financial services industry has recognized the potential of\\nKGs in enhancing data integration of heterogeneous data sources,\\nrisk management, and predictive analytics [ 18?–21]. Financial KGs'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 1}, page_content='The financial services industry has recognized the potential of\\nKGs in enhancing data integration of heterogeneous data sources,\\nrisk management, and predictive analytics [ 18?–21]. Financial KGs\\nintegrate various financial data sources such as market data, fi-\\nnancial reports, and news articles, creating a comprehensive view\\nof financial entities and their relationships. This unified view im-\\nproves the accuracy and comprehensiveness of financial analysis,\\nfacilitates risk management by identifying hidden relationships,\\nand supports advanced predictive analytics for more accurate mar-\\nket predictions and investment decisions. However, handling large\\nvolumes of financial data and continuously updating the knowledge\\ngraph to reflect the dynamic nature of financial markets can be\\nchallenging and resource-intensive.\\nGraphRAG (Graph-based Retrieval-Augmented Generation) [ 22–\\n27] is a novel approach that leverages knowledge graphs (KGs) to'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 1}, page_content='challenging and resource-intensive.\\nGraphRAG (Graph-based Retrieval-Augmented Generation) [ 22–\\n27] is a novel approach that leverages knowledge graphs (KGs) to\\nenhance the performance of NLP tasks such as Q&A systems. By\\nintegrating KGs with RAG techniques, GraphRAG enables more\\naccurate and context-aware generation of responses based on the\\nstructured information extracted from financial documents. But\\nGraphRAG generally underperforms in abstractive Q&A tasks or\\nwhen there is not explicit entity mentioned in the question.\\nIn the present work, we propose a combination of VectorRAG and\\nGraphRAG, called HybridRAG, to retrieve the relevant information\\nfrom external documents for a given query to the LLM that brings\\nadvantages of both the RAGs together to provide demonstrably\\nmore accurate answers to the queries.\\n1.1 Prior Work and Our Contribution\\nVectorRAG has been extensively investigated in the recent years\\nand focuses on enhancing NLP tasks by retrieving relevant textual'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 1}, page_content='1.1 Prior Work and Our Contribution\\nVectorRAG has been extensively investigated in the recent years\\nand focuses on enhancing NLP tasks by retrieving relevant textual\\ninformation to support generation processes [ 9–11,26]. However,\\nthe effectiveness of the retrieval mechanism across multiple docu-\\nments and longer contexts poses a significant challenge in extract-\\ning relevant responses. GraphRAG combines the capabilities of KGs\\nwith RAG to improve traditional NLP tasks [ 23–25]. Within our\\nimplementations of both VectorRAG GraphRAG techniques, we\\nexplicitly add information on the metadata of the documents that\\nis also shown to improve the performance of VectorRAG [8].\\nTo the best of our knowledge the present work is the first work\\nthat proposes a RAG approach that is hybrid of both VectorRAG\\nand GraphRAG and demonstrates its potential of more effective\\nanalysis and utilization of financial documents by leveraging the\\ncombined strengths of VectorRAG and GraphRAG. We also utilize a'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 1}, page_content='and GraphRAG and demonstrates its potential of more effective\\nanalysis and utilization of financial documents by leveraging the\\ncombined strengths of VectorRAG and GraphRAG. We also utilize a\\nnovel ground-truth Q&A dataset extracted from publicly availablefinancial call transcripts of the companies included in the Nifty-50\\nindex which is an Indian stock market index that represents the\\nweighted average of 50 of the largest Indian companies listed on\\nthe National Stock Exchange1.\\n2 METHODOLOGY\\nIn this Section, we provide details of the proposed methodology\\nby first discussing details of VectorRAG, then methodologies of\\nconstructing KGs from given documents and our proposed method-\\nology of GraphRAG and finally the HybridGraph technique.\\n2.1 VectorRAG'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 1}, page_content='by first discussing details of VectorRAG, then methodologies of\\nconstructing KGs from given documents and our proposed method-\\nology of GraphRAG and finally the HybridGraph technique.\\n2.1 VectorRAG\\nThe traditional RAG [ 9] process begins with a query that is related\\nto the information possessed within external document(s) that are\\nnot a part of the training dataset for the LLM. This query is used to\\nsearch an external repository, such as a vector database or indexed\\ncorpus, to fetch relevant documents or passages containing useful\\ninformation. These retrieved documents are then fed back into the\\nLLM as additional context. Hence, in turn, for the given query, the\\nlanguage model generates a response based not only on its inter-\\nnal training data but also by incorporating the retrieved external\\ninformation. This integration ensures that the generated content is\\ngrounded in more recent and verifiable data, improving the accu-\\nracy and contextual relevance of the responses. By combining the'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 1}, page_content='information. This integration ensures that the generated content is\\ngrounded in more recent and verifiable data, improving the accu-\\nracy and contextual relevance of the responses. By combining the\\nretrieval of external information with the generative capabilities\\nof large language models, RAG enhances the overall quality and\\nreliability of the generated text.\\nIn traditional VectorRAG, the given external documents are di-\\nvided into multiple chunks because of the limitation of context size\\nof the language model. Those chunks are converted into embed-\\ndings using an embeddings model and then stored into a vector\\ndatabase. After that, the retrieval component performs a similarity\\nsearch within the vector database to identify and rank the chunks\\nmost relevant to the query. The top-ranked chunks are retrieved\\nand aggregated to provide context for the generative model.\\nThen, the generative model takes this retrieved context along'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 1}, page_content='most relevant to the query. The top-ranked chunks are retrieved\\nand aggregated to provide context for the generative model.\\nThen, the generative model takes this retrieved context along\\nwith the original query and synthesizes a response. Thus, it merges\\nthe real-time information from the retrieved chunks with its pre-\\nexisting knowledge, ensuring that the response is both contextually\\nrelevant and detailed.\\nThe schematic diagram in Figure 1 provides details on the part\\nof RAG that generates vector database from given external docu-\\nments in the traditional VectorRAG methodology where we also\\ninclude explicit reference of metadata [8]. Section 4.2 will provide\\nimplementation details for our experiments.\\n2.2 Knowledge Graph Construction\\nA KG is a structured representation of real-world entities, their\\nattributes, and their relations, usually stored in a graph database\\nor a triplet store, i.e., a KG consists of nodes that represent entities'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 1}, page_content='attributes, and their relations, usually stored in a graph database\\nor a triplet store, i.e., a KG consists of nodes that represent entities\\nand edges that represent relations, as well as labels and attributes\\nfor both. A graph triplet is a basic unit of information in a KG,\\nconsisting of a subject, a predicate, and an object.\\nIn most methodologies to build a KG from given documents,\\nthree main steps are involved: knowledge extraction, knowledge\\n1https://www.nseindia.com/products/content/equities/indices/nifty_50.htm'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 2}, page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\\nFigure 1: A schematic diagram describing the vector database\\ncreation of a RAG application.\\nimprovement, and knowledge adaptation [ 28]. Here, we do not use\\nknowledge adaptation and treat the KGs as static graphs.\\nKnowledge Extraction:- This step aims to extract structured in-\\nformation from unstructured or semi-structured data, such as text,\\ndatabases, and existing ontologies. The main tasks in this step are\\nentity recognition, relationship extraction, and co-reference reso-\\nlution. Entity recognition and relationship extraction techniques\\nuse typical NLP tasks to identify entities and their relationships\\nfrom textual sources [ 29]. Coreference resolution identifies and\\nconnects different references of the same entity, keeping coherence\\nwithin the knowledge graph. For example, if the text refers to a\\ncompany as both \"the company\" and \"it\", coreference resolution'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 2}, page_content='connects different references of the same entity, keeping coherence\\nwithin the knowledge graph. For example, if the text refers to a\\ncompany as both \"the company\" and \"it\", coreference resolution\\ncan link these mentions to the same entity node in the graph.\\nKnowledge Improvement:- This step aims to enhance the quality\\nand completeness of the KG by removing redundancies and address-\\ning gaps in the extracted information. The primary tasks in this\\nstep are KG completion and fusion. KG completion technique infers\\nmissing entities and relationships within the graph using meth-\\nods such as link prediction and entity resolution. Link prediction\\npredicts the existence and type of a relation between two entities\\nbased on the graph structure and features, while entity resolution\\nmatches and merges different representations of the same entity\\nfrom different sources.\\nKnowledge fusion combines information from multiple sources\\nto create a coherent and unified KG. This involves resolving conflicts'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 2}, page_content='from different sources.\\nKnowledge fusion combines information from multiple sources\\nto create a coherent and unified KG. This involves resolving conflicts\\nand redundancies among the sources, such as contradictory or\\nduplicate facts, and aggregating or reconciling the information\\nbased on rules, probabilities, or semantic similarity.\\nWe utilized a robust methodology for creating KG triplets from\\nunstructured text data, specifically focusing on corporate docu-\\nments such as earnings call transcripts, adapted from Ref. [ 18?].\\nThis process involves several interconnected stages, each designed\\nto extract, refine, and structure information effectively.\\nWe implement a two-tiered LLM chain for content refinement\\nand information extraction. The first tier employs an LLM to gen-\\nerate an abstract representation of each document chunk. This\\nrefinement process is crucial as it distills the essential information\\nwhile preserving the original meaning and key relationships be-'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 2}, page_content='erate an abstract representation of each document chunk. This\\nrefinement process is crucial as it distills the essential information\\nwhile preserving the original meaning and key relationships be-\\ntween concepts that serves as a more focused input for subsequent\\nprocessing, enhancing the overall efficiency and accuracy of ourtriplet extraction pipeline. The second tier of our LLM chain is\\ndedicated to entity extraction and relationship identification.\\nBoth the steps are executed using carefully performed prompt\\nengineering on a pre-trained LLM. A detailed discussion on imple-\\nmentation of the methodology will be provided in Section 4.1\\n2.3 GraphRAG\\nKG based RAG [ 22], or GraphRAG, also begins with a query based\\non the user’s input same as VectorRAG. The main difference be-\\ntween VectorRAG and GraphRAG lies in the retrieval part. The\\nquery here is now used to search the KG to retrieve relevant nodes\\n(entities) and edges (relationships) related to the query. A subgraph,'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 2}, page_content='tween VectorRAG and GraphRAG lies in the retrieval part. The\\nquery here is now used to search the KG to retrieve relevant nodes\\n(entities) and edges (relationships) related to the query. A subgraph,\\nwhich consists of these relevant nodes and edges, is extracted from\\nthe full KG to provide context. This subgraph is then integrated\\nwith the language model’s internal knowledge, by encoding the\\ngraph structure into embeddings that the model can interpret. The\\nlanguage model uses this combined context to generate responses\\nthat are informed by both the structured information from the KG\\nand its pre-trained knowledge. Crucially, when responding to user\\nqueries about a particular company, we leveraged the metadata\\ninformation to selectively filter and retrieve only those document'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 2}, page_content='and its pre-trained knowledge. Crucially, when responding to user\\nqueries about a particular company, we leveraged the metadata\\ninformation to selectively filter and retrieve only those document\\nsegments pertinent to the queried company [ 8]. This integration\\nhelps ensure that the generated outputs are accurate, contextually\\nrelevant, and grounded in verifiable information.\\nA schematic diagram of the retrieval methodology of GraphRAG\\nis given in Figure 2. Here we first write a prompt to clean the\\ndata and then write another prompt in the second stage to create\\nknowledge triplets along with metadata. It will be covered in more\\ndetail in section 4.1\\nFigure 2: A schematic diagram describing knowledge graph\\ncreation process of GraphRAG.\\n2.3.1 HybridRAG. For the HybridRAG technique, we propose to\\nintegrate the aforementioned two distinct RAG techniques: Vec-\\ntorRAG and GraphRAG. This integration involves a systematic\\ncombination of contextual information retrieved from both the'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 2}, page_content='integrate the aforementioned two distinct RAG techniques: Vec-\\ntorRAG and GraphRAG. This integration involves a systematic\\ncombination of contextual information retrieved from both the\\ntraditional vector-based retrieval mechanism and the KG-based\\nretrieval system, the latter of which was constructed specifically\\nfor this study.\\nThe amalgamation of these two contexts allows us to leverage the\\nstrengths of both approaches. The VectorRAG component provides\\na broad, similarity-based retrieval of relevant information, while\\nthe GraphRAG element contributes structured, relationship-rich'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 3}, page_content='Sarmah et al.\\ncontextual data. This combined context is then utilized as input for a\\nLLM to generate the final responses. Details on the implementation\\nof the HybridRAG will be provided in Section 4.4.\\n2.4 Evaluation Metrics\\nTo assess the efficacy of this integrated approach, we conducted a\\ncomparative analysis among the three approaches in a controlled ex-\\nperimental set up: VectorRAG, GraphRAG and HybridRAG. The re-\\nsponses generated using the combined VectorRAG and GraphRAG\\ncontexts were juxtaposed against those produced individually by\\nVectorRAG and GraphRAG. This comparative evaluation aimed to\\ndiscern potential improvements in response quality, accuracy, and\\ncomprehensiveness that might arise from the synergistic integra-\\ntion of these two RAG methodologies.\\nTo objectively evaluate different RAG approaches (VectorRAG\\nand GraphRAG in their case), Ref. [ 22] utilized metrics such as\\ncomprehensiveness (i.e., the amount of details the answer provides'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 3}, page_content='To objectively evaluate different RAG approaches (VectorRAG\\nand GraphRAG in their case), Ref. [ 22] utilized metrics such as\\ncomprehensiveness (i.e., the amount of details the answer provides\\nto cover all aspects and details of the question?); diversity (i.e.,\\nthe richness of the answer in providing different perspectives and\\ninsights on the question); empowerment (i.e., the helpfulness of the\\nanswer to the reader understand and make informed judgements\\nabout the topic); and, directness (i.e., clearness of the answer in\\naddressing the question). here, the LLM was provided tuples of\\nquestion, target metric, and a pair of answers, and was asked to\\nassess which answer was better according to the metric and why.\\nThese metrics though compare the final generated answers, do\\nnot necessarily directly evaluate the retrieval and generation parts\\nseparately. Instead, here we implement a comprehensive set of\\nevaluation metrics which are designed to capture different aspects'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 3}, page_content='not necessarily directly evaluate the retrieval and generation parts\\nseparately. Instead, here we implement a comprehensive set of\\nevaluation metrics which are designed to capture different aspects\\nof a given RAG system’s output quality, focusing on faithfulness,\\nanswer relevance, and context relevance [ 30]. Each metric provides\\nunique insights into the system’s capabilities and limitations.\\n2.4.1 Faithfulness. Faithfulness is a crucial metric that measures\\nthe extent to which the generated answer can be inferred from the\\nprovided context. Our implementation of the faithfulness metric\\ninvolves a two-step process:\\nStatement Extraction:- We use an LLM to decompose the gener-\\nated answer into a set of concise statements. This step is crucial\\nfor breaking down complex answers into more manageable and\\nverifiable units. The prompt used for this step is:\\n\"Given a question and answer, create one or more statements\\nfrom each sentence in the given answer. question: [question] an-'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 3}, page_content='verifiable units. The prompt used for this step is:\\n\"Given a question and answer, create one or more statements\\nfrom each sentence in the given answer. question: [question] an-\\nswer: [answer]\".\\nStatement Verification:- For each extracted statement, we employ\\nthe LLM to determine if it can be inferred from the given context.\\nThis verification process uses the following prompt:\\n\"Consider the given context and following statements, then de-\\ntermine whether they are supported by the information present in\\nthe context. Provide a brief explanation for each statement before\\narriving at the verdict (Yes/No). Provide a final verdict for each\\nstatement in order at the end in the given format. Do not deviate\\nfrom the specified format. statement: [statement 1] ... statement:\\n[statement n]\".\\nThe faithfulness score ( 𝐹) is𝐹=|𝑉|/|𝑆|, where |𝑉|is the number\\nof supported statements and |𝑆|is the total number of statements.2.4.2 Answer Relevance:-. The answer relevance metric assesses'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 3}, page_content='The faithfulness score ( 𝐹) is𝐹=|𝑉|/|𝑆|, where |𝑉|is the number\\nof supported statements and |𝑆|is the total number of statements.2.4.2 Answer Relevance:-. The answer relevance metric assesses\\nhow well the generated answer addresses the original question,\\nirrespective of factual accuracy. This metric helps identify cases of\\nincomplete answers or responses containing irrelevant information.\\nOur implementation involves the following steps:\\nQuestion Generation: We prompt the LLM to generate n potential\\nquestions based on the given answer:\\n\"Generate a question for the given answer. answer: [answer]\".\\nThen, we obtain embeddings for all generated questions and the\\noriginal question using OpenAI’s text-embedding-ada-002 model2.\\nWe then calculate the cosine similarity between each generated'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 3}, page_content='Then, we obtain embeddings for all generated questions and the\\noriginal question using OpenAI’s text-embedding-ada-002 model2.\\nWe then calculate the cosine similarity between each generated\\nquestion’s embedding and the original question’s embedding.\\nFinally, the answer relevance score (AR) is computed as the aver-\\nage similarity across all generated questions: 𝐴𝑅=1\\n𝑛Í(𝑠𝑖𝑚(𝑞,𝑞𝑖)),\\nwhere𝑠𝑖𝑚(𝑞,𝑞𝑖)is the cosine similarity between the embedding\\nof the original question 𝑞and the embeddings of each of the 𝑛\\ngenerated questions 𝑞𝑖.\\n2.4.3 Context Precision. It is a metric used to evaluate the relevance\\nof retrieved context chunks in relation to a specified ground truth\\nfor a given question3. It calculates the proportion of relevant items\\nthat appear in the top ranks of the context. The formula for context\\nprecision at K is the sum of the products of precision at each rank\\nk and a binary relevance indicator v_k, divided by the total number'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 3}, page_content='precision at K is the sum of the products of precision at each rank\\nk and a binary relevance indicator v_k, divided by the total number\\nof relevant items in the top K results. Precision at each rank k is\\ndetermined by the ratio of true positives at k to the sum of true\\npositives and false positives at k. This metric helps in assessing\\nhow well the context supports the ground truth, aiming for higher\\nscores which indicate better precision.\\n2.4.4 Context Recall. It is a metric used to evaluate how well the\\nretrieved context aligns with the ground truth answer, which is\\nconsidered the definitive correct response4. It is quantified by com-\\nparing each sentence in the ground truth answer to see if it can\\nbe traced back to the retrieved context. The formula for context\\nrecall is the ratio of the number of ground truth sentences that can\\nbe attributed to the context to the total number of sentences in\\nthe ground truth. Higher values, ranging from 0 to 1, indicate bet-'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 3}, page_content='be attributed to the context to the total number of sentences in\\nthe ground truth. Higher values, ranging from 0 to 1, indicate bet-\\nter alignment and thus better context recall. This metric is crucial\\nfor assessing the effectiveness of information retrieval systems in\\nproviding relevant context.\\n3 DATA DESCRIPTION\\nAlthough there do exist some public financial datasets, none of them\\nwere suitable for the present experiments: e.g., FinQA [ 31], TAT-\\nQA [ 32], FIQA [ 33], FinanceBench [ 34], etc. datasets are limited to\\nspecific usecases such as benchmarking LLMs’ abilities to perform\\ncomplex numerical reasoning or sentiment analysis. On the other\\nhand, FinTextQA [ 35] dataset was not publicly available at the\\ntime of writing the present work. In addition, in most of these\\ndatasets, access to the actual documents from which the ground-\\ntruth Q&As were created is not available, making it impossible to\\nuse them for our RAG techniques evaluation purposes. Hence, we'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 3}, page_content='datasets, access to the actual documents from which the ground-\\ntruth Q&As were created is not available, making it impossible to\\nuse them for our RAG techniques evaluation purposes. Hence, we\\nresorted to a dataset of our own though through publicly available\\n2https://platform.openai.com/docs/guides/embeddings/embedding-models\\n3https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html\\n4https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 4}, page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\\ndocuments but such that we finally have access to both the actual\\nfinancial documents and the ground-truth Q&As. Datasets like\\nFinanceBench5provides question-context-answer triplets but they\\nare not useful as here we are comparing VectorRAG, GraphRAG\\nand HybridRAG and they do not provide the context generated\\nfrom a KG. A recent paper [ 22] has not made the KG and triplets\\nconstructed by their algorithm public to the best of our knowledge\\neither.\\nIn short, there is no publicly available benchmark dataset to\\ncompare VectorRAG and GraphRAG either for financial or general\\ndomains to the best of our knowledge. Hence, we had to rely on\\nour own dataset constructed as explained below.\\nWe used transcripts from earnings calls of Nifty 50 constituents\\nfor our analysis. The NIFTY 50 is popular index in the Indian stock'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 4}, page_content='our own dataset constructed as explained below.\\nWe used transcripts from earnings calls of Nifty 50 constituents\\nfor our analysis. The NIFTY 50 is popular index in the Indian stock\\nmarket that represents the weighted average of 50 of the largest\\nIndian companies listed on the National Stock Exchange (NSE).\\nThe dataset of the earning call documents of Nifty 50 companies is\\nwidely recognized in the investment realm and is esteemed as an\\nauthoritative and extensive collection of earnings call transcripts.\\nIn our investigation, we focus on data spanning the quarter ending\\nin June, 2023 i.e. the earnings reports for Q1 of the financial year\\n2024 (A financial year in India starts on the 1st April and ends in\\n31st March, so the quarter from 1st April to 30th June is the first\\nquarter of 2024 for the Indian market).\\nOur dataset encompasses 50 transcripts for this quarter, span-\\nning over 50 companies within Nifty 50 universe from diverse'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 4}, page_content='quarter of 2024 for the Indian market).\\nOur dataset encompasses 50 transcripts for this quarter, span-\\nning over 50 companies within Nifty 50 universe from diverse\\nrange of sectors including Infrastructure, Healthcare, Consumer\\nDurables, Banking, Automobile, Financial Services, Energy - Oil\\n& Gas, Telecommunication, Consumer Goods, Pharmaceuticals,\\nEnergy - Coal, Materials, Information Technology, Construction,\\nDiversified, Metals, Energy - Power and Chemicals providing a\\nsubstantial and diverse foundation for our study.\\nWe start the data collection process focused on acquiring earn-\\nings reports from company websites within the Nifty 50 universe by\\ndeveloping and deploying a custom web scraping tool to navigate\\nthrough the websites of each company within the Nifty 50 index,\\nsystematically retrieving the pertinent earnings reports for Q1 of\\nthe financial year 2024. By utilizing this web scraping approach,\\nwe aimed to compile a comprehensive dataset encompassing the'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 4}, page_content='systematically retrieving the pertinent earnings reports for Q1 of\\nthe financial year 2024. By utilizing this web scraping approach,\\nwe aimed to compile a comprehensive dataset encompassing the\\nearnings reports of the constituent companies.\\nTable 1 summarizes basic statistics of the documents we will be\\nexperimenting with in the remainder of this work.\\nNumber of companies/documents 50\\nAverage number of pages 27\\nAverage number of questions 16\\nAverage number of tokens 60,000\\nTable 1: Summary Statistics for the call transcript documents\\nused in the present work.\\nThese call transcripts documents consist of questions and an-\\nswers between financial analysts and the company representatives\\nfor the respective companies, hence, there already exist certain\\nQ&A pairs within these documents along with additional text. We\\nexamined the earnings reports within the Nifty50 universe, system-\\natically curated a comprehensive array of randomly selected 400'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 4}, page_content='Q&A pairs within these documents along with additional text. We\\nexamined the earnings reports within the Nifty50 universe, system-\\natically curated a comprehensive array of randomly selected 400\\n5https://huggingface.co/datasets/PatronusAI/financebench-testquestions posed during the earnings calls from all the documents,\\nand gathered the exact responses corresponding to these questions.\\nThese questions constitute the specific queries articulated by finan-\\ncial analysts to the management during these calls.\\n4 IMPLEMENTATION DETAILS\\nIn this Section, we provide details of implementation of the pro-\\nposed methodology.\\n4.1 Knowledge Graph Construction\\nThe initial phase of our approach centers on document preprocess-\\ning. We utilize the PyPDFLoader6to import PDF documents, which\\nare subsequently segmented into manageable chunks using the\\nRecursiveCharacterTextSplitter. This chunking strategy employs a'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 4}, page_content='ing. We utilize the PyPDFLoader6to import PDF documents, which\\nare subsequently segmented into manageable chunks using the\\nRecursiveCharacterTextSplitter. This chunking strategy employs a\\nsize of 2024 characters with an overlap of 204 characters, ensuring\\ncomprehensive coverage while maintaining context across segment\\nboundaries.\\nFollowing the preprocessing stage, we implement the two-tiered\\nlanguage model chain for content refinement and information ex-\\ntraction. It is not possible to include the exact prompt here due to\\nthe limited space, but a baseline prompt can be found in Ref. [18].\\nEntity Type Examples\\nCompanies and Corpo-\\nrationsOfficial names, abbreviations, infor-\\nmal references\\nFinancial Metrics and\\nIndicatorsRevenue, profit margins, EBITDA\\nCorporate Executives\\nand Key PersonnelCEOs, CFOs, board members\\nProducts and Services Tangible products and intangible ser-\\nvices\\nGeographical Locations Headquarters, operational regions,\\nmarkets'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 4}, page_content='Corporate Executives\\nand Key PersonnelCEOs, CFOs, board members\\nProducts and Services Tangible products and intangible ser-\\nvices\\nGeographical Locations Headquarters, operational regions,\\nmarkets\\nCorporate Events Mergers, acquisitions, product\\nlaunches, earnings calls\\nLegal and Regulatory\\nInformationLegal cases, regulatory compliance\\nTable 2: Entities extracted from earnings call transcripts\\nTable 2 summarizes details on entities extracted from the earn-\\ning calls transcripts using our prompt based method. Concurrently,\\nLLM identifies relationships between these entities using a curated\\nset of verbs, capturing the nuanced interactions within the cor-\\nporate narrative. A key improvement in our methodology is the\\nenhanced prompt engineering to generate the structured output\\nformat for knowledge triplets. Each triplet is represented as a nested\\nlist [’h’, ’type’, ’r’, ’o’, ’type’, ’metadata’], where ’h’ and ’o’ denote\\nthe head and object entities respectively, ’type’ specifies the entity'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 4}, page_content='list [’h’, ’type’, ’r’, ’o’, ’type’, ’metadata’], where ’h’ and ’o’ denote\\nthe head and object entities respectively, ’type’ specifies the entity\\ncategory, ’r’ represents the relationship, and ’metadata’ encapsu-\\nlates additional contextual information. This format allows for a\\nrich, multidimensional representation of information, facilitating\\nmore nuanced downstream analysis.\\n6https://python.langchain.com/v0.1/docs/modules/data_connection/document_\\nloaders/pdf/'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 5}, page_content='Sarmah et al.\\nOur process incorporates several advanced features to enhance\\nthe quality and utility of the extracted triplets. Entity disambigua-\\ntion techniques are employed to consolidate different references to\\nthe same entity, improving consistency across the KG. We also pri-\\noritize conciseness in entity representation, aiming for descriptions\\nof less than four words where possible, which aids in maintaining\\na clean and navigable graph structure.\\nThe extraction pipeline is applied iteratively to each document\\nchunk, with results aggregated to form a comprehensive knowledge\\ngraph representation of the entire document, allowing for scalable\\nprocessing of large documents while maintaining local context\\nwithin each chunk. We have added explicit instruction on obtaining\\nmetadata following Ref. [8] for both VectorRAG and GraphRAG.\\nFinally, we implement a data persistence strategy, converting\\nthe extracted triplets from their initial string format to Python data'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 5}, page_content='metadata following Ref. [8] for both VectorRAG and GraphRAG.\\nFinally, we implement a data persistence strategy, converting\\nthe extracted triplets from their initial string format to Python data\\nstructures and storing them in a pickle file. This facilitates easy\\nretrieval and further manipulation of the knowledge graph data in\\nsubsequent analysis stages.\\nOur methodology represents a significant advancement in auto-\\nmated knowledge extraction from corporate documents. By com-\\nbining advanced NLP techniques with a structured approach to\\ninformation representation, we create a rich, queryable knowledge\\nbase that captures the complex relationships and key information\\npresent in corporate narratives. This approach opens up new pos-\\nsibilities for financial analysis and automated reasoning in the\\nbusiness domain that will be explored further in the future.\\n4.2 VectorRAG\\nOur methodology builds upon the concept of RAG [ 9] which al-'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 5}, page_content='business domain that will be explored further in the future.\\n4.2 VectorRAG\\nOur methodology builds upon the concept of RAG [ 9] which al-\\nlows for the creation of a system that can provide context-aware,\\naccurate responses to queries about company financial informa-\\ntion, leveraging both the power of large language models and the\\nefficiency of semantic search.\\nAt the core of our system is a Pinecone vector database7, which\\nserves as the foundation for our information retrieval process. We\\nemploy OpenAI’s text-embedding-ada-002 model to transform tex-\\ntual data from earnings call transcripts into high-dimensional vec-\\ntor representations. This vectorization process enables semantic\\nsimilarity searches, significantly enhancing the relevance and ac-\\ncuracy of retrieved information. Table 3 provides summary of the\\nconfiguration of the set up in use for our experiments.\\nLLM GPT-3.5-Turbo\\nLLM Temperature 0\\nEmbedding Model text-embedding-ada-002\\nFramework LangChain\\nVector Database Pinecone'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 5}, page_content='configuration of the set up in use for our experiments.\\nLLM GPT-3.5-Turbo\\nLLM Temperature 0\\nEmbedding Model text-embedding-ada-002\\nFramework LangChain\\nVector Database Pinecone\\nChunk Size 1024\\nChunk Overlap 0\\nMaximum Output Tokens 1024\\nChunks for Similarity Algorithm 20\\nNumber of Context Retrieved 4\\nTable 3: VectorRAG Configuration\\n7https://www.pinecone.io/The Q&A pipeline is constructed using the LangChain frame-\\nwork8. The begins with a context retrieval step, where we query the\\nPinecone vector store to obtain the most relevant document chunks\\nfor a given question. This retrieval process is fine-tuned with spe-\\ncific filters for quarters, years, and company names, ensuring that\\nthe retrieved information is both relevant and current.\\nFollowing retrieval, we implement a context formatting step that\\nconsolidates the retrieved document chunks into a coherent context\\nstring. This formatted context serves as the informational basis for'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 5}, page_content='consolidates the retrieved document chunks into a coherent context\\nstring. This formatted context serves as the informational basis for\\nthe language model’s response generation. We have developed a\\nsophisticated prompt template, that instructs the language model\\nto function as an expert Q&A system, emphasizing the importance\\nof utilizing only the provided context information and avoiding\\ndirect references to the context in the generated responses.\\nFor the core language processing task, we integrate OpenAI’s\\nGPT-3.5-turbo model which processes the formatted context and\\nquery to generate natural language responses that are informative,\\ncoherent, and contextually appropriate.\\nTo evaluate the performance of our system, we developed a'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 5}, page_content='query to generate natural language responses that are informative,\\ncoherent, and contextually appropriate.\\nTo evaluate the performance of our system, we developed a\\ncomprehensive framework that includes the preparation of a cus-\\ntom dataset of question-answer pairs specific to each company’s\\nearnings call. Our system processes each question in this dataset,\\ngenerating answers based on the retrieved context. The evaluation\\nresults, including the original question, generated answer, retrieved\\ncontexts, and ground truth, are compiled into structured formats\\n(CSV and JSON) to facilitate further analysis. The outputs generated\\nby our system are stored in both CSV and JSON formats, enabling\\neasy integration with various analysis tools and dashboards. This\\napproach facilitates both quantitative performance metrics and\\nqualitative assessment of the system’s responses, providing a com-\\nprehensive view of its effectiveness.\\nBy parameterizing company names, quarters, and years, we can'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 5}, page_content='qualitative assessment of the system’s responses, providing a com-\\nprehensive view of its effectiveness.\\nBy parameterizing company names, quarters, and years, we can\\neasily adapt the system to different datasets and time periods. This\\ndesign choice allows for seamless integration of new data and\\nexpansion to cover multiple companies and earnings calls.\\n4.3 GraphRAG\\nFor GraphRAG, we developed an Q&A system specifically designed\\nfor corporate earnings call transcripts. Our implementation of\\nGraphRAG leverages several key components and techniques:\\nLLM GPT-3.5-Turbo\\nLLM Temperature 0\\nFramework LangChain\\nKG Manipulation Networkx\\nChunk Size 1024\\nChunk Overlap 0\\nNumber of Triplets 13950\\nNumber of nodes 11405\\nNumber of edges 13883\\nDFS Depth 1\\nTable 4: GraphRAG Configuration\\n8https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 6}, page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\\nKnowledge Graph Construction:- We begin by constructing\\na KG from a set of knowledge triplets extracted from corporate\\ndocuments using the prompt engineering based methodology as\\ndescribed in Section 4.1. These triplets, stored in a pickle file, repre-\\nsent structured information in the form of subject-predicate-object\\nrelationships. We use the NetworkxEntityGraph class from the\\nLangChain library to create and manage this graph structure. Each\\ntriple is added to the graph, which encapsulates the head entity,\\nrelation, tail entity, and additional metadata.\\nWe implement the Q&A functionality using the GraphQAChain\\nclass from LangChain. This chain combines the KG with an LLM\\n(in our case, OpenAI’s GPT-3.5-turbo) to generate responses. The\\nGraphQAChain traverses the KG to find relevant information and\\nuses the language model to formulate coherent answers based on'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 6}, page_content='(in our case, OpenAI’s GPT-3.5-turbo) to generate responses. The\\nGraphQAChain traverses the KG to find relevant information and\\nuses the language model to formulate coherent answers based on\\nthe retrieved context. A summary of configuration of our LLM\\nmodels and other libraries used for GraphRAG is shown in Table 4.\\nIn KG as the information is stored in the form of entities and\\nrelationships and there can be multiple relations emanating from\\none single entity, in this experiment, to extract relevant information\\nfrom the KG, we employ a depth-first search strategy constrained\\nby a depth of one from the specified entity.\\nTo prepare for assessing the performance of our GraphRAG\\nsystem, we follow the below steps: Dataset Preparation:- We use\\na pre-generated CSV file containing question-answer pairs specific\\nto the earnings call transcript.\\nIterative Processing:- For each question in the dataset, we run\\nthe GraphQAChain to generate an answer.'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 6}, page_content='to the earnings call transcript.\\nIterative Processing:- For each question in the dataset, we run\\nthe GraphQAChain to generate an answer.\\nResult Compilation:- We compile the results, including the origi-\\nnal questions, generated answers, retrieved contexts, and ground\\ntruth answers, into a structured format.\\nFinally, the evaluation results are saved in both CSV and JSON\\nformats for further analysis and comparison. We then fed these\\noutputs into our RAG evaluation pipeline. For each Q&A pair in\\nour dataset, we compute all three metrics: faithfulness, answer\\nrelevance, context precision and context recall.\\n4.4 HybridRAG\\nFor the proposed HybridRAG technique, upon obtaining all the\\ncontextual information from VectorRAG and GraphRAG, we con-\\ncatenate these contexts to form a unified context utilizing both\\ntechniques. This combined context is then fed into the answer gen-\\nerator model to produce a response. The context used for response'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 6}, page_content='catenate these contexts to form a unified context utilizing both\\ntechniques. This combined context is then fed into the answer gen-\\nerator model to produce a response. The context used for response\\ngeneration is relatively larger, which affects the precision of the\\ngenerated response. The context from VectorRAG is appended first,\\nfollowed by the context from GraphRAG. Consequently, the preci-\\nsion of the generated answer depends on the source context. If the\\nanswer is generated from the GraphRAG context, it will have lower\\nprecision, as the GraphRAG context is added last in the sequence of\\ncontexts provided to the answer generator model, and vice versa.\\n5 RESULTS\\nWe evaluate both the retrieval and generation parts of RAG for\\nthe three different RAG pipelines. Evaluating the RAG outputs is\\nalso an active area of research there is no standard tool which is\\nuniversally accepted as of yet, though we use a currently popular'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 6}, page_content='the three different RAG pipelines. Evaluating the RAG outputs is\\nalso an active area of research there is no standard tool which is\\nuniversally accepted as of yet, though we use a currently popular\\nframework RAGAS[ 30] to evaluate the three RAG pipelines in thepresent work where we have modified them a bit to make more\\nprecise comparisons.\\nThe results of our comparative analysis reveal notable differences\\nin performance among VectorRAG, GraphRAG, and HybridRAG\\napproaches as summarized in Table 5. In terms of Faithfulness,\\nGraphRAG and HybridRAG demonstrated superior performance,\\nboth achieving a score of 0.96, while VectorRAG trailed slightly with\\na score of 0.94. Answer relevancy scores varied across the meth-\\nods, with HybridRAG outperforming the others at 0.96, followed\\nby VectorRAG at 0.91, and GraphRAG at 0.89. Context precision'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 6}, page_content='a score of 0.94. Answer relevancy scores varied across the meth-\\nods, with HybridRAG outperforming the others at 0.96, followed\\nby VectorRAG at 0.91, and GraphRAG at 0.89. Context precision\\nwas highest for GraphRAG at 0.96, significantly surpassing Vec-\\ntorRAG (0.84) and HybridRAG (0.79). However, in context recall,\\nboth VectorRAG and HybridRAG achieved perfect scores of 1, while\\nGraphRAG lagged behind at 0.85.\\nOverall, these results suggest that GraphRAG offers improve-\\nments over VectorRAG, particularly in faithfulness and context pre-\\ncision. Furthermore, HybridRAG emerges as the most balanced and\\neffective approach, outperforming both VectorRAG and GraphRAG\\nin key metrics such as faithfulness and answer relevancy, while\\nmaintaining high context recall.\\nThe relatively lower context precision observed for HybridRAG\\n(0.79) can be attributed to its unique approach of combining con-\\ntexts from both VectorRAG and GraphRAG methods. While this'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 6}, page_content='The relatively lower context precision observed for HybridRAG\\n(0.79) can be attributed to its unique approach of combining con-\\ntexts from both VectorRAG and GraphRAG methods. While this\\nintegration allows for more comprehensive information retrieval,\\nit also introduces additional content that may not align precisely\\nwith the ground truth, thus affecting the context precision met-\\nric. Despite this trade-off, HybridRAG’s superior performance in\\nfaithfulness, answer relevancy, and context recall underscores its\\neffectiveness. When considering the overall evaluation metrics,\\nHybridRAG emerges as the most promising approach, balancing\\nhigh-quality answers with comprehensive context retrieval.\\nOverall GraphRAG performs better in extractive questions com-\\npared to VectorRAG. And VectorRAG does better in abstractive\\nquestions where information is not explicitly mentioned in the\\nraw data. And also GraphRAG sometimes fails to answer ques-'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 6}, page_content='pared to VectorRAG. And VectorRAG does better in abstractive\\nquestions where information is not explicitly mentioned in the\\nraw data. And also GraphRAG sometimes fails to answer ques-\\ntions correctly whenever there is no entity explicitly mentioned in\\nthe question. So HybridRAG does a good job overall, as whenever\\nVectorRAG fails to fetch correct context in extractive questions\\nit falls back to GraphRAG to generate the answer. And whenever\\nGraphRAG fails to fetch correct context in abstractive questions it\\nfalls back to VectorRAG to generate the answer.\\nVectorRAG GraphRAG HybridRAG\\nF 0.94 0.96 0.96\\nAR 0.91 0.89 0.96\\nCP 0.84 0.96 0.79\\nCR 1 0.85 1\\nTable 5: Performance Metrics for Different RAG Pipelines.\\nHere, F, AR, CP and CR refer to Faithfulness, Answer\\nRelevence, Context Precision and Context Recall.\\n6 CONCLUSION AND FUTURE DIRECTIONS\\nAmong the current approaches to mitigate issues regarding infor-\\nmation extraction from external documents that were not part of'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='Sarmah et al.\\ntraining data for the LLM, Retrieval Augmented Generation (RAG)\\ntechniques have emerged as the most popular ones that aim to\\nimprove the performance of LLMs by incorporating relevant re-\\ntrieval mechanisms. RAG methods enhance the LLMs’ capabilities\\nby retrieving pertinent documents or text to provide additional\\ncontext during the generation process. However, these approaches\\nencounter significant limitations when applied to the specialized\\nand intricate domain of financial documents. Furthermore, the qual-\\nity of the retrieved context from a vast and heterogeneous corpus\\ncan be inconsistent, leading to inaccuracies and incomplete analyses.\\nThese challenges highlight the need for more sophisticated methods\\nthat can effectively integrate and process the detailed and domain-\\nspecific information found in financial documents, ensuring more\\nreliable and accurate outputs for informed decision-making.\\nIn the present work, we have introduced a novel approach that'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='specific information found in financial documents, ensuring more\\nreliable and accurate outputs for informed decision-making.\\nIn the present work, we have introduced a novel approach that\\nsignificantly advances the field of information extraction from finan-\\ncial documents through the development of a hybrid RAG system.\\nThis system, called HybridRAG, which integrates the strengths\\nof both Knowledge Graphs (KGs) and advanced language models,\\nrepresents a leap forward in our ability to extract and interpret\\ncomplex information from unstructured financial texts. The hybrid\\nRAG system, by combining traditional vector-based RAG and KG-\\nbased RAG, has shown superior performance in terms of retrieval\\naccuracy and answer generation, marking a pivotal step towards\\nmore effective financial analysis tools.\\nThrough a comparative analysis using objecive evaluation met-\\nrics, we have highlighted the distinct performance advantages of\\nthe HybridRAG approach over its vector-based and KG-based coun-'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='Through a comparative analysis using objecive evaluation met-\\nrics, we have highlighted the distinct performance advantages of\\nthe HybridRAG approach over its vector-based and KG-based coun-\\nterparts. The HybridRAG system excels in faithfulness, answer\\nrelevancy, and context recall, demonstrating the benefits of inte-\\ngrating contexts from both VectorRAG and GraphRAG methods,\\ndespite potential trade-offs in context precision.\\nThe implications of this research extend beyond the immedi-\\nate realm of financial analysis. By developing a system capable\\nof understanding and responding to nuanced queries about com-\\nplex financial documents, we pave the way for more sophisticated\\nAI-assisted financial decision-making tools that could potentially\\ndemocratize access to financial insights, allowing a broader range of\\nstakeholders to engage with and understand financial information.\\nFuture directions for this research include expanding the system'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='stakeholders to engage with and understand financial information.\\nFuture directions for this research include expanding the system\\nto handle multi-modal inputs, incorporating numerical data analysis\\ncapabilities, and developing more sophisticated evaluation metrics\\nthat capture the nuances of financial language and the accuracy\\nof numerical information in the responses. Additionally, exploring\\nthe integration of this system with real-time financial data streams\\ncould further enhance its utility in dynamic financial environments.\\n7 ACKNOWLEDGEMENT\\nThe views expressed here are those of the authors alone and not of\\nBlackRock, Inc or NVIDIA. We are grateful to Emma Lind for her\\ninvaluable support for this collaboration.\\nREFERENCES\\n[1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation\\nof word representations in vector space. 2013.\\n[2]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='of word representations in vector space. 2013.\\n[2]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.\\nAdvances inneural information processing systems, 30, 2017.[3]Yi Yang, Mark Christopher Siy Uy, and Allen Huang. Finbert: A pretrained\\nlanguage model for financial communications. arXiv preprint arXiv:2006.08097 ,\\n2020.\\n[4]Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Se-\\nbastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon\\nMann. Bloomberggpt: A large language model for finance. arXiv preprint\\narXiv:2303.17564, 2023.\\n[5] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qing-'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='Mann. Bloomberggpt: A large language model for finance. arXiv preprint\\narXiv:2303.17564, 2023.\\n[5] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qing-\\nsong Wen, and Stefan Zohren. A survey of large language models for financial ap-\\nplications: Progress, prospects and challenges. arXiv preprint arXiv:2406.11903 ,\\n2024.\\n[6]Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu,\\nShaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, Ninghao Liu, and Tianming\\nLiu. Revolutionizing finance with llms: An overview of applications and insights,\\n2024.\\n[7] Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang\\nWang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. Domain spe-\\ncialization as the key to make large language models disruptive: A comprehensive\\nsurvey. arXiv preprint arXiv:2305.18703, 2023.\\n[8] Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, and Tianjie Zhu. Towards'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='survey. arXiv preprint arXiv:2305.18703, 2023.\\n[8] Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, and Tianjie Zhu. Towards\\nreducing hallucination in extracting information from financial reports using\\nlarge language models. In Proceedings oftheThird International Conference\\nonAI-ML Systems, pages 1–5, 2023.\\n[9]Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\\nRocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp\\ntasks. Advances inNeural Information Processing Systems , 33:9459–9474, 2020.\\n[10] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative\\nmodels for open domain question answering. arXiv preprint arXiv:2007.01282 ,\\n2021.\\n[11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.\\nRealm: Retrieval-augmented language model pre-training. arXiv preprint\\narXiv:2002.08909, 2020.'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='2021.\\n[11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.\\nRealm: Retrieval-augmented language model pre-training. arXiv preprint\\narXiv:2002.08909, 2020.\\n[12] Antonio Jose Jimeno Yepes et al. Financial report chunking for effective retrieval\\naugmented generation. arXiv preprint arXiv:2402.05131, 2024.\\n[13] SuperAcc. Retrieval-augmented generation on financial statements. SuperAcc\\nInsights, 2024.\\n[14] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A\\nsurvey on knowledge graphs: Representation, acquisition, and applications. IEEE\\ntransactions onneural networks andlearning systems, 33(2):494–514, 2021.\\n[15] Heiko Paulheim. Knowledge graphs: State of the art and future directions.\\nSemantic Web Journal, 10(4):1–20, 2017.\\n[16] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia D’Amato, Gerard de Melo,\\nClaudio Gutiérrez, and Maria ... Maleshkova. Knowledge graphs. arXiv preprint\\narXiv:2003.02320, 2021.'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='[16] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia D’Amato, Gerard de Melo,\\nClaudio Gutiérrez, and Maria ... Maleshkova. Knowledge graphs. arXiv preprint\\narXiv:2003.02320, 2021.\\n[17] Lisa Ehrlinger and Wolfram Wöß. Towards a definition of knowledge graphs. In\\nSEMANTiCS (Posters, Demos, SuCCESS), pages 1–4, 2016.\\n[18] Xiaohui Victor Li and Francesco Sanna Passino. Findkg: Dynamic knowledge\\ngraphs with large language models for detecting global trends in financial mar-\\nkets. arXiv preprint arXiv:2407.10909, 2024.\\n[19] Shourya De, Anima Aggarwal, and Anna Cinzia Squicciarini. Financial knowl-\\nedge graphs: A novel approach to empower data-driven financial applications. In\\nIEEE International Conference onBigData (Big Data), pages 1311–1320, 2018.\\n[20] Marina Petrova and Birgit Reinwald. Knowledge graphs in finance: Applications\\nand opportunities. Journal ofFinancial Data Science, 2(2):10–19, 2020.\\n[21] Wei Liu, Jun Zhang, and Li Pan. Utilizing knowledge graphs for financial data'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='and opportunities. Journal ofFinancial Data Science, 2(2):10–19, 2020.\\n[21] Wei Liu, Jun Zhang, and Li Pan. Utilizing knowledge graphs for financial data\\nintegration and analysis. Data Science Journal, 18(1):1–15, 2019.\\n[22] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva\\nMody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag\\napproach to query-focused summarization. arXiv preprint arXiv:2404.16130 ,\\n2024.\\n[23] Xuchen Yao, Yanan Sun, Zhen Huang, and Dong Li. Retrieval-augmented gener-\\nation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2101.07554 , 2021.\\n[24] Weizhi Zhao, Haoyu Chen, Kai Liu, and Jun Zhao. Graph-based retrieval-\\naugmented generation for open-domain question answering. In Proceedings'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 7}, page_content='[24] Weizhi Zhao, Haoyu Chen, Kai Liu, and Jun Zhao. Graph-based retrieval-\\naugmented generation for open-domain question answering. In Proceedings\\noftheAAAI Conference onArtificial Intelligence , volume 36, pages 3098–3106,\\n2022.\\n[25] Bill Yuchen Lin, Yuanhe Liu, Ming Shen, and Xiang Ren. Kgpt: Knowledge-\\ngrounded pre-training for data-to-text generation. In Findings oftheAssociation\\nforComputational Linguistics: EMNLP 2020, pages 710–724, 2020.\\n[26] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Ji-\\nawei Sun, and Haofen Wang. Retrieval-augmented generation for large language\\nmodels: A survey. arXiv preprint arXiv:2312.10997, 2023.\\n[27] Tyler Procko. Graph retrieval-augmented generation for large language models:\\nA survey. Available atSSRN, 2024.\\n[28] Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. A comprehensive\\nsurvey on automatic knowledge graph construction. ACM Computing Surveys ,\\n56(4):1–62, 2023.'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 8}, page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\\n[29] Ishani Mondal, Yufang Hou, and Charles Jochim. End-to-end nlp knowledge\\ngraph construction. arXiv preprint arXiv:2106.01167, 2021.\\n[30] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ra-\\ngas: Automated evaluation of retrieval augmented generation. arXiv preprint\\narXiv:2309.15217, 2023.\\n[31] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan\\nLangdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al.\\nFinqa: A dataset of numerical reasoning over financial data. arXiv preprint\\narXiv:2109.00122, 2021.\\n[32] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott,\\nManel Zarrouk, and Alexandra Balahur. Www’18 open challenge: financial\\nopinion mining and question answering. In Companion proceedings ofthetheweb conference 2018, pages 1941–1942, 2018.'),\n",
              " Document(metadata={'source': '/content/hybridrag.pdf', 'page': 8}, page_content='Manel Zarrouk, and Alexandra Balahur. Www’18 open challenge: financial\\nopinion mining and question answering. In Companion proceedings ofthetheweb conference 2018, pages 1941–1942, 2018.\\n[33] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang,\\nJiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: A question answering\\nbenchmark on a hybrid of tabular and textual content in finance. arXiv preprint\\narXiv:2105.07624, 2021.\\n[34] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer,\\nand Bertie Vidgen. Financebench: A new benchmark for financial question\\nanswering. arXiv preprint arXiv:2311.11944, 2023.\\n[35] Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing\\nZhu, and Junwei Liang. Fintextqa: A dataset for long-form financial question\\nanswering. arXiv preprint arXiv:2405.09980, 2024.')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pdf_splits)):\n",
        "    print(\"Chunk \",i,\" : \",pdf_splits[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPLO2Hf4fJax",
        "outputId": "6419c05d-33f8-46a6-e920-ad81e2732573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk  0  :  page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval\n",
            "Augmented Generation for Efficient Information Extraction\n",
            "Bhaskarjit Sarmah\n",
            "bhaskarjit.sarmah@blackrock.com\n",
            "BlackRock, Inc.\n",
            "Gurugram, IndiaBenika Hall\n",
            "bhall@nvidia.com\n",
            "NVIDIA\n",
            "Santa Clara, CA, USARohan Rao\n",
            "rohrao@nvidia.com\n",
            "NVIDIA\n",
            "Santa Clara, CA, USA\n",
            "Sunil Patel\n",
            "supatel@nvidia.com\n",
            "NVIDIA\n",
            "Santa Clara, CA, USAStefano Pasquali\n",
            "stefano.pasquali@blackrock.com\n",
            "BlackRock, Inc.\n",
            "New York, NY, USADhagash Mehta\n",
            "dhagash.mehta@blackrock.com\n",
            "BlackRock, Inc.\n",
            "New York, NY, USA\n",
            "ABSTRACT\n",
            "Extraction and interpretation of intricate information from unstruc-\n",
            "tured text data arising in financial applications, such as earnings\n",
            "call transcripts, present substantial challenges to large language\n",
            "models (LLMs) even using the current best practices to use Re-\n",
            "trieval Augmented Generation (RAG) (referred to as VectorRAG\n",
            "techniques which utilize vector databases for information retrieval)\n",
            "due to challenges such as domain specific terminology and complex' metadata={'source': '/content/hybridrag.pdf', 'page': 0}\n",
            "Chunk  1  :  page_content='trieval Augmented Generation (RAG) (referred to as VectorRAG\n",
            "techniques which utilize vector databases for information retrieval)\n",
            "due to challenges such as domain specific terminology and complex\n",
            "formats of the documents. We introduce a novel approach based\n",
            "on a combination, called HybridRAG , of the Knowledge Graphs\n",
            "(KGs) based RAG techniques (called GraphRAG) and VectorRAG\n",
            "techniques to enhance question-answer (Q&A) systems for infor-\n",
            "mation extraction from financial documents that is shown to be\n",
            "capable of generating accurate and contextually relevant answers.\n",
            "Using experiments on a set of financial earning call transcripts\n",
            "documents which come in the form of Q&A format, and hence\n",
            "provide a natural set of pairs of ground-truth Q&As, we show that\n",
            "HybridRAG which retrieves context from both vector database and\n",
            "KG outperforms both traditional VectorRAG and GraphRAG indi-\n",
            "vidually when evaluated at both the retrieval and generation stages' metadata={'source': '/content/hybridrag.pdf', 'page': 0}\n",
            "Chunk  2  :  page_content='HybridRAG which retrieves context from both vector database and\n",
            "KG outperforms both traditional VectorRAG and GraphRAG indi-\n",
            "vidually when evaluated at both the retrieval and generation stages\n",
            "in terms of retrieval accuracy and answer generation. The proposed\n",
            "technique has applications beyond the financial domain.\n",
            "1 INTRODUCTION\n",
            "For the financial analyst, it is crucial to extract and analyze infor-\n",
            "mation from unstructured data sources like news articles, earnings\n",
            "reports, and other financial documents to have at least some chance\n",
            "to be on the better side of potential information asymmetry. These\n",
            "sources hold valuable insights that can impact investment decisions,\n",
            "market predictions, and overall sentiment. However, traditional\n",
            "data analysis methods struggle to effectively extract and utilize\n",
            "this information due to its unstructured nature. Large language\n",
            "models (LLMs) [ 1–4] have emerged as powerful tools for financial\n",
            "services and investment management. Their ability to process and' metadata={'source': '/content/hybridrag.pdf', 'page': 0}\n",
            "Chunk  3  :  page_content='this information due to its unstructured nature. Large language\n",
            "models (LLMs) [ 1–4] have emerged as powerful tools for financial\n",
            "services and investment management. Their ability to process and\n",
            "understand vast amounts of textual data makes them invaluable\n",
            "for tasks such as sentiment analysis, market trend predictions, and\n",
            "automated report generation. Specifically, extracting information\n",
            "from annual reports and other financial documents can greatly en-\n",
            "hance the efficiency and accuracy of financial analysts [ 5]. A robust\n",
            "information extraction system can help analysts quickly gather\n",
            "relevant data, identify market trends, and make informed decisions,\n",
            "leading to better investment strategies and risk management [6].Although LLMs have substantial potential in financial applica-\n",
            "tions, there are notable challenges in using pre-trained models to\n",
            "extract information from financial documents outside their training\n",
            "data while also reducing hallucination [ 7,8]. Financial documents' metadata={'source': '/content/hybridrag.pdf', 'page': 0}\n",
            "Chunk  4  :  page_content='extract information from financial documents outside their training\n",
            "data while also reducing hallucination [ 7,8]. Financial documents\n",
            "typically contain domain-specific language, multiple data formats,\n",
            "and unique contextual relationships that general purpose-trained\n",
            "LLMs do not handle well. In addition, extracting consistent and\n",
            "coherent information from multiple financial documents can be\n",
            "challenging due to variations in terminology, format, and context\n",
            "across different textual sources. The specialized terminology and\n",
            "complex data formats in financial documents make it difficult for\n",
            "models to extract meaningful insights, in turn, causing inaccurate\n",
            "predictions, overlooked insights, and unreliable analysis, which' metadata={'source': '/content/hybridrag.pdf', 'page': 0}\n",
            "Chunk  5  :  page_content='complex data formats in financial documents make it difficult for\n",
            "models to extract meaningful insights, in turn, causing inaccurate\n",
            "predictions, overlooked insights, and unreliable analysis, which\n",
            "ultimately hinder the ability to make well-informed decisions.\n",
            "Current approaches to mitigate these issues include various\n",
            "Retrieval-Augmented Generation (RAG) techniques [ 9], which aim\n",
            "to improve the performance of LLMs by incorporating relevant\n",
            "retrieval techniques. VectorRAG (the traditional RAG techniques\n",
            "that are based on vector databases) focuses on improving Natural\n",
            "Language Processing (NLP) tasks by retrieving relevant textual\n",
            "information to support the generation tasks. VectorRAG excels in\n",
            "situations where context from related textual documents is crucial\n",
            "for generating meaningful and coherent responses [ 9–11]. RAG-\n",
            "based methods ensure the LLMs generate relevant and coherent\n",
            "responses that are aligned with the original input query. How-' metadata={'source': '/content/hybridrag.pdf', 'page': 0}\n",
            "Chunk  6  :  page_content='for generating meaningful and coherent responses [ 9–11]. RAG-\n",
            "based methods ensure the LLMs generate relevant and coherent\n",
            "responses that are aligned with the original input query. How-\n",
            "ever, for financial documents, these approaches have significant\n",
            "challenges as a standalone solution. For instance, traditional RAG\n",
            "systems often use paragraph-level chunking techniques, which\n",
            "assume the text in those documents are uniform in length. This\n",
            "approach neglects the hierarchical nature of financial statements\n",
            "and can result in the loss of critical contextual information for an\n",
            "accurate analysis[ 12,13]. Due to the complexities in analyzing fi-\n",
            "nancial documents, the quality of the LLM retrieved-context from\n",
            "a vast and heterogeneous corpus can be inconsistent, leading to\n",
            "inaccuracies and incomplete analyses. These challenges demon-\n",
            "strate the need for more sophisticated methods that can effectively\n",
            "integrate and process the detailed and domain-specific information' metadata={'source': '/content/hybridrag.pdf', 'page': 0}\n",
            "Chunk  7  :  page_content='inaccuracies and incomplete analyses. These challenges demon-\n",
            "strate the need for more sophisticated methods that can effectively\n",
            "integrate and process the detailed and domain-specific information\n",
            "found in financial documents, ensuring more reliable and accurate\n",
            "results for informed decision-making.\n",
            "Knowledge graphs (KGs) [ 14] may provide a different point of\n",
            "view to looking at the financial documents where the documentsarXiv:2408.04948v1  [cs.CL]  9 Aug 2024' metadata={'source': '/content/hybridrag.pdf', 'page': 0}\n",
            "Chunk  8  :  page_content='Sarmah et al.\n",
            "are viewed as a collection of triplets of entities and their relation-\n",
            "ships as depicted in the text of the documents. KGs have become\n",
            "a pivotal technology in data management and analysis, provid-\n",
            "ing a structured way to represent knowledge through entities and\n",
            "relationships and have been widely adopted in various domains,\n",
            "including search engines, recommendation systems, and biomedical\n",
            "research [ 15–17]. The primary advantage of KGs lies in their ability\n",
            "to offer a structured representation, which facilitates efficient query-\n",
            "ing and reasoning. However, building and maintaining KGs and\n",
            "integrating data from different sources, such as documents, news\n",
            "articles, and other external sources, into a coherent knowledge\n",
            "graph poses significant challenges.\n",
            "The financial services industry has recognized the potential of\n",
            "KGs in enhancing data integration of heterogeneous data sources,\n",
            "risk management, and predictive analytics [ 18?–21]. Financial KGs' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Chunk  9  :  page_content='The financial services industry has recognized the potential of\n",
            "KGs in enhancing data integration of heterogeneous data sources,\n",
            "risk management, and predictive analytics [ 18?–21]. Financial KGs\n",
            "integrate various financial data sources such as market data, fi-\n",
            "nancial reports, and news articles, creating a comprehensive view\n",
            "of financial entities and their relationships. This unified view im-\n",
            "proves the accuracy and comprehensiveness of financial analysis,\n",
            "facilitates risk management by identifying hidden relationships,\n",
            "and supports advanced predictive analytics for more accurate mar-\n",
            "ket predictions and investment decisions. However, handling large\n",
            "volumes of financial data and continuously updating the knowledge\n",
            "graph to reflect the dynamic nature of financial markets can be\n",
            "challenging and resource-intensive.\n",
            "GraphRAG (Graph-based Retrieval-Augmented Generation) [ 22–\n",
            "27] is a novel approach that leverages knowledge graphs (KGs) to' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Chunk  10  :  page_content='challenging and resource-intensive.\n",
            "GraphRAG (Graph-based Retrieval-Augmented Generation) [ 22–\n",
            "27] is a novel approach that leverages knowledge graphs (KGs) to\n",
            "enhance the performance of NLP tasks such as Q&A systems. By\n",
            "integrating KGs with RAG techniques, GraphRAG enables more\n",
            "accurate and context-aware generation of responses based on the\n",
            "structured information extracted from financial documents. But\n",
            "GraphRAG generally underperforms in abstractive Q&A tasks or\n",
            "when there is not explicit entity mentioned in the question.\n",
            "In the present work, we propose a combination of VectorRAG and\n",
            "GraphRAG, called HybridRAG, to retrieve the relevant information\n",
            "from external documents for a given query to the LLM that brings\n",
            "advantages of both the RAGs together to provide demonstrably\n",
            "more accurate answers to the queries.\n",
            "1.1 Prior Work and Our Contribution\n",
            "VectorRAG has been extensively investigated in the recent years\n",
            "and focuses on enhancing NLP tasks by retrieving relevant textual' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Chunk  11  :  page_content='1.1 Prior Work and Our Contribution\n",
            "VectorRAG has been extensively investigated in the recent years\n",
            "and focuses on enhancing NLP tasks by retrieving relevant textual\n",
            "information to support generation processes [ 9–11,26]. However,\n",
            "the effectiveness of the retrieval mechanism across multiple docu-\n",
            "ments and longer contexts poses a significant challenge in extract-\n",
            "ing relevant responses. GraphRAG combines the capabilities of KGs\n",
            "with RAG to improve traditional NLP tasks [ 23–25]. Within our\n",
            "implementations of both VectorRAG GraphRAG techniques, we\n",
            "explicitly add information on the metadata of the documents that\n",
            "is also shown to improve the performance of VectorRAG [8].\n",
            "To the best of our knowledge the present work is the first work\n",
            "that proposes a RAG approach that is hybrid of both VectorRAG\n",
            "and GraphRAG and demonstrates its potential of more effective\n",
            "analysis and utilization of financial documents by leveraging the\n",
            "combined strengths of VectorRAG and GraphRAG. We also utilize a' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Chunk  12  :  page_content='and GraphRAG and demonstrates its potential of more effective\n",
            "analysis and utilization of financial documents by leveraging the\n",
            "combined strengths of VectorRAG and GraphRAG. We also utilize a\n",
            "novel ground-truth Q&A dataset extracted from publicly availablefinancial call transcripts of the companies included in the Nifty-50\n",
            "index which is an Indian stock market index that represents the\n",
            "weighted average of 50 of the largest Indian companies listed on\n",
            "the National Stock Exchange1.\n",
            "2 METHODOLOGY\n",
            "In this Section, we provide details of the proposed methodology\n",
            "by first discussing details of VectorRAG, then methodologies of\n",
            "constructing KGs from given documents and our proposed method-\n",
            "ology of GraphRAG and finally the HybridGraph technique.\n",
            "2.1 VectorRAG' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Chunk  13  :  page_content='by first discussing details of VectorRAG, then methodologies of\n",
            "constructing KGs from given documents and our proposed method-\n",
            "ology of GraphRAG and finally the HybridGraph technique.\n",
            "2.1 VectorRAG\n",
            "The traditional RAG [ 9] process begins with a query that is related\n",
            "to the information possessed within external document(s) that are\n",
            "not a part of the training dataset for the LLM. This query is used to\n",
            "search an external repository, such as a vector database or indexed\n",
            "corpus, to fetch relevant documents or passages containing useful\n",
            "information. These retrieved documents are then fed back into the\n",
            "LLM as additional context. Hence, in turn, for the given query, the\n",
            "language model generates a response based not only on its inter-\n",
            "nal training data but also by incorporating the retrieved external\n",
            "information. This integration ensures that the generated content is\n",
            "grounded in more recent and verifiable data, improving the accu-\n",
            "racy and contextual relevance of the responses. By combining the' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Chunk  14  :  page_content='information. This integration ensures that the generated content is\n",
            "grounded in more recent and verifiable data, improving the accu-\n",
            "racy and contextual relevance of the responses. By combining the\n",
            "retrieval of external information with the generative capabilities\n",
            "of large language models, RAG enhances the overall quality and\n",
            "reliability of the generated text.\n",
            "In traditional VectorRAG, the given external documents are di-\n",
            "vided into multiple chunks because of the limitation of context size\n",
            "of the language model. Those chunks are converted into embed-\n",
            "dings using an embeddings model and then stored into a vector\n",
            "database. After that, the retrieval component performs a similarity\n",
            "search within the vector database to identify and rank the chunks\n",
            "most relevant to the query. The top-ranked chunks are retrieved\n",
            "and aggregated to provide context for the generative model.\n",
            "Then, the generative model takes this retrieved context along' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Chunk  15  :  page_content='most relevant to the query. The top-ranked chunks are retrieved\n",
            "and aggregated to provide context for the generative model.\n",
            "Then, the generative model takes this retrieved context along\n",
            "with the original query and synthesizes a response. Thus, it merges\n",
            "the real-time information from the retrieved chunks with its pre-\n",
            "existing knowledge, ensuring that the response is both contextually\n",
            "relevant and detailed.\n",
            "The schematic diagram in Figure 1 provides details on the part\n",
            "of RAG that generates vector database from given external docu-\n",
            "ments in the traditional VectorRAG methodology where we also\n",
            "include explicit reference of metadata [8]. Section 4.2 will provide\n",
            "implementation details for our experiments.\n",
            "2.2 Knowledge Graph Construction\n",
            "A KG is a structured representation of real-world entities, their\n",
            "attributes, and their relations, usually stored in a graph database\n",
            "or a triplet store, i.e., a KG consists of nodes that represent entities' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Chunk  16  :  page_content='attributes, and their relations, usually stored in a graph database\n",
            "or a triplet store, i.e., a KG consists of nodes that represent entities\n",
            "and edges that represent relations, as well as labels and attributes\n",
            "for both. A graph triplet is a basic unit of information in a KG,\n",
            "consisting of a subject, a predicate, and an object.\n",
            "In most methodologies to build a KG from given documents,\n",
            "three main steps are involved: knowledge extraction, knowledge\n",
            "1https://www.nseindia.com/products/content/equities/indices/nifty_50.htm' metadata={'source': '/content/hybridrag.pdf', 'page': 1}\n",
            "Chunk  17  :  page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\n",
            "Figure 1: A schematic diagram describing the vector database\n",
            "creation of a RAG application.\n",
            "improvement, and knowledge adaptation [ 28]. Here, we do not use\n",
            "knowledge adaptation and treat the KGs as static graphs.\n",
            "Knowledge Extraction:- This step aims to extract structured in-\n",
            "formation from unstructured or semi-structured data, such as text,\n",
            "databases, and existing ontologies. The main tasks in this step are\n",
            "entity recognition, relationship extraction, and co-reference reso-\n",
            "lution. Entity recognition and relationship extraction techniques\n",
            "use typical NLP tasks to identify entities and their relationships\n",
            "from textual sources [ 29]. Coreference resolution identifies and\n",
            "connects different references of the same entity, keeping coherence\n",
            "within the knowledge graph. For example, if the text refers to a\n",
            "company as both \"the company\" and \"it\", coreference resolution' metadata={'source': '/content/hybridrag.pdf', 'page': 2}\n",
            "Chunk  18  :  page_content='connects different references of the same entity, keeping coherence\n",
            "within the knowledge graph. For example, if the text refers to a\n",
            "company as both \"the company\" and \"it\", coreference resolution\n",
            "can link these mentions to the same entity node in the graph.\n",
            "Knowledge Improvement:- This step aims to enhance the quality\n",
            "and completeness of the KG by removing redundancies and address-\n",
            "ing gaps in the extracted information. The primary tasks in this\n",
            "step are KG completion and fusion. KG completion technique infers\n",
            "missing entities and relationships within the graph using meth-\n",
            "ods such as link prediction and entity resolution. Link prediction\n",
            "predicts the existence and type of a relation between two entities\n",
            "based on the graph structure and features, while entity resolution\n",
            "matches and merges different representations of the same entity\n",
            "from different sources.\n",
            "Knowledge fusion combines information from multiple sources\n",
            "to create a coherent and unified KG. This involves resolving conflicts' metadata={'source': '/content/hybridrag.pdf', 'page': 2}\n",
            "Chunk  19  :  page_content='from different sources.\n",
            "Knowledge fusion combines information from multiple sources\n",
            "to create a coherent and unified KG. This involves resolving conflicts\n",
            "and redundancies among the sources, such as contradictory or\n",
            "duplicate facts, and aggregating or reconciling the information\n",
            "based on rules, probabilities, or semantic similarity.\n",
            "We utilized a robust methodology for creating KG triplets from\n",
            "unstructured text data, specifically focusing on corporate docu-\n",
            "ments such as earnings call transcripts, adapted from Ref. [ 18?].\n",
            "This process involves several interconnected stages, each designed\n",
            "to extract, refine, and structure information effectively.\n",
            "We implement a two-tiered LLM chain for content refinement\n",
            "and information extraction. The first tier employs an LLM to gen-\n",
            "erate an abstract representation of each document chunk. This\n",
            "refinement process is crucial as it distills the essential information\n",
            "while preserving the original meaning and key relationships be-' metadata={'source': '/content/hybridrag.pdf', 'page': 2}\n",
            "Chunk  20  :  page_content='erate an abstract representation of each document chunk. This\n",
            "refinement process is crucial as it distills the essential information\n",
            "while preserving the original meaning and key relationships be-\n",
            "tween concepts that serves as a more focused input for subsequent\n",
            "processing, enhancing the overall efficiency and accuracy of ourtriplet extraction pipeline. The second tier of our LLM chain is\n",
            "dedicated to entity extraction and relationship identification.\n",
            "Both the steps are executed using carefully performed prompt\n",
            "engineering on a pre-trained LLM. A detailed discussion on imple-\n",
            "mentation of the methodology will be provided in Section 4.1\n",
            "2.3 GraphRAG\n",
            "KG based RAG [ 22], or GraphRAG, also begins with a query based\n",
            "on the user’s input same as VectorRAG. The main difference be-\n",
            "tween VectorRAG and GraphRAG lies in the retrieval part. The\n",
            "query here is now used to search the KG to retrieve relevant nodes\n",
            "(entities) and edges (relationships) related to the query. A subgraph,' metadata={'source': '/content/hybridrag.pdf', 'page': 2}\n",
            "Chunk  21  :  page_content='tween VectorRAG and GraphRAG lies in the retrieval part. The\n",
            "query here is now used to search the KG to retrieve relevant nodes\n",
            "(entities) and edges (relationships) related to the query. A subgraph,\n",
            "which consists of these relevant nodes and edges, is extracted from\n",
            "the full KG to provide context. This subgraph is then integrated\n",
            "with the language model’s internal knowledge, by encoding the\n",
            "graph structure into embeddings that the model can interpret. The\n",
            "language model uses this combined context to generate responses\n",
            "that are informed by both the structured information from the KG\n",
            "and its pre-trained knowledge. Crucially, when responding to user\n",
            "queries about a particular company, we leveraged the metadata\n",
            "information to selectively filter and retrieve only those document' metadata={'source': '/content/hybridrag.pdf', 'page': 2}\n",
            "Chunk  22  :  page_content='and its pre-trained knowledge. Crucially, when responding to user\n",
            "queries about a particular company, we leveraged the metadata\n",
            "information to selectively filter and retrieve only those document\n",
            "segments pertinent to the queried company [ 8]. This integration\n",
            "helps ensure that the generated outputs are accurate, contextually\n",
            "relevant, and grounded in verifiable information.\n",
            "A schematic diagram of the retrieval methodology of GraphRAG\n",
            "is given in Figure 2. Here we first write a prompt to clean the\n",
            "data and then write another prompt in the second stage to create\n",
            "knowledge triplets along with metadata. It will be covered in more\n",
            "detail in section 4.1\n",
            "Figure 2: A schematic diagram describing knowledge graph\n",
            "creation process of GraphRAG.\n",
            "2.3.1 HybridRAG. For the HybridRAG technique, we propose to\n",
            "integrate the aforementioned two distinct RAG techniques: Vec-\n",
            "torRAG and GraphRAG. This integration involves a systematic\n",
            "combination of contextual information retrieved from both the' metadata={'source': '/content/hybridrag.pdf', 'page': 2}\n",
            "Chunk  23  :  page_content='integrate the aforementioned two distinct RAG techniques: Vec-\n",
            "torRAG and GraphRAG. This integration involves a systematic\n",
            "combination of contextual information retrieved from both the\n",
            "traditional vector-based retrieval mechanism and the KG-based\n",
            "retrieval system, the latter of which was constructed specifically\n",
            "for this study.\n",
            "The amalgamation of these two contexts allows us to leverage the\n",
            "strengths of both approaches. The VectorRAG component provides\n",
            "a broad, similarity-based retrieval of relevant information, while\n",
            "the GraphRAG element contributes structured, relationship-rich' metadata={'source': '/content/hybridrag.pdf', 'page': 2}\n",
            "Chunk  24  :  page_content='Sarmah et al.\n",
            "contextual data. This combined context is then utilized as input for a\n",
            "LLM to generate the final responses. Details on the implementation\n",
            "of the HybridRAG will be provided in Section 4.4.\n",
            "2.4 Evaluation Metrics\n",
            "To assess the efficacy of this integrated approach, we conducted a\n",
            "comparative analysis among the three approaches in a controlled ex-\n",
            "perimental set up: VectorRAG, GraphRAG and HybridRAG. The re-\n",
            "sponses generated using the combined VectorRAG and GraphRAG\n",
            "contexts were juxtaposed against those produced individually by\n",
            "VectorRAG and GraphRAG. This comparative evaluation aimed to\n",
            "discern potential improvements in response quality, accuracy, and\n",
            "comprehensiveness that might arise from the synergistic integra-\n",
            "tion of these two RAG methodologies.\n",
            "To objectively evaluate different RAG approaches (VectorRAG\n",
            "and GraphRAG in their case), Ref. [ 22] utilized metrics such as\n",
            "comprehensiveness (i.e., the amount of details the answer provides' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Chunk  25  :  page_content='To objectively evaluate different RAG approaches (VectorRAG\n",
            "and GraphRAG in their case), Ref. [ 22] utilized metrics such as\n",
            "comprehensiveness (i.e., the amount of details the answer provides\n",
            "to cover all aspects and details of the question?); diversity (i.e.,\n",
            "the richness of the answer in providing different perspectives and\n",
            "insights on the question); empowerment (i.e., the helpfulness of the\n",
            "answer to the reader understand and make informed judgements\n",
            "about the topic); and, directness (i.e., clearness of the answer in\n",
            "addressing the question). here, the LLM was provided tuples of\n",
            "question, target metric, and a pair of answers, and was asked to\n",
            "assess which answer was better according to the metric and why.\n",
            "These metrics though compare the final generated answers, do\n",
            "not necessarily directly evaluate the retrieval and generation parts\n",
            "separately. Instead, here we implement a comprehensive set of\n",
            "evaluation metrics which are designed to capture different aspects' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Chunk  26  :  page_content='not necessarily directly evaluate the retrieval and generation parts\n",
            "separately. Instead, here we implement a comprehensive set of\n",
            "evaluation metrics which are designed to capture different aspects\n",
            "of a given RAG system’s output quality, focusing on faithfulness,\n",
            "answer relevance, and context relevance [ 30]. Each metric provides\n",
            "unique insights into the system’s capabilities and limitations.\n",
            "2.4.1 Faithfulness. Faithfulness is a crucial metric that measures\n",
            "the extent to which the generated answer can be inferred from the\n",
            "provided context. Our implementation of the faithfulness metric\n",
            "involves a two-step process:\n",
            "Statement Extraction:- We use an LLM to decompose the gener-\n",
            "ated answer into a set of concise statements. This step is crucial\n",
            "for breaking down complex answers into more manageable and\n",
            "verifiable units. The prompt used for this step is:\n",
            "\"Given a question and answer, create one or more statements\n",
            "from each sentence in the given answer. question: [question] an-' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Chunk  27  :  page_content='verifiable units. The prompt used for this step is:\n",
            "\"Given a question and answer, create one or more statements\n",
            "from each sentence in the given answer. question: [question] an-\n",
            "swer: [answer]\".\n",
            "Statement Verification:- For each extracted statement, we employ\n",
            "the LLM to determine if it can be inferred from the given context.\n",
            "This verification process uses the following prompt:\n",
            "\"Consider the given context and following statements, then de-\n",
            "termine whether they are supported by the information present in\n",
            "the context. Provide a brief explanation for each statement before\n",
            "arriving at the verdict (Yes/No). Provide a final verdict for each\n",
            "statement in order at the end in the given format. Do not deviate\n",
            "from the specified format. statement: [statement 1] ... statement:\n",
            "[statement n]\".\n",
            "The faithfulness score ( 𝐹) is𝐹=|𝑉|/|𝑆|, where |𝑉|is the number\n",
            "of supported statements and |𝑆|is the total number of statements.2.4.2 Answer Relevance:-. The answer relevance metric assesses' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Chunk  28  :  page_content='The faithfulness score ( 𝐹) is𝐹=|𝑉|/|𝑆|, where |𝑉|is the number\n",
            "of supported statements and |𝑆|is the total number of statements.2.4.2 Answer Relevance:-. The answer relevance metric assesses\n",
            "how well the generated answer addresses the original question,\n",
            "irrespective of factual accuracy. This metric helps identify cases of\n",
            "incomplete answers or responses containing irrelevant information.\n",
            "Our implementation involves the following steps:\n",
            "Question Generation: We prompt the LLM to generate n potential\n",
            "questions based on the given answer:\n",
            "\"Generate a question for the given answer. answer: [answer]\".\n",
            "Then, we obtain embeddings for all generated questions and the\n",
            "original question using OpenAI’s text-embedding-ada-002 model2.\n",
            "We then calculate the cosine similarity between each generated' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Chunk  29  :  page_content='Then, we obtain embeddings for all generated questions and the\n",
            "original question using OpenAI’s text-embedding-ada-002 model2.\n",
            "We then calculate the cosine similarity between each generated\n",
            "question’s embedding and the original question’s embedding.\n",
            "Finally, the answer relevance score (AR) is computed as the aver-\n",
            "age similarity across all generated questions: 𝐴𝑅=1\n",
            "𝑛Í(𝑠𝑖𝑚(𝑞,𝑞𝑖)),\n",
            "where𝑠𝑖𝑚(𝑞,𝑞𝑖)is the cosine similarity between the embedding\n",
            "of the original question 𝑞and the embeddings of each of the 𝑛\n",
            "generated questions 𝑞𝑖.\n",
            "2.4.3 Context Precision. It is a metric used to evaluate the relevance\n",
            "of retrieved context chunks in relation to a specified ground truth\n",
            "for a given question3. It calculates the proportion of relevant items\n",
            "that appear in the top ranks of the context. The formula for context\n",
            "precision at K is the sum of the products of precision at each rank\n",
            "k and a binary relevance indicator v_k, divided by the total number' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Chunk  30  :  page_content='precision at K is the sum of the products of precision at each rank\n",
            "k and a binary relevance indicator v_k, divided by the total number\n",
            "of relevant items in the top K results. Precision at each rank k is\n",
            "determined by the ratio of true positives at k to the sum of true\n",
            "positives and false positives at k. This metric helps in assessing\n",
            "how well the context supports the ground truth, aiming for higher\n",
            "scores which indicate better precision.\n",
            "2.4.4 Context Recall. It is a metric used to evaluate how well the\n",
            "retrieved context aligns with the ground truth answer, which is\n",
            "considered the definitive correct response4. It is quantified by com-\n",
            "paring each sentence in the ground truth answer to see if it can\n",
            "be traced back to the retrieved context. The formula for context\n",
            "recall is the ratio of the number of ground truth sentences that can\n",
            "be attributed to the context to the total number of sentences in\n",
            "the ground truth. Higher values, ranging from 0 to 1, indicate bet-' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Chunk  31  :  page_content='be attributed to the context to the total number of sentences in\n",
            "the ground truth. Higher values, ranging from 0 to 1, indicate bet-\n",
            "ter alignment and thus better context recall. This metric is crucial\n",
            "for assessing the effectiveness of information retrieval systems in\n",
            "providing relevant context.\n",
            "3 DATA DESCRIPTION\n",
            "Although there do exist some public financial datasets, none of them\n",
            "were suitable for the present experiments: e.g., FinQA [ 31], TAT-\n",
            "QA [ 32], FIQA [ 33], FinanceBench [ 34], etc. datasets are limited to\n",
            "specific usecases such as benchmarking LLMs’ abilities to perform\n",
            "complex numerical reasoning or sentiment analysis. On the other\n",
            "hand, FinTextQA [ 35] dataset was not publicly available at the\n",
            "time of writing the present work. In addition, in most of these\n",
            "datasets, access to the actual documents from which the ground-\n",
            "truth Q&As were created is not available, making it impossible to\n",
            "use them for our RAG techniques evaluation purposes. Hence, we' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Chunk  32  :  page_content='datasets, access to the actual documents from which the ground-\n",
            "truth Q&As were created is not available, making it impossible to\n",
            "use them for our RAG techniques evaluation purposes. Hence, we\n",
            "resorted to a dataset of our own though through publicly available\n",
            "2https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
            "3https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html\n",
            "4https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html' metadata={'source': '/content/hybridrag.pdf', 'page': 3}\n",
            "Chunk  33  :  page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\n",
            "documents but such that we finally have access to both the actual\n",
            "financial documents and the ground-truth Q&As. Datasets like\n",
            "FinanceBench5provides question-context-answer triplets but they\n",
            "are not useful as here we are comparing VectorRAG, GraphRAG\n",
            "and HybridRAG and they do not provide the context generated\n",
            "from a KG. A recent paper [ 22] has not made the KG and triplets\n",
            "constructed by their algorithm public to the best of our knowledge\n",
            "either.\n",
            "In short, there is no publicly available benchmark dataset to\n",
            "compare VectorRAG and GraphRAG either for financial or general\n",
            "domains to the best of our knowledge. Hence, we had to rely on\n",
            "our own dataset constructed as explained below.\n",
            "We used transcripts from earnings calls of Nifty 50 constituents\n",
            "for our analysis. The NIFTY 50 is popular index in the Indian stock' metadata={'source': '/content/hybridrag.pdf', 'page': 4}\n",
            "Chunk  34  :  page_content='our own dataset constructed as explained below.\n",
            "We used transcripts from earnings calls of Nifty 50 constituents\n",
            "for our analysis. The NIFTY 50 is popular index in the Indian stock\n",
            "market that represents the weighted average of 50 of the largest\n",
            "Indian companies listed on the National Stock Exchange (NSE).\n",
            "The dataset of the earning call documents of Nifty 50 companies is\n",
            "widely recognized in the investment realm and is esteemed as an\n",
            "authoritative and extensive collection of earnings call transcripts.\n",
            "In our investigation, we focus on data spanning the quarter ending\n",
            "in June, 2023 i.e. the earnings reports for Q1 of the financial year\n",
            "2024 (A financial year in India starts on the 1st April and ends in\n",
            "31st March, so the quarter from 1st April to 30th June is the first\n",
            "quarter of 2024 for the Indian market).\n",
            "Our dataset encompasses 50 transcripts for this quarter, span-\n",
            "ning over 50 companies within Nifty 50 universe from diverse' metadata={'source': '/content/hybridrag.pdf', 'page': 4}\n",
            "Chunk  35  :  page_content='quarter of 2024 for the Indian market).\n",
            "Our dataset encompasses 50 transcripts for this quarter, span-\n",
            "ning over 50 companies within Nifty 50 universe from diverse\n",
            "range of sectors including Infrastructure, Healthcare, Consumer\n",
            "Durables, Banking, Automobile, Financial Services, Energy - Oil\n",
            "& Gas, Telecommunication, Consumer Goods, Pharmaceuticals,\n",
            "Energy - Coal, Materials, Information Technology, Construction,\n",
            "Diversified, Metals, Energy - Power and Chemicals providing a\n",
            "substantial and diverse foundation for our study.\n",
            "We start the data collection process focused on acquiring earn-\n",
            "ings reports from company websites within the Nifty 50 universe by\n",
            "developing and deploying a custom web scraping tool to navigate\n",
            "through the websites of each company within the Nifty 50 index,\n",
            "systematically retrieving the pertinent earnings reports for Q1 of\n",
            "the financial year 2024. By utilizing this web scraping approach,\n",
            "we aimed to compile a comprehensive dataset encompassing the' metadata={'source': '/content/hybridrag.pdf', 'page': 4}\n",
            "Chunk  36  :  page_content='systematically retrieving the pertinent earnings reports for Q1 of\n",
            "the financial year 2024. By utilizing this web scraping approach,\n",
            "we aimed to compile a comprehensive dataset encompassing the\n",
            "earnings reports of the constituent companies.\n",
            "Table 1 summarizes basic statistics of the documents we will be\n",
            "experimenting with in the remainder of this work.\n",
            "Number of companies/documents 50\n",
            "Average number of pages 27\n",
            "Average number of questions 16\n",
            "Average number of tokens 60,000\n",
            "Table 1: Summary Statistics for the call transcript documents\n",
            "used in the present work.\n",
            "These call transcripts documents consist of questions and an-\n",
            "swers between financial analysts and the company representatives\n",
            "for the respective companies, hence, there already exist certain\n",
            "Q&A pairs within these documents along with additional text. We\n",
            "examined the earnings reports within the Nifty50 universe, system-\n",
            "atically curated a comprehensive array of randomly selected 400' metadata={'source': '/content/hybridrag.pdf', 'page': 4}\n",
            "Chunk  37  :  page_content='Q&A pairs within these documents along with additional text. We\n",
            "examined the earnings reports within the Nifty50 universe, system-\n",
            "atically curated a comprehensive array of randomly selected 400\n",
            "5https://huggingface.co/datasets/PatronusAI/financebench-testquestions posed during the earnings calls from all the documents,\n",
            "and gathered the exact responses corresponding to these questions.\n",
            "These questions constitute the specific queries articulated by finan-\n",
            "cial analysts to the management during these calls.\n",
            "4 IMPLEMENTATION DETAILS\n",
            "In this Section, we provide details of implementation of the pro-\n",
            "posed methodology.\n",
            "4.1 Knowledge Graph Construction\n",
            "The initial phase of our approach centers on document preprocess-\n",
            "ing. We utilize the PyPDFLoader6to import PDF documents, which\n",
            "are subsequently segmented into manageable chunks using the\n",
            "RecursiveCharacterTextSplitter. This chunking strategy employs a' metadata={'source': '/content/hybridrag.pdf', 'page': 4}\n",
            "Chunk  38  :  page_content='ing. We utilize the PyPDFLoader6to import PDF documents, which\n",
            "are subsequently segmented into manageable chunks using the\n",
            "RecursiveCharacterTextSplitter. This chunking strategy employs a\n",
            "size of 2024 characters with an overlap of 204 characters, ensuring\n",
            "comprehensive coverage while maintaining context across segment\n",
            "boundaries.\n",
            "Following the preprocessing stage, we implement the two-tiered\n",
            "language model chain for content refinement and information ex-\n",
            "traction. It is not possible to include the exact prompt here due to\n",
            "the limited space, but a baseline prompt can be found in Ref. [18].\n",
            "Entity Type Examples\n",
            "Companies and Corpo-\n",
            "rationsOfficial names, abbreviations, infor-\n",
            "mal references\n",
            "Financial Metrics and\n",
            "IndicatorsRevenue, profit margins, EBITDA\n",
            "Corporate Executives\n",
            "and Key PersonnelCEOs, CFOs, board members\n",
            "Products and Services Tangible products and intangible ser-\n",
            "vices\n",
            "Geographical Locations Headquarters, operational regions,\n",
            "markets' metadata={'source': '/content/hybridrag.pdf', 'page': 4}\n",
            "Chunk  39  :  page_content='Corporate Executives\n",
            "and Key PersonnelCEOs, CFOs, board members\n",
            "Products and Services Tangible products and intangible ser-\n",
            "vices\n",
            "Geographical Locations Headquarters, operational regions,\n",
            "markets\n",
            "Corporate Events Mergers, acquisitions, product\n",
            "launches, earnings calls\n",
            "Legal and Regulatory\n",
            "InformationLegal cases, regulatory compliance\n",
            "Table 2: Entities extracted from earnings call transcripts\n",
            "Table 2 summarizes details on entities extracted from the earn-\n",
            "ing calls transcripts using our prompt based method. Concurrently,\n",
            "LLM identifies relationships between these entities using a curated\n",
            "set of verbs, capturing the nuanced interactions within the cor-\n",
            "porate narrative. A key improvement in our methodology is the\n",
            "enhanced prompt engineering to generate the structured output\n",
            "format for knowledge triplets. Each triplet is represented as a nested\n",
            "list [’h’, ’type’, ’r’, ’o’, ’type’, ’metadata’], where ’h’ and ’o’ denote\n",
            "the head and object entities respectively, ’type’ specifies the entity' metadata={'source': '/content/hybridrag.pdf', 'page': 4}\n",
            "Chunk  40  :  page_content='list [’h’, ’type’, ’r’, ’o’, ’type’, ’metadata’], where ’h’ and ’o’ denote\n",
            "the head and object entities respectively, ’type’ specifies the entity\n",
            "category, ’r’ represents the relationship, and ’metadata’ encapsu-\n",
            "lates additional contextual information. This format allows for a\n",
            "rich, multidimensional representation of information, facilitating\n",
            "more nuanced downstream analysis.\n",
            "6https://python.langchain.com/v0.1/docs/modules/data_connection/document_\n",
            "loaders/pdf/' metadata={'source': '/content/hybridrag.pdf', 'page': 4}\n",
            "Chunk  41  :  page_content='Sarmah et al.\n",
            "Our process incorporates several advanced features to enhance\n",
            "the quality and utility of the extracted triplets. Entity disambigua-\n",
            "tion techniques are employed to consolidate different references to\n",
            "the same entity, improving consistency across the KG. We also pri-\n",
            "oritize conciseness in entity representation, aiming for descriptions\n",
            "of less than four words where possible, which aids in maintaining\n",
            "a clean and navigable graph structure.\n",
            "The extraction pipeline is applied iteratively to each document\n",
            "chunk, with results aggregated to form a comprehensive knowledge\n",
            "graph representation of the entire document, allowing for scalable\n",
            "processing of large documents while maintaining local context\n",
            "within each chunk. We have added explicit instruction on obtaining\n",
            "metadata following Ref. [8] for both VectorRAG and GraphRAG.\n",
            "Finally, we implement a data persistence strategy, converting\n",
            "the extracted triplets from their initial string format to Python data' metadata={'source': '/content/hybridrag.pdf', 'page': 5}\n",
            "Chunk  42  :  page_content='metadata following Ref. [8] for both VectorRAG and GraphRAG.\n",
            "Finally, we implement a data persistence strategy, converting\n",
            "the extracted triplets from their initial string format to Python data\n",
            "structures and storing them in a pickle file. This facilitates easy\n",
            "retrieval and further manipulation of the knowledge graph data in\n",
            "subsequent analysis stages.\n",
            "Our methodology represents a significant advancement in auto-\n",
            "mated knowledge extraction from corporate documents. By com-\n",
            "bining advanced NLP techniques with a structured approach to\n",
            "information representation, we create a rich, queryable knowledge\n",
            "base that captures the complex relationships and key information\n",
            "present in corporate narratives. This approach opens up new pos-\n",
            "sibilities for financial analysis and automated reasoning in the\n",
            "business domain that will be explored further in the future.\n",
            "4.2 VectorRAG\n",
            "Our methodology builds upon the concept of RAG [ 9] which al-' metadata={'source': '/content/hybridrag.pdf', 'page': 5}\n",
            "Chunk  43  :  page_content='business domain that will be explored further in the future.\n",
            "4.2 VectorRAG\n",
            "Our methodology builds upon the concept of RAG [ 9] which al-\n",
            "lows for the creation of a system that can provide context-aware,\n",
            "accurate responses to queries about company financial informa-\n",
            "tion, leveraging both the power of large language models and the\n",
            "efficiency of semantic search.\n",
            "At the core of our system is a Pinecone vector database7, which\n",
            "serves as the foundation for our information retrieval process. We\n",
            "employ OpenAI’s text-embedding-ada-002 model to transform tex-\n",
            "tual data from earnings call transcripts into high-dimensional vec-\n",
            "tor representations. This vectorization process enables semantic\n",
            "similarity searches, significantly enhancing the relevance and ac-\n",
            "curacy of retrieved information. Table 3 provides summary of the\n",
            "configuration of the set up in use for our experiments.\n",
            "LLM GPT-3.5-Turbo\n",
            "LLM Temperature 0\n",
            "Embedding Model text-embedding-ada-002\n",
            "Framework LangChain\n",
            "Vector Database Pinecone' metadata={'source': '/content/hybridrag.pdf', 'page': 5}\n",
            "Chunk  44  :  page_content='configuration of the set up in use for our experiments.\n",
            "LLM GPT-3.5-Turbo\n",
            "LLM Temperature 0\n",
            "Embedding Model text-embedding-ada-002\n",
            "Framework LangChain\n",
            "Vector Database Pinecone\n",
            "Chunk Size 1024\n",
            "Chunk Overlap 0\n",
            "Maximum Output Tokens 1024\n",
            "Chunks for Similarity Algorithm 20\n",
            "Number of Context Retrieved 4\n",
            "Table 3: VectorRAG Configuration\n",
            "7https://www.pinecone.io/The Q&A pipeline is constructed using the LangChain frame-\n",
            "work8. The begins with a context retrieval step, where we query the\n",
            "Pinecone vector store to obtain the most relevant document chunks\n",
            "for a given question. This retrieval process is fine-tuned with spe-\n",
            "cific filters for quarters, years, and company names, ensuring that\n",
            "the retrieved information is both relevant and current.\n",
            "Following retrieval, we implement a context formatting step that\n",
            "consolidates the retrieved document chunks into a coherent context\n",
            "string. This formatted context serves as the informational basis for' metadata={'source': '/content/hybridrag.pdf', 'page': 5}\n",
            "Chunk  45  :  page_content='consolidates the retrieved document chunks into a coherent context\n",
            "string. This formatted context serves as the informational basis for\n",
            "the language model’s response generation. We have developed a\n",
            "sophisticated prompt template, that instructs the language model\n",
            "to function as an expert Q&A system, emphasizing the importance\n",
            "of utilizing only the provided context information and avoiding\n",
            "direct references to the context in the generated responses.\n",
            "For the core language processing task, we integrate OpenAI’s\n",
            "GPT-3.5-turbo model which processes the formatted context and\n",
            "query to generate natural language responses that are informative,\n",
            "coherent, and contextually appropriate.\n",
            "To evaluate the performance of our system, we developed a' metadata={'source': '/content/hybridrag.pdf', 'page': 5}\n",
            "Chunk  46  :  page_content='query to generate natural language responses that are informative,\n",
            "coherent, and contextually appropriate.\n",
            "To evaluate the performance of our system, we developed a\n",
            "comprehensive framework that includes the preparation of a cus-\n",
            "tom dataset of question-answer pairs specific to each company’s\n",
            "earnings call. Our system processes each question in this dataset,\n",
            "generating answers based on the retrieved context. The evaluation\n",
            "results, including the original question, generated answer, retrieved\n",
            "contexts, and ground truth, are compiled into structured formats\n",
            "(CSV and JSON) to facilitate further analysis. The outputs generated\n",
            "by our system are stored in both CSV and JSON formats, enabling\n",
            "easy integration with various analysis tools and dashboards. This\n",
            "approach facilitates both quantitative performance metrics and\n",
            "qualitative assessment of the system’s responses, providing a com-\n",
            "prehensive view of its effectiveness.\n",
            "By parameterizing company names, quarters, and years, we can' metadata={'source': '/content/hybridrag.pdf', 'page': 5}\n",
            "Chunk  47  :  page_content='qualitative assessment of the system’s responses, providing a com-\n",
            "prehensive view of its effectiveness.\n",
            "By parameterizing company names, quarters, and years, we can\n",
            "easily adapt the system to different datasets and time periods. This\n",
            "design choice allows for seamless integration of new data and\n",
            "expansion to cover multiple companies and earnings calls.\n",
            "4.3 GraphRAG\n",
            "For GraphRAG, we developed an Q&A system specifically designed\n",
            "for corporate earnings call transcripts. Our implementation of\n",
            "GraphRAG leverages several key components and techniques:\n",
            "LLM GPT-3.5-Turbo\n",
            "LLM Temperature 0\n",
            "Framework LangChain\n",
            "KG Manipulation Networkx\n",
            "Chunk Size 1024\n",
            "Chunk Overlap 0\n",
            "Number of Triplets 13950\n",
            "Number of nodes 11405\n",
            "Number of edges 13883\n",
            "DFS Depth 1\n",
            "Table 4: GraphRAG Configuration\n",
            "8https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain' metadata={'source': '/content/hybridrag.pdf', 'page': 5}\n",
            "Chunk  48  :  page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\n",
            "Knowledge Graph Construction:- We begin by constructing\n",
            "a KG from a set of knowledge triplets extracted from corporate\n",
            "documents using the prompt engineering based methodology as\n",
            "described in Section 4.1. These triplets, stored in a pickle file, repre-\n",
            "sent structured information in the form of subject-predicate-object\n",
            "relationships. We use the NetworkxEntityGraph class from the\n",
            "LangChain library to create and manage this graph structure. Each\n",
            "triple is added to the graph, which encapsulates the head entity,\n",
            "relation, tail entity, and additional metadata.\n",
            "We implement the Q&A functionality using the GraphQAChain\n",
            "class from LangChain. This chain combines the KG with an LLM\n",
            "(in our case, OpenAI’s GPT-3.5-turbo) to generate responses. The\n",
            "GraphQAChain traverses the KG to find relevant information and\n",
            "uses the language model to formulate coherent answers based on' metadata={'source': '/content/hybridrag.pdf', 'page': 6}\n",
            "Chunk  49  :  page_content='(in our case, OpenAI’s GPT-3.5-turbo) to generate responses. The\n",
            "GraphQAChain traverses the KG to find relevant information and\n",
            "uses the language model to formulate coherent answers based on\n",
            "the retrieved context. A summary of configuration of our LLM\n",
            "models and other libraries used for GraphRAG is shown in Table 4.\n",
            "In KG as the information is stored in the form of entities and\n",
            "relationships and there can be multiple relations emanating from\n",
            "one single entity, in this experiment, to extract relevant information\n",
            "from the KG, we employ a depth-first search strategy constrained\n",
            "by a depth of one from the specified entity.\n",
            "To prepare for assessing the performance of our GraphRAG\n",
            "system, we follow the below steps: Dataset Preparation:- We use\n",
            "a pre-generated CSV file containing question-answer pairs specific\n",
            "to the earnings call transcript.\n",
            "Iterative Processing:- For each question in the dataset, we run\n",
            "the GraphQAChain to generate an answer.' metadata={'source': '/content/hybridrag.pdf', 'page': 6}\n",
            "Chunk  50  :  page_content='to the earnings call transcript.\n",
            "Iterative Processing:- For each question in the dataset, we run\n",
            "the GraphQAChain to generate an answer.\n",
            "Result Compilation:- We compile the results, including the origi-\n",
            "nal questions, generated answers, retrieved contexts, and ground\n",
            "truth answers, into a structured format.\n",
            "Finally, the evaluation results are saved in both CSV and JSON\n",
            "formats for further analysis and comparison. We then fed these\n",
            "outputs into our RAG evaluation pipeline. For each Q&A pair in\n",
            "our dataset, we compute all three metrics: faithfulness, answer\n",
            "relevance, context precision and context recall.\n",
            "4.4 HybridRAG\n",
            "For the proposed HybridRAG technique, upon obtaining all the\n",
            "contextual information from VectorRAG and GraphRAG, we con-\n",
            "catenate these contexts to form a unified context utilizing both\n",
            "techniques. This combined context is then fed into the answer gen-\n",
            "erator model to produce a response. The context used for response' metadata={'source': '/content/hybridrag.pdf', 'page': 6}\n",
            "Chunk  51  :  page_content='catenate these contexts to form a unified context utilizing both\n",
            "techniques. This combined context is then fed into the answer gen-\n",
            "erator model to produce a response. The context used for response\n",
            "generation is relatively larger, which affects the precision of the\n",
            "generated response. The context from VectorRAG is appended first,\n",
            "followed by the context from GraphRAG. Consequently, the preci-\n",
            "sion of the generated answer depends on the source context. If the\n",
            "answer is generated from the GraphRAG context, it will have lower\n",
            "precision, as the GraphRAG context is added last in the sequence of\n",
            "contexts provided to the answer generator model, and vice versa.\n",
            "5 RESULTS\n",
            "We evaluate both the retrieval and generation parts of RAG for\n",
            "the three different RAG pipelines. Evaluating the RAG outputs is\n",
            "also an active area of research there is no standard tool which is\n",
            "universally accepted as of yet, though we use a currently popular' metadata={'source': '/content/hybridrag.pdf', 'page': 6}\n",
            "Chunk  52  :  page_content='the three different RAG pipelines. Evaluating the RAG outputs is\n",
            "also an active area of research there is no standard tool which is\n",
            "universally accepted as of yet, though we use a currently popular\n",
            "framework RAGAS[ 30] to evaluate the three RAG pipelines in thepresent work where we have modified them a bit to make more\n",
            "precise comparisons.\n",
            "The results of our comparative analysis reveal notable differences\n",
            "in performance among VectorRAG, GraphRAG, and HybridRAG\n",
            "approaches as summarized in Table 5. In terms of Faithfulness,\n",
            "GraphRAG and HybridRAG demonstrated superior performance,\n",
            "both achieving a score of 0.96, while VectorRAG trailed slightly with\n",
            "a score of 0.94. Answer relevancy scores varied across the meth-\n",
            "ods, with HybridRAG outperforming the others at 0.96, followed\n",
            "by VectorRAG at 0.91, and GraphRAG at 0.89. Context precision' metadata={'source': '/content/hybridrag.pdf', 'page': 6}\n",
            "Chunk  53  :  page_content='a score of 0.94. Answer relevancy scores varied across the meth-\n",
            "ods, with HybridRAG outperforming the others at 0.96, followed\n",
            "by VectorRAG at 0.91, and GraphRAG at 0.89. Context precision\n",
            "was highest for GraphRAG at 0.96, significantly surpassing Vec-\n",
            "torRAG (0.84) and HybridRAG (0.79). However, in context recall,\n",
            "both VectorRAG and HybridRAG achieved perfect scores of 1, while\n",
            "GraphRAG lagged behind at 0.85.\n",
            "Overall, these results suggest that GraphRAG offers improve-\n",
            "ments over VectorRAG, particularly in faithfulness and context pre-\n",
            "cision. Furthermore, HybridRAG emerges as the most balanced and\n",
            "effective approach, outperforming both VectorRAG and GraphRAG\n",
            "in key metrics such as faithfulness and answer relevancy, while\n",
            "maintaining high context recall.\n",
            "The relatively lower context precision observed for HybridRAG\n",
            "(0.79) can be attributed to its unique approach of combining con-\n",
            "texts from both VectorRAG and GraphRAG methods. While this' metadata={'source': '/content/hybridrag.pdf', 'page': 6}\n",
            "Chunk  54  :  page_content='The relatively lower context precision observed for HybridRAG\n",
            "(0.79) can be attributed to its unique approach of combining con-\n",
            "texts from both VectorRAG and GraphRAG methods. While this\n",
            "integration allows for more comprehensive information retrieval,\n",
            "it also introduces additional content that may not align precisely\n",
            "with the ground truth, thus affecting the context precision met-\n",
            "ric. Despite this trade-off, HybridRAG’s superior performance in\n",
            "faithfulness, answer relevancy, and context recall underscores its\n",
            "effectiveness. When considering the overall evaluation metrics,\n",
            "HybridRAG emerges as the most promising approach, balancing\n",
            "high-quality answers with comprehensive context retrieval.\n",
            "Overall GraphRAG performs better in extractive questions com-\n",
            "pared to VectorRAG. And VectorRAG does better in abstractive\n",
            "questions where information is not explicitly mentioned in the\n",
            "raw data. And also GraphRAG sometimes fails to answer ques-' metadata={'source': '/content/hybridrag.pdf', 'page': 6}\n",
            "Chunk  55  :  page_content='pared to VectorRAG. And VectorRAG does better in abstractive\n",
            "questions where information is not explicitly mentioned in the\n",
            "raw data. And also GraphRAG sometimes fails to answer ques-\n",
            "tions correctly whenever there is no entity explicitly mentioned in\n",
            "the question. So HybridRAG does a good job overall, as whenever\n",
            "VectorRAG fails to fetch correct context in extractive questions\n",
            "it falls back to GraphRAG to generate the answer. And whenever\n",
            "GraphRAG fails to fetch correct context in abstractive questions it\n",
            "falls back to VectorRAG to generate the answer.\n",
            "VectorRAG GraphRAG HybridRAG\n",
            "F 0.94 0.96 0.96\n",
            "AR 0.91 0.89 0.96\n",
            "CP 0.84 0.96 0.79\n",
            "CR 1 0.85 1\n",
            "Table 5: Performance Metrics for Different RAG Pipelines.\n",
            "Here, F, AR, CP and CR refer to Faithfulness, Answer\n",
            "Relevence, Context Precision and Context Recall.\n",
            "6 CONCLUSION AND FUTURE DIRECTIONS\n",
            "Among the current approaches to mitigate issues regarding infor-\n",
            "mation extraction from external documents that were not part of' metadata={'source': '/content/hybridrag.pdf', 'page': 6}\n",
            "Chunk  56  :  page_content='Sarmah et al.\n",
            "training data for the LLM, Retrieval Augmented Generation (RAG)\n",
            "techniques have emerged as the most popular ones that aim to\n",
            "improve the performance of LLMs by incorporating relevant re-\n",
            "trieval mechanisms. RAG methods enhance the LLMs’ capabilities\n",
            "by retrieving pertinent documents or text to provide additional\n",
            "context during the generation process. However, these approaches\n",
            "encounter significant limitations when applied to the specialized\n",
            "and intricate domain of financial documents. Furthermore, the qual-\n",
            "ity of the retrieved context from a vast and heterogeneous corpus\n",
            "can be inconsistent, leading to inaccuracies and incomplete analyses.\n",
            "These challenges highlight the need for more sophisticated methods\n",
            "that can effectively integrate and process the detailed and domain-\n",
            "specific information found in financial documents, ensuring more\n",
            "reliable and accurate outputs for informed decision-making.\n",
            "In the present work, we have introduced a novel approach that' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  57  :  page_content='specific information found in financial documents, ensuring more\n",
            "reliable and accurate outputs for informed decision-making.\n",
            "In the present work, we have introduced a novel approach that\n",
            "significantly advances the field of information extraction from finan-\n",
            "cial documents through the development of a hybrid RAG system.\n",
            "This system, called HybridRAG, which integrates the strengths\n",
            "of both Knowledge Graphs (KGs) and advanced language models,\n",
            "represents a leap forward in our ability to extract and interpret\n",
            "complex information from unstructured financial texts. The hybrid\n",
            "RAG system, by combining traditional vector-based RAG and KG-\n",
            "based RAG, has shown superior performance in terms of retrieval\n",
            "accuracy and answer generation, marking a pivotal step towards\n",
            "more effective financial analysis tools.\n",
            "Through a comparative analysis using objecive evaluation met-\n",
            "rics, we have highlighted the distinct performance advantages of\n",
            "the HybridRAG approach over its vector-based and KG-based coun-' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  58  :  page_content='Through a comparative analysis using objecive evaluation met-\n",
            "rics, we have highlighted the distinct performance advantages of\n",
            "the HybridRAG approach over its vector-based and KG-based coun-\n",
            "terparts. The HybridRAG system excels in faithfulness, answer\n",
            "relevancy, and context recall, demonstrating the benefits of inte-\n",
            "grating contexts from both VectorRAG and GraphRAG methods,\n",
            "despite potential trade-offs in context precision.\n",
            "The implications of this research extend beyond the immedi-\n",
            "ate realm of financial analysis. By developing a system capable\n",
            "of understanding and responding to nuanced queries about com-\n",
            "plex financial documents, we pave the way for more sophisticated\n",
            "AI-assisted financial decision-making tools that could potentially\n",
            "democratize access to financial insights, allowing a broader range of\n",
            "stakeholders to engage with and understand financial information.\n",
            "Future directions for this research include expanding the system' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  59  :  page_content='stakeholders to engage with and understand financial information.\n",
            "Future directions for this research include expanding the system\n",
            "to handle multi-modal inputs, incorporating numerical data analysis\n",
            "capabilities, and developing more sophisticated evaluation metrics\n",
            "that capture the nuances of financial language and the accuracy\n",
            "of numerical information in the responses. Additionally, exploring\n",
            "the integration of this system with real-time financial data streams\n",
            "could further enhance its utility in dynamic financial environments.\n",
            "7 ACKNOWLEDGEMENT\n",
            "The views expressed here are those of the authors alone and not of\n",
            "BlackRock, Inc or NVIDIA. We are grateful to Emma Lind for her\n",
            "invaluable support for this collaboration.\n",
            "REFERENCES\n",
            "[1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation\n",
            "of word representations in vector space. 2013.\n",
            "[2]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  60  :  page_content='of word representations in vector space. 2013.\n",
            "[2]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\n",
            "Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.\n",
            "Advances inneural information processing systems, 30, 2017.[3]Yi Yang, Mark Christopher Siy Uy, and Allen Huang. Finbert: A pretrained\n",
            "language model for financial communications. arXiv preprint arXiv:2006.08097 ,\n",
            "2020.\n",
            "[4]Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Se-\n",
            "bastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon\n",
            "Mann. Bloomberggpt: A large language model for finance. arXiv preprint\n",
            "arXiv:2303.17564, 2023.\n",
            "[5] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qing-' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  61  :  page_content='Mann. Bloomberggpt: A large language model for finance. arXiv preprint\n",
            "arXiv:2303.17564, 2023.\n",
            "[5] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qing-\n",
            "song Wen, and Stefan Zohren. A survey of large language models for financial ap-\n",
            "plications: Progress, prospects and challenges. arXiv preprint arXiv:2406.11903 ,\n",
            "2024.\n",
            "[6]Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu,\n",
            "Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, Ninghao Liu, and Tianming\n",
            "Liu. Revolutionizing finance with llms: An overview of applications and insights,\n",
            "2024.\n",
            "[7] Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang\n",
            "Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. Domain spe-\n",
            "cialization as the key to make large language models disruptive: A comprehensive\n",
            "survey. arXiv preprint arXiv:2305.18703, 2023.\n",
            "[8] Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, and Tianjie Zhu. Towards' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  62  :  page_content='survey. arXiv preprint arXiv:2305.18703, 2023.\n",
            "[8] Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, and Tianjie Zhu. Towards\n",
            "reducing hallucination in extracting information from financial reports using\n",
            "large language models. In Proceedings oftheThird International Conference\n",
            "onAI-ML Systems, pages 1–5, 2023.\n",
            "[9]Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\n",
            "Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\n",
            "Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp\n",
            "tasks. Advances inNeural Information Processing Systems , 33:9459–9474, 2020.\n",
            "[10] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative\n",
            "models for open domain question answering. arXiv preprint arXiv:2007.01282 ,\n",
            "2021.\n",
            "[11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.\n",
            "Realm: Retrieval-augmented language model pre-training. arXiv preprint\n",
            "arXiv:2002.08909, 2020.' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  63  :  page_content='2021.\n",
            "[11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.\n",
            "Realm: Retrieval-augmented language model pre-training. arXiv preprint\n",
            "arXiv:2002.08909, 2020.\n",
            "[12] Antonio Jose Jimeno Yepes et al. Financial report chunking for effective retrieval\n",
            "augmented generation. arXiv preprint arXiv:2402.05131, 2024.\n",
            "[13] SuperAcc. Retrieval-augmented generation on financial statements. SuperAcc\n",
            "Insights, 2024.\n",
            "[14] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A\n",
            "survey on knowledge graphs: Representation, acquisition, and applications. IEEE\n",
            "transactions onneural networks andlearning systems, 33(2):494–514, 2021.\n",
            "[15] Heiko Paulheim. Knowledge graphs: State of the art and future directions.\n",
            "Semantic Web Journal, 10(4):1–20, 2017.\n",
            "[16] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia D’Amato, Gerard de Melo,\n",
            "Claudio Gutiérrez, and Maria ... Maleshkova. Knowledge graphs. arXiv preprint\n",
            "arXiv:2003.02320, 2021.' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  64  :  page_content='[16] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia D’Amato, Gerard de Melo,\n",
            "Claudio Gutiérrez, and Maria ... Maleshkova. Knowledge graphs. arXiv preprint\n",
            "arXiv:2003.02320, 2021.\n",
            "[17] Lisa Ehrlinger and Wolfram Wöß. Towards a definition of knowledge graphs. In\n",
            "SEMANTiCS (Posters, Demos, SuCCESS), pages 1–4, 2016.\n",
            "[18] Xiaohui Victor Li and Francesco Sanna Passino. Findkg: Dynamic knowledge\n",
            "graphs with large language models for detecting global trends in financial mar-\n",
            "kets. arXiv preprint arXiv:2407.10909, 2024.\n",
            "[19] Shourya De, Anima Aggarwal, and Anna Cinzia Squicciarini. Financial knowl-\n",
            "edge graphs: A novel approach to empower data-driven financial applications. In\n",
            "IEEE International Conference onBigData (Big Data), pages 1311–1320, 2018.\n",
            "[20] Marina Petrova and Birgit Reinwald. Knowledge graphs in finance: Applications\n",
            "and opportunities. Journal ofFinancial Data Science, 2(2):10–19, 2020.\n",
            "[21] Wei Liu, Jun Zhang, and Li Pan. Utilizing knowledge graphs for financial data' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  65  :  page_content='and opportunities. Journal ofFinancial Data Science, 2(2):10–19, 2020.\n",
            "[21] Wei Liu, Jun Zhang, and Li Pan. Utilizing knowledge graphs for financial data\n",
            "integration and analysis. Data Science Journal, 18(1):1–15, 2019.\n",
            "[22] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva\n",
            "Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag\n",
            "approach to query-focused summarization. arXiv preprint arXiv:2404.16130 ,\n",
            "2024.\n",
            "[23] Xuchen Yao, Yanan Sun, Zhen Huang, and Dong Li. Retrieval-augmented gener-\n",
            "ation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2101.07554 , 2021.\n",
            "[24] Weizhi Zhao, Haoyu Chen, Kai Liu, and Jun Zhao. Graph-based retrieval-\n",
            "augmented generation for open-domain question answering. In Proceedings' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  66  :  page_content='[24] Weizhi Zhao, Haoyu Chen, Kai Liu, and Jun Zhao. Graph-based retrieval-\n",
            "augmented generation for open-domain question answering. In Proceedings\n",
            "oftheAAAI Conference onArtificial Intelligence , volume 36, pages 3098–3106,\n",
            "2022.\n",
            "[25] Bill Yuchen Lin, Yuanhe Liu, Ming Shen, and Xiang Ren. Kgpt: Knowledge-\n",
            "grounded pre-training for data-to-text generation. In Findings oftheAssociation\n",
            "forComputational Linguistics: EMNLP 2020, pages 710–724, 2020.\n",
            "[26] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Ji-\n",
            "awei Sun, and Haofen Wang. Retrieval-augmented generation for large language\n",
            "models: A survey. arXiv preprint arXiv:2312.10997, 2023.\n",
            "[27] Tyler Procko. Graph retrieval-augmented generation for large language models:\n",
            "A survey. Available atSSRN, 2024.\n",
            "[28] Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. A comprehensive\n",
            "survey on automatic knowledge graph construction. ACM Computing Surveys ,\n",
            "56(4):1–62, 2023.' metadata={'source': '/content/hybridrag.pdf', 'page': 7}\n",
            "Chunk  67  :  page_content='HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\n",
            "[29] Ishani Mondal, Yufang Hou, and Charles Jochim. End-to-end nlp knowledge\n",
            "graph construction. arXiv preprint arXiv:2106.01167, 2021.\n",
            "[30] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ra-\n",
            "gas: Automated evaluation of retrieval augmented generation. arXiv preprint\n",
            "arXiv:2309.15217, 2023.\n",
            "[31] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan\n",
            "Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al.\n",
            "Finqa: A dataset of numerical reasoning over financial data. arXiv preprint\n",
            "arXiv:2109.00122, 2021.\n",
            "[32] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott,\n",
            "Manel Zarrouk, and Alexandra Balahur. Www’18 open challenge: financial\n",
            "opinion mining and question answering. In Companion proceedings ofthetheweb conference 2018, pages 1941–1942, 2018.' metadata={'source': '/content/hybridrag.pdf', 'page': 8}\n",
            "Chunk  68  :  page_content='Manel Zarrouk, and Alexandra Balahur. Www’18 open challenge: financial\n",
            "opinion mining and question answering. In Companion proceedings ofthetheweb conference 2018, pages 1941–1942, 2018.\n",
            "[33] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang,\n",
            "Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: A question answering\n",
            "benchmark on a hybrid of tabular and textual content in finance. arXiv preprint\n",
            "arXiv:2105.07624, 2021.\n",
            "[34] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer,\n",
            "and Bertie Vidgen. Financebench: A new benchmark for financial question\n",
            "answering. arXiv preprint arXiv:2311.11944, 2023.\n",
            "[35] Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing\n",
            "Zhu, and Junwei Liang. Fintextqa: A dataset for long-form financial question\n",
            "answering. arXiv preprint arXiv:2405.09980, 2024.' metadata={'source': '/content/hybridrag.pdf', 'page': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_vector_store = Chroma(\n",
        "    collection_name=\"hybridrag_rag\",\n",
        "    embedding_function=embed,\n",
        "    persist_directory=\"./hybrid_chroma_db\"\n",
        ")\n",
        "pdf_vector_store.add_documents(documents=pdf_splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXR-h9EDfMD4",
        "outputId": "a175f053-cb03-4512-e1e0-08558c97941f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['792fa978-0874-4136-8230-9cec6f1caaa3',\n",
              " '477e36d9-a727-4cf0-a248-8709c7afbde5',\n",
              " '3cfa14e9-342a-4bac-8c51-a61ec6942dbc',\n",
              " '6b5c553c-d354-4d13-b01f-5e6d79949fb2',\n",
              " '5efa1c31-25d5-4b47-bd51-bb6e365e21d0',\n",
              " '25247dac-dc95-4841-a4dd-7fb0d00f7799',\n",
              " 'd6b3bb17-82b7-4d26-ab54-a482a814720d',\n",
              " '3e84c0e6-1a50-48fb-948a-24ee2a4f0018',\n",
              " '315313c1-c621-4476-8a2a-f2a9515b51d6',\n",
              " '10165816-446a-4b4c-ae57-b20f194ac114',\n",
              " 'abbfa836-cd6d-48ee-9bdb-34f26fdb7d56',\n",
              " '3bbdd206-f597-416b-8ea3-1b29ea03c798',\n",
              " '42f681df-e238-490b-b7c9-4b6d58544323',\n",
              " '7704b2c6-1aec-48f8-870c-a95a60418c0d',\n",
              " '0d678bfa-1243-42fa-923d-b3665e671c73',\n",
              " '19d5dbb1-ca52-468d-8f1e-d369ee95e2e0',\n",
              " '9e7e81b2-8c2d-4301-b4d7-7899a3822b1a',\n",
              " '8b07b270-8db1-405d-87b3-215f43a49279',\n",
              " 'e21ab5ce-4e4e-4868-a2ae-f2247b428c09',\n",
              " '0dc08c16-211a-4f6a-80e9-2b1946e7e5f2',\n",
              " 'c39b8889-3637-44f0-85e3-c498c99851c2',\n",
              " 'b10e3fe0-c2d3-436f-8f35-33aadfa6a9ef',\n",
              " '6533f53d-4b39-49f5-8522-6b40aa3b470b',\n",
              " '02889f30-01f2-4c2f-a0de-eaf7d6bcd004',\n",
              " 'cf8dc420-4595-4126-9636-e2eb51168fc3',\n",
              " '1ba68230-7d00-4556-88ab-49b4701de6d7',\n",
              " '5624151e-8d83-45ec-8d64-b5c007a87394',\n",
              " '76243a67-8394-4bed-938f-a42c19d18527',\n",
              " '17fcfd6e-e764-45ef-a5df-ebad25ee9a02',\n",
              " '469e0a03-32dc-4295-8a48-f869b38ee78d',\n",
              " '433dc4b3-aa8b-4b5e-b163-992c95496319',\n",
              " 'c7898f95-2b05-4462-b833-d6366bd6a145',\n",
              " '67d09219-a07b-4038-8fd1-f5890e63b8d4',\n",
              " '81d2a3d7-3263-46b6-bd2d-201f40c0c648',\n",
              " 'ba96a27f-7cb0-4667-ae8c-54cc15558b4b',\n",
              " 'bd22727f-77f3-416f-9f3c-4be3b363c359',\n",
              " '092dabb7-5b40-478d-90f8-1285ec59140b',\n",
              " '5b313d20-05c5-4ccf-821b-1d36ba12420f',\n",
              " 'dffd531d-313d-4939-99de-dd751b91dd35',\n",
              " 'cf2d9f2c-1b69-4a63-a2c4-34eec5fce681',\n",
              " '2ad6830c-d188-4c8e-acf5-8c41d12deb40',\n",
              " 'e679823b-8e03-4e7c-acfd-682f5d561844',\n",
              " 'bc998194-6491-4adb-9613-52c553fc1b88',\n",
              " 'cd46d59c-c9f1-40db-b610-40139480b599',\n",
              " 'aba4e34b-ecdc-4a3d-9c5b-ffe4d1cb6f4e',\n",
              " 'fbcd98cc-a648-4fd0-aeee-50f2571e52ef',\n",
              " '7021e27c-8ef4-4875-a123-abf8332d0ab5',\n",
              " '28a88883-7979-4c65-9e90-c39beba5feea',\n",
              " 'ffe6013a-a022-427c-af00-a102a923577a',\n",
              " 'fadbf7c3-1fcd-4a28-a55f-4e691060b493',\n",
              " '2d1db2f5-f53f-42c7-b671-213829c3e3df',\n",
              " 'f0ed33a3-81ad-4c55-8fe4-b2a14116708c',\n",
              " 'f6fc2c14-7f83-4d37-a30c-4756b76dd9e5',\n",
              " '2d63a3dc-0c89-400f-9def-3587fa92ba25',\n",
              " 'dfea80ff-df63-4d62-b194-60378ed04346',\n",
              " 'fc94d591-78f2-4201-86bd-96e791e3f53e',\n",
              " 'c0ecf6a0-165c-41ee-9a76-157cfba8e514',\n",
              " '77b0f4c5-0f2b-4904-b78e-f6b16df5f763',\n",
              " '897fdc80-82f1-4d6f-8f60-29be12ce2fa4',\n",
              " '1ca2585b-b345-4c0e-b90c-830d0ff13c03',\n",
              " '15621506-7a2f-4d4c-bcc9-0f41a577d09d',\n",
              " '8c665335-4bf6-4b8a-a96d-270173e76a3c',\n",
              " 'f9bc6367-4710-4d34-a917-e9891a20f683',\n",
              " 'cce30e70-019a-48c9-9bf5-9a7dc02f1ebc',\n",
              " '75f42978-d2e1-4721-81ff-0ac4b2e593ef',\n",
              " 'aba3f08d-6543-480b-822f-3f4103c034b7',\n",
              " 'f4dd8057-0e9b-4b3d-807c-43f961bde9fd',\n",
              " '48f6e997-329d-436a-bc4e-fe1a882914f1',\n",
              " '3b9ced9d-7225-49b2-a4f5-5d27bca1abf6']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_retreiver = pdf_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "pdf_ret_docs = pdf_retreiver.get_relevant_documents(\"What is hybrid RAG?\")"
      ],
      "metadata": {
        "id": "UeaEV-NefXXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pdf_ret_docs)):\n",
        "    print(\"Ret \",i,\" : \",pdf_ret_docs[i].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Xt-8MMfgbV",
        "outputId": "8170f77d-680e-48f2-c548-c9fbc71473c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ret  0  :  trieval Augmented Generation (RAG) (referred to as VectorRAG\n",
            "techniques which utilize vector databases for information retrieval)\n",
            "due to challenges such as domain specific terminology and complex\n",
            "formats of the documents. We introduce a novel approach based\n",
            "on a combination, called HybridRAG , of the Knowledge Graphs\n",
            "(KGs) based RAG techniques (called GraphRAG) and VectorRAG\n",
            "techniques to enhance question-answer (Q&A) systems for infor-\n",
            "mation extraction from financial documents that is shown to be\n",
            "capable of generating accurate and contextually relevant answers.\n",
            "Using experiments on a set of financial earning call transcripts\n",
            "documents which come in the form of Q&A format, and hence\n",
            "provide a natural set of pairs of ground-truth Q&As, we show that\n",
            "HybridRAG which retrieves context from both vector database and\n",
            "KG outperforms both traditional VectorRAG and GraphRAG indi-\n",
            "vidually when evaluated at both the retrieval and generation stages\n",
            "Ret  1  :  specific information found in financial documents, ensuring more\n",
            "reliable and accurate outputs for informed decision-making.\n",
            "In the present work, we have introduced a novel approach that\n",
            "significantly advances the field of information extraction from finan-\n",
            "cial documents through the development of a hybrid RAG system.\n",
            "This system, called HybridRAG, which integrates the strengths\n",
            "of both Knowledge Graphs (KGs) and advanced language models,\n",
            "represents a leap forward in our ability to extract and interpret\n",
            "complex information from unstructured financial texts. The hybrid\n",
            "RAG system, by combining traditional vector-based RAG and KG-\n",
            "based RAG, has shown superior performance in terms of retrieval\n",
            "accuracy and answer generation, marking a pivotal step towards\n",
            "more effective financial analysis tools.\n",
            "Through a comparative analysis using objecive evaluation met-\n",
            "rics, we have highlighted the distinct performance advantages of\n",
            "the HybridRAG approach over its vector-based and KG-based coun-\n",
            "Ret  2  :  Through a comparative analysis using objecive evaluation met-\n",
            "rics, we have highlighted the distinct performance advantages of\n",
            "the HybridRAG approach over its vector-based and KG-based coun-\n",
            "terparts. The HybridRAG system excels in faithfulness, answer\n",
            "relevancy, and context recall, demonstrating the benefits of inte-\n",
            "grating contexts from both VectorRAG and GraphRAG methods,\n",
            "despite potential trade-offs in context precision.\n",
            "The implications of this research extend beyond the immedi-\n",
            "ate realm of financial analysis. By developing a system capable\n",
            "of understanding and responding to nuanced queries about com-\n",
            "plex financial documents, we pave the way for more sophisticated\n",
            "AI-assisted financial decision-making tools that could potentially\n",
            "democratize access to financial insights, allowing a broader range of\n",
            "stakeholders to engage with and understand financial information.\n",
            "Future directions for this research include expanding the system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "pdf_rag_chain = (\n",
        "    {\"context\": pdf_retreiver, \"question\": RunnablePassthrough()}\n",
        "    | pdf_prompt\n",
        "    | ChatOpenAI(model='gpt-4o',temperature=0)\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "XnoGtYZRfjK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_rag_chain.invoke(\"What is hybrid RAG?\")"
      ],
      "metadata": {
        "id": "9GdVm7GLfyFy",
        "outputId": "d6d463b3-9992-4d84-aac1-37e2b5ed93fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hybrid RAG is a system that combines Knowledge Graph (KG)-based RAG techniques (GraphRAG) and vector-based RAG techniques (VectorRAG) to enhance question-answering systems. It is designed to improve information extraction from complex documents, such as financial texts, by leveraging the strengths of both methods. This hybrid approach has shown superior performance in terms of retrieval accuracy and answer generation compared to using either method alone.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLmxmk1ofz8R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}